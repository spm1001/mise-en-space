{"id": "mise-02b", "title": "Integration test with real account", "type": "action", "status": "done", "done_at": "2026-01-25T16:41:13.180683Z", "brief": {"why": "End-to-end test: search → fetch → read file. Uses test fixtures from V2.md.", "what": "## Test Fixtures (from EXPERIMENTS.md)\n\n| Fixture | ID | Description |\n|---------|-----|-------------|\n| Doc: Simple | 1w8KcJtwlHcWn3HLlp9QVMQzmfVegW0lhVOd8K8kuLig | ROUNDTRIP_TEST |\n| Doc: Multi-tab | 1vqkOoNbPuc1MNolYZRhKvBONGZrrIopAb7bzYzn5x0k | Team Handbook |\n| Sheets | 1Hf2smDVDjBlxaZuGZvAJML5XmdFM-sPZW3xkCRfUfLY | Team meeting |\n| Gmail: Long thread | 194ac68f6091de54 | 72 messages |\n\n## Test Cases\n\n1. search(\"budget\") → returns Drive + Gmail results\n2. fetch(doc_id) → writes markdown, returns path\n3. fetch(sheet_id) → writes CSV, returns path\n4. fetch(thread_id) → writes cleaned text, returns path\n5. Read returned path → content is valid\n\n## Acceptance Criteria\n\n- [ ] All fixture types tested\n- [ ] Paths exist and readable\n- [ ] Content is valid markdown/CSV/text\n- [ ] No inline content in MCP response", "done": "When complete"}, "created_at": "2026-01-23T07:25:46.343318Z", "created_by": "spm1001", "order": 1, "parent": "mise-52d", "waiting_for": null}
{"id": "mise-067", "title": "Enable Claude to access browser dev tools directly", "type": "action", "status": "done", "brief": {"why": "Claude needs to sniff network requests autonomously, not via human-in-the-loop. Explore: webctl network intercept, Playwright request/response events, or Anthropic's claude-in-chrome extension.", "what": "## Problem\n\nReverse-engineering private APIs requires inspecting network traffic. Currently:\n- User opens DevTools manually\n- User copies requests\n- Claude interprets\n- Slow, error-prone, breaks flow\n\n## Options\n\n1. **webctl enhancement**: Add `webctl intercept` command that captures requests/responses\n   - Playwright has `page.on('request')`, `page.on('response')`\n   - Could filter by URL pattern\n   - Return as JSON for Claude to parse\n\n2. **claude-in-chrome**: Anthropic's extension at ~/Repos/ (check if exists)\n   - May already have DevTools integration\n   - Native to Claude's ecosystem\n\n3. **Playwright directly**: Write a Python script that launches browser + intercepts\n   - More control, but heavier lift\n\n## Acceptance Criteria\n\n- [ ] Claude can autonomously capture network requests\n- [ ] Filter by URL pattern (e.g., \"timedtext\", \"gemini\")\n- [ ] Get request headers, payload, response body\n- [ ] No human copy-paste required", "done": "When complete"}, "created_at": "2026-01-24T08:54:12.814936Z", "created_by": "spm1001", "order": 1, "parent": "mise-jy3", "waiting_for": null, "done_at": "2026-01-29T20:47:52Z"}
{"id": "mise-138", "title": "Push to GitHub as mise-en-space", "type": "action", "status": "done", "brief": {"why": "Create GitHub repo, push initial scaffold. Currently local-only.", "what": "See title", "done": "When complete"}, "created_at": "2026-01-20T22:10:42.331554Z", "created_by": "spm1001", "order": 1, "waiting_for": null, "done_at": "2026-01-31T19:52:09Z"}
{"id": "mise-1k9", "title": "Slides adapter: batch thumbnails", "type": "action", "status": "done", "done_at": "2026-01-23T18:04:15.647345Z", "brief": {"why": "Wire presentations().get() + batch pages().getThumbnail() to extractor and workspace. Use timing from scripts/slides_timing.py as reference.", "what": "See title", "done": "When complete"}, "created_at": "2026-01-23T17:31:15.20764Z", "created_by": "spm1001", "order": 1, "waiting_for": null}
{"id": "mise-2bl", "title": "Port slides extractor", "type": "action", "status": "done", "done_at": "2026-01-23T17:26:59.130711Z", "brief": {"why": "Port slides.py extraction (658 lines). Structured text + speaker notes, visual flags.", "what": "## What to Port\n\nFrom v1 tools/slides.py:\n- Text extraction per slide\n- Speaker notes\n- Visual flags: [CHART], [IMAGE], [CONNECTOR], [SHAPE]\n- Detection: single_large_image, fragmented_text\n\n## NOT Porting (for now)\n\n- Thumbnail generation (~1s per thumbnail, bottleneck)\n- Can add later if needed\n- **When added: thumbnails MUST use batch API** — `new_batch_http_request()` for selected slides, not per-slide calls\n\n## Signature\n\nextract_slides(presentation_response: dict) -> str\n\n## Acceptance Criteria\n\n- [ ] extractors/slides.py exists\n- [ ] Text + speaker notes extracted\n- [ ] Visual flags present\n- [ ] No thumbnail dependency\n- [ ] Unit test with fixture passes", "done": "When complete"}, "created_at": "2026-01-23T07:24:35.46529Z", "created_by": "spm1001", "order": 1, "parent": "mise-52d", "waiting_for": null}
{"id": "mise-2h2", "title": "Handle rowSpan in table parsing", "type": "action", "status": "done", "done_at": "2026-01-24T22:06:37.079881Z", "brief": {"why": "Slides tables can have merged cells with rowSpan > 1. Currently only colSpan is handled. Without rowSpan handling, vertically merged cells produce silently wrong output — a 'silent killer' that corrupts data without warning.", "what": "See title", "done": "When complete"}, "created_at": "2026-01-23T17:31:16.750633Z", "created_by": "spm1001", "order": 1, "parent": "mise-52d", "waiting_for": null}
{"id": "mise-2jz", "title": "Save sanitized real doc fixture", "type": "action", "status": "done", "done_at": "2026-01-23T15:47:44.399211Z", "brief": {"why": "User's test doc (1z-nQAcKlgortu22NbcL8ov9EfshWKabCFON3h-KYruQ) has linked slides, linked charts, linked tables, multiple tabs. Perfect test case. Sanitize and save as fixtures/docs/real_multi_tab.json for regression testing.", "what": "See title", "done": "When complete"}, "created_at": "2026-01-23T14:50:13.983104Z", "created_by": "spm1001", "order": 1, "waiting_for": null}
{"id": "mise-356", "title": "Port docs extractor", "type": "action", "status": "done", "done_at": "2026-01-23T13:15:19.926008Z", "brief": {"why": "Port docs.py extraction logic (661 lines). Handles multi-tab, footnotes, link merging.", "what": "## What to Port\n\nFrom v1 tools/docs.py:\n- Multi-tab document handling\n- Footnote extraction\n- Hyperlink merging (adjacent runs)\n- Heading/list hierarchy\n- Table extraction\n\n## Signature\n\nextract_doc(doc_response: dict) -> str\n\nInput: Raw Docs API documents.get() response\nOutput: Markdown string\n\n## Acceptance Criteria\n\n- [ ] extractors/docs.py exists\n- [ ] Handles multi-tab with === Tab: Name === separators\n- [ ] Preserves footnotes, links, tables\n- [ ] Unit test with fixture passes", "done": "When complete"}, "created_at": "2026-01-23T07:24:20.419125Z", "created_by": "spm1001", "order": 1, "parent": "mise-52d", "waiting_for": null}
{"id": "mise-3ab", "title": "Wire create tool", "type": "action", "status": "done", "done_at": "2026-01-24T20:22:11.242796Z", "brief": {"why": "Create Google Doc/Sheet/Slides from markdown. Secondary priority.", "what": "## Signature\n\ncreate(content: str, title: str, doc_type: str = 'doc', folder_id: str = None) -> dict\n\n## Response Shape\n\n{\n  \"file_id\": \"...\",\n  \"url\": \"https://docs.google.com/...\",\n  \"title\": \"...\",\n  \"type\": \"doc|sheet|slides\"\n}\n\n## Current State\n\nserver.py:100 has:\n```python\n# TODO: Wire to tools/create.py\nreturn {\"status\": \"not_implemented\", \"title\": title}\n```\n\nNo tools/create.py exists yet.\n\n## Implementation Approach\n\n### 1. Drive Native Markdown Import (Docs)\n\nUse Drive's native markdown import (discovered via about.get):\n```python\nfile_metadata = {\n    'name': title,\n    'mimeType': 'application/vnd.google-apps.document',\n    'parents': [folder_id] if folder_id else []\n}\nmedia = MediaIoBaseUpload(\n    io.BytesIO(content.encode()),\n    mimetype='text/markdown'\n)\nfile = service.files().create(\n    body=file_metadata, \n    media_body=media,\n    fields='id,webViewLink'\n).execute()\n```\n\n### 2. Sheets from CSV/TSV\n\nFor doc_type='sheet':\n- Parse markdown tables → CSV\n- Upload with mimeType text/csv → converts to Sheet\n- Or: Create empty sheet, populate via Sheets API (more control)\n\n### 3. Slides from Markdown\n\nFor doc_type='slides':\n- Parse markdown headings as slides\n- Use Slides API to create presentation\n- Or: Create from template with placeholder replacement\n\n## File Structure\n\n```\ntools/\n  create.py         # Tool implementation\nadapters/\n  create.py         # Drive create operations (if needed)\n```\n\n## Acceptance Criteria\n\n- [ ] tools/create.py exists\n- [ ] server.py wires create() to tool\n- [ ] Creates Google Doc from markdown\n- [ ] Returns file_id and webViewLink\n- [ ] Optional folder_id parameter works\n- [ ] Error handling for invalid content\n- [ ] Integration test creates real doc", "done": "When complete"}, "created_at": "2026-01-23T07:25:22.84022Z", "created_by": "spm1001", "order": 1, "parent": "mise-52d", "waiting_for": null}
{"id": "mise-3uu", "title": "Confident deployability via comprehensive test coverage", "type": "outcome", "status": "done", "brief": {"why": "Testing infrastructure. Can refactor without fear, CI catches regressions, edge cases covered.", "what": "## Success Criteria\n\n- [ ] Integration tests run against real account\n- [ ] Negative path tests cover malformed input\n- [ ] Adapter mocking infrastructure enables unit testing\n- [ ] Real fixtures captured and sanitized\n- [ ] Warning behaviors tested systematically\n- [ ] Retry decorator behavior verified\n\n## Execution Order\n\n1. **mise-h6m** Adapter mocking infrastructure (enables all other tests)\n2. **mise-02b** Integration test with real account (baseline)\n3. **mise-eqi** Add negative path tests (resilience)\n4. **mise-bl9** Round-trip extractor tests with real fixtures\n5. **mise-5kj** Sanitized test fixtures\n6. **mise-679** Capture linked-content doc fixture\n7. **mise-8g5** Test warning behaviors systematically\n8. **mise-52d.1** Test retry decorator\n9. **mise-qa6** Test hybrid PDF threshold logic\n10. **mise-e5o** Create pre-exfil test fixture\n\n## Why This Matters\n\nWithout tests, every change is a gamble. With comprehensive coverage, can ship with confidence. Mocking infrastructure is the foundation.", "done": "When complete"}, "created_at": "2026-01-24T20:16:45.38323Z", "created_by": "spm1001", "order": 2, "done_at": "2026-02-09T11:56:38Z"}
{"id": "mise-3uu.1", "title": "Gmail body round-trip tests", "type": "action", "status": "done", "brief": {"why": "The real_gmail_thread fixture stores raw API format (base64 bodies in payload). conftest.py extracts headers but leaves body_text/body_html as None. We test header extraction but not actual body extraction on real data. Either: (1) enhance fixture loading to parse bodies, or (2) capture a new fixture with pre-extracted bodies.", "what": "See title", "done": "When complete"}, "created_at": "2026-01-25T17:43:01.90871Z", "created_by": "spm1001", "order": 1, "parent": "mise-3uu", "waiting_for": null, "tactical": {"steps": ["Check what existing adapter tests assert about bodies", "Wire parse_message_payload into real_gmail_thread conftest loader", "Add round-trip tests: real payload → _build_message → decoded bodies", "Add round-trip tests: decoded bodies → extract_message_content → content string", "Verify all existing tests still pass"], "current": 5}, "done_at": "2026-02-09T09:08:04Z"}
{"id": "mise-3uu.2", "title": "Adapter integration tests — verify retry wiring", "type": "action", "status": "done", "brief": {"why": "Current retry tests mock everything. No test verifies that adapters actually use @with_retry correctly. Add thin integration tests (can use mocked services) that verify: (1) an adapter function is decorated, (2) retryable errors trigger retry, (3) non-retryable errors fail immediately. Catches wiring bugs where decorator is forgotten or misconfigured.", "what": "See title", "done": "When complete"}, "created_at": "2026-01-25T17:43:03.471932Z", "created_by": "spm1001", "order": 2, "parent": "mise-3uu", "waiting_for": null, "done_at": "2026-02-09T07:05:05Z"}
{"id": "mise-3xr", "title": "Implement MCP Resources for self-documentation", "type": "action", "status": "done", "done_at": "2026-01-25T17:13:35.99957Z", "brief": {"why": "User expected auto-generated docs via MCP resources. CLAUDE.md says 'Documentation via MCP Resources, not a tool' but no resources are registered in server.py.", "what": "## What MCP Resources Are\nMCP Resources expose read-only content to Claude. Perfect for:\n- Tool documentation\n- Usage examples\n- API patterns\n\n## Implementation\n```python\n@mcp.resource(\"mise://docs/search\")\ndef search_docs():\n    return \"Search tool documentation...\"\n```\n\n## Auto-generation Idea\nCould generate resources from docstrings:\n- Extract tool docstrings at startup\n- Register as resources\n- Claude can read them on demand\n\n## Acceptance Criteria\n- [ ] At least one resource registered\n- [ ] Resource content is useful\n- [ ] Consider auto-generation from docstrings", "done": "When complete"}, "created_at": "2026-01-25T16:59:50.617989Z", "created_by": "spm1001", "order": 1, "waiting_for": null}
{"id": "mise-40q", "title": "Add unit tests for charts adapter", "type": "action", "status": "done", "brief": {"why": "Add mocked unit tests for adapters/charts.py. Currently only integration tested via test_chart_fetch.py. Should test: get_charts_from_spreadsheet() parsing, render_charts_as_pngs() with mocked Slides/Drive services, error handling (missing contentUrl, failed PNG fetch).", "what": "See title", "done": "When complete"}, "created_at": "2026-01-25T20:49:21.043712Z", "created_by": "spm1001", "order": 1, "waiting_for": null, "done_at": "2026-01-29T20:55:36Z"}
{"id": "mise-4h4", "title": "Extraction logic is testable and maintainable", "type": "outcome", "status": "done", "done_at": "2026-01-24T20:34:40.814005Z", "brief": {"why": "Architecture quality. Adapters are pure, can be unit tested without API calls. DRY, clean separation.", "what": "## Success Criteria\n\n- [ ] PDF extraction in adapters/pdf.py (not fetch.py)\n- [ ] Office extraction in adapters/office.py (not fetch.py)\n- [ ] Shared Drive conversion logic factored out\n- [ ] Retry decorator has no duplication\n- [ ] Each adapter testable with mocked responses\n\n## Execution Order\n\n1. **mise-k6g** Factor PDF extraction into adapter\n2. **mise-gtg** Factor Office extraction into adapter\n   (These share Drive conversion - do together, factor shared code)\n3. **mise-v79** Refactor retry decorator duplication\n\n## Why This Matters\n\nHardcoded logic in tools layer = untestable. Can't verify PDF threshold logic without calling real APIs. Clean adapters = confident refactoring.", "done": "When complete"}, "created_at": "2026-01-24T20:16:35.215186Z", "created_by": "spm1001", "order": 2}
{"id": "mise-4mj", "title": "Gmail attachment content in fetch", "type": "outcome", "status": "done", "brief": {"why": "Gmail threads have PDF/image attachments. Pre-exfil detection is wired in, but fetch doesn't yet surface the extracted content — it knows attachments exist in Drive but doesn't include them in the deposit.", "what": "1. Single-attachment fetch API (lahero) 2. Embed extracted PDF content in content.md (voSovu) 3. Integration test for attachment extraction (gifiku) 4. Investigate pre-exfil filename matching robustness (rosite)", "done": "Gmail fetch with PDF attachments deposits attachment content inline in content.md. Integration test proves the round-trip."}, "created_at": "2026-01-25T19:14:31.687721Z", "created_by": "spm1001", "order": 3, "done_at": "2026-02-09T06:40:09Z"}
{"id": "mise-4mj.1", "title": "Pre-exfil folder should be configurable", "type": "action", "status": "done", "brief": {"why": "Pre-exfil detection assumes folder named 'Email Attachments'. Should be configurable via env var or discover by name. If folder doesn't exist or can't be found, skip pre-exfil check gracefully.", "what": "## Config options\n1. MISE_EMAIL_ATTACHMENTS_FOLDER_ID env var (explicit)\n2. Search by name \"Email Attachments\" (auto-discover)\n3. Both: try env var first, fall back to name search\n\n## Graceful degradation\nIf folder not found → skip pre-exfil, download from Gmail\nDon't fail the fetch, just miss the optimization", "done": "When complete"}, "created_at": "2026-01-25T19:18:21.077203Z", "created_by": "spm1001", "order": 1, "parent": "mise-4mj", "waiting_for": null, "done_at": "2026-01-29T23:25:38Z"}
{"id": "mise-52d", "title": "mise-en-space MVP — search + fetch that works", "type": "outcome", "status": "done", "done_at": "2026-01-24T20:18:20.726508Z", "brief": {"why": "Opinionated Google Workspace MCP with 4 verbs: search, fetch, create, help. Filesystem-first, token-efficient, ergonomic for Claude.", "what": "## Core Opinions\n\n1. **4 verbs** — search, fetch, create, help (not 17 tools)\n2. **Filesystem-first** — fetch returns path, Claude reads with standard tools\n3. **Auto-detect IDs** — Gmail/Drive IDs detected, URLs converted\n4. **Always LLM-analysis** — no purpose parameter, always markdown/CSV\n5. **Internal batching** — Gmail batch API under the hood\n6. **Pre-exfil detection** — check \"Email Attachments\" folder first\n\n## Architecture\n\n```\nextractors/   Pure functions (API response → content)\nadapters/     Thin Google API wrappers\ntools/        MCP tool definitions\nworkspace/    File deposit management\n```\n\n## Acceptance Criteria\n\n- [ ] search(query, sources) returns metadata for Drive + Gmail\n- [ ] fetch(id) deposits file, returns path\n- [ ] create(content, title) makes Google Doc from markdown\n- [ ] help() self-documents\n- [ ] Pre-exfiltrated attachments detected and used\n- [ ] Can replace v1 in claude.json", "done": "When complete"}, "created_at": "2026-01-23T07:24:03.211479Z", "created_by": "spm1001", "order": 2}
{"id": "mise-52d.1", "title": "Test retry decorator", "type": "action", "status": "done", "done_at": "2026-01-25T17:38:13.043508Z", "brief": {"why": "Zero coverage on core infrastructure. Write tests for _get_http_status, _should_retry, _convert_to_mise_error, and the decorator itself with both sync and async functions.", "what": "See title", "done": "When complete"}, "created_at": "2026-01-23T09:03:34.812391Z", "created_by": "spm1001", "order": 1, "parent": "mise-52d", "waiting_for": null}
{"id": "mise-52d.2", "title": "Add Makefile with check target", "type": "action", "status": "open", "brief": {"why": "make check runs pytest + mypy in one command. Consolidates quality gates.", "what": "## What\n\nSingle command to run all quality gates:\n```bash\nmake check  # runs pytest + mypy\n```\n\n## Implementation\n\n```makefile\n.PHONY: check test lint\n\ncheck: test lint\n\ntest:\n\tuv run pytest tests/unit/ -q\n\nlint:\n\tuv run mypy models.py extractors/ adapters/ validation.py workspace/\n```\n\n## Acceptance Criteria\n\n- [ ] Makefile exists at repo root\n- [ ] `make check` runs tests and mypy\n- [ ] Exit code is non-zero if either fails", "done": "When complete"}, "created_at": "2026-01-23T09:03:46.345057Z", "created_by": "spm1001", "order": 2, "parent": "mise-52d", "waiting_for": null}
{"id": "mise-52d.3", "title": "Add search_threads to gmail adapter", "type": "action", "status": "done", "done_at": "2026-01-23T22:10:53.876656Z", "brief": {"why": "Gmail adapter has fetch_thread and fetch_message but no search. Needed for mise-e0e (Wire search tool).", "what": "See title", "done": "When complete"}, "created_at": "2026-01-23T21:59:43.543047Z", "created_by": "spm1001", "order": 3, "parent": "mise-52d", "waiting_for": null}
{"id": "mise-52d.4", "title": "Pre-exfil attachment detection", "type": "action", "status": "open", "brief": {"why": "When fetching Gmail threads with attachments, check Drive's 'Email Attachments' folder first. Use Drive copy if found (already indexed by fullText search).", "what": "## Why\n\nDrive indexes PDF content via fullText. Gmail doesn't search inside attachments.\nBackground extractor saves attachments to Drive → they become searchable.\nFetch should use the indexed copy when available.\n\n## Approach\n\n1. Config for Email Attachments folder ID (env var or discover by name)\n2. When processing Gmail attachment, query Drive:\n   `'{folder_id}' in parents and name = '{filename}'`\n3. If found, return Drive file_id instead of downloading from Gmail\n4. If not found, fall back to Gmail attachment download\n\n## Edge cases\n\n- Duplicate filenames (same name, different emails) — may need date matching\n- Stale copies (attachment updated, Drive copy old) — probably rare\n- Folder doesn't exist — graceful fallback\n\n## Acceptance Criteria\n\n- [ ] Looks up attachments in configured Drive folder\n- [ ] Uses Drive copy when found\n- [ ] Falls back to Gmail when not found\n- [ ] Works when folder not configured (skip lookup)", "done": "When complete"}, "created_at": "2026-01-23T22:20:19.27301Z", "created_by": "spm1001", "order": 4, "parent": "mise-52d", "waiting_for": null}
{"id": "mise-52d.5", "title": "Support PDF files in fetch", "type": "action", "status": "done", "done_at": "2026-01-24T17:11:55.843775Z", "brief": {"why": "fetch() should handle PDF files using markitdown for text extraction. Drive conversion doesn't help here (PDFs stay as PDFs).", "what": "## Approach\n\n1. Detect PDF MIME type in fetch_drive()\n2. Download file via Drive API\n3. Run through markitdown\n4. Deposit as markdown\n\n## Reference\n\nmarkitdown chosen over PyMuPDF: MIT licensed vs AGPL.\n\n## Acceptance Criteria\n\n- [ ] PDF files extract to markdown via markitdown\n- [ ] Handle multi-page PDFs\n- [ ] Graceful fallback for scanned/image PDFs\n- [ ] Integration test with real PDF", "done": "When complete"}, "created_at": "2026-01-23T23:08:29.79669Z", "created_by": "spm1001", "order": 5, "parent": "mise-52d", "waiting_for": null}
{"id": "mise-52d.6", "title": "Selective thumbnail fetching for slides", "type": "action", "status": "done", "done_at": "2026-01-24T22:21:48.938603Z", "brief": {"why": "Port v1's selective thumbnail logic. Skip stock photos and text-only slides. Reduces 43-slide deck from 20s to ~5s.", "what": "## Problem\n\nSequential thumbnail API calls: ~0.5s each (unavoidable, batch disabled).\n43-slide deck = 20+ seconds blocking.\n\n## v1 Logic (mcp-google-workspace)\n\nLocation: workspace_mcp/tools/slides.py lines 209-239, 562-589\n\nSkip thumbnails when:\n- single_large_image: >50% slide coverage = stock photo\n- text_only: No images, just text shapes\n- empty: No content\n\nOnly thumbnail:\n- Charts, diagrams\n- Fragmented layouts (multiple images/shapes)\n- Complex visual content\n\n## Approach\n\n1. Analyze slide content before fetching thumbnails\n2. Classify each slide: needs_thumbnail | skip\n3. Only fetch for needs_thumbnail slides\n4. Include skip reason in manifest\n\n## Acceptance Criteria\n\n- [ ] Classify slides by visual complexity\n- [ ] Skip single_large_image slides\n- [ ] Skip text_only slides\n- [ ] Manifest shows which slides were skipped and why\n- [ ] Timing improvement measurable (scripts/slides_timing.py)", "done": "When complete"}, "created_at": "2026-01-23T23:08:42.923276Z", "created_by": "spm1001", "order": 6, "parent": "mise-52d", "waiting_for": null}
{"id": "mise-52d.7", "title": "Support Office files in fetch (DOCX, XLSX, PPTX)", "type": "action", "status": "done", "done_at": "2026-01-24T17:24:43.904977Z", "brief": {"why": "fetch() should handle Office files using Drive API conversion (not markitdown). Bakeoff showed Drive conversion is 29% smaller and cleaner.", "what": "## Approach\n\n1. Detect Office MIME types in fetch_drive()\n2. Use Drive API export to convert:\n   - DOCX → text/markdown or text/plain\n   - XLSX → text/csv\n   - PPTX → text/plain (or markdown if available)\n3. Deposit converted content\n\n## Why Drive conversion?\n\nBakeoff (Jan 2026): Drive conversion beats markitdown for Office files.\n- 29% smaller output\n- Cleaner formatting\n- No external dependency\n\n## Acceptance Criteria\n\n- [ ] DOCX files convert via Drive API\n- [ ] XLSX files convert to CSV\n- [ ] PPTX files convert to text\n- [ ] Integration test for each type", "done": "When complete"}, "created_at": "2026-01-24T10:50:23.340289Z", "created_by": "spm1001", "order": 7, "parent": "mise-52d", "waiting_for": null}
{"id": "mise-52d.8", "title": "Support video/audio summaries in fetch", "type": "action", "status": "done", "done_at": "2026-01-24T11:18:30.527307Z", "brief": {"why": "fetch() should return Google's pre-computed AI summaries for video/audio files, rather than just the raw file. Leverages GenAI API discovered via reverse engineering.", "what": "## Approach\n\nUse Google's internal GenAI API (appsgenaiserver-pa.clients6.google.com) to get:\n- AI-generated summary\n- Transcript snippets\n- Suggested prompts\n\n## Reference\n\nFull implementation: ~/Repos/skill-chrome-log/references/DRIVE_VIDEO_SUMMARY_HOWTO.md\n\n## Open Questions\n\n1. **SAPISID cookie**: The API requires browser cookies, not OAuth tokens.\n   Options: webctl, manual, or find alternative auth path.\n\n2. **timedtext endpoint**: Separate /timedtext?fmt=json3 endpoint exists for\n   raw transcripts. May not be needed if summary API returns transcript snippets.\n\n## Acceptance Criteria\n\n- [ ] Solve SAPISID authentication problem\n- [ ] Call GenAI streamGenerate endpoint\n- [ ] Extract summary text from streaming response\n- [ ] Deposit summary + transcript to mise-fetch/\n- [ ] Handle videos without summaries gracefully", "done": "When complete"}, "created_at": "2026-01-24T10:50:24.743143Z", "created_by": "spm1001", "order": 8, "parent": "mise-52d", "waiting_for": null}
{"id": "mise-5kj", "title": "Sanitized test fixtures", "type": "action", "status": "open", "brief": {"why": "Test fixtures contain real data (ITV budget figures, email addresses). Create sanitized versions with fake data that preserves structure. Required before open-sourcing or sharing.", "what": "See title", "done": "When complete"}, "created_at": "2026-01-23T18:04:02.022456Z", "created_by": "spm1001", "order": 1, "parent": "mise-52d", "waiting_for": null}
{"id": "mise-626", "title": "Verify linked-content fixture has edge cases", "type": "action", "status": "done", "done_at": "2026-01-25T18:22:19.64487Z", "brief": {"why": "Doc 1bREiVmvgSsRKJLjamTOE0Wasq1ze7R3bIPOdtJMomKM should contain linked slides, linked charts, linked tables. Fetch it and confirm the edge cases exist before relying on it for testing. URL from user: https://docs.google.com/document/d/1bREiVmvgSsRKJLjamTOE0Wasq1ze7R3bIPOdtJMomKM", "what": "See title", "done": "When complete"}, "created_at": "2026-01-23T18:07:57.340642Z", "created_by": "spm1001", "order": 1, "parent": "mise-3uu", "waiting_for": null}
{"id": "mise-679", "title": "Capture linked-content doc fixture", "type": "action", "status": "done", "done_at": "2026-01-25T18:22:20.567657Z", "brief": {"why": "Capture fixture from https://docs.google.com/document/d/1bREiVmvgSsRKJLjamTOE0Wasq1ze7R3bIPOdtJMomKM — contains linked slides, linked charts, linked tables. Test that extractor handles linkedContentReference edge cases correctly.", "what": "## Approach\n\n1. Verify the doc still exists and has linked content\n2. Add to TEST_IDS in capture_fixtures.py\n3. Capture as fixtures/docs/real_linked_content.json\n4. Add test that verifies linked chart/slide placeholders appear\n\n## Why this matters\nCurrent real_multi_tab.json may not have linked content. The docs extractor has special handling for linkedContentReference that needs real-world testing.", "done": "When complete"}, "created_at": "2026-01-23T15:49:05.83736Z", "created_by": "spm1001", "order": 1, "parent": "mise-52d", "waiting_for": null}
{"id": "mise-6bo", "title": "Workspace manager — file deposit", "type": "action", "status": "done", "done_at": "2026-01-23T22:28:29.949174Z", "brief": {"why": "Manages ~/.mcp-workspace/ folder structure. Creates paths, cleans temp, returns paths.", "what": "## Structure\n\n~/.mcp-workspace/\n├── [account@domain.com]/\n│   ├── drive/{fileId}.md\n│   ├── gmail/{threadId}.txt\n│   └── attachments/\n└── temp/  (auto-cleanup on startup)\n\n## Functions\n\nworkspace/manager.py:\n- get_account_folder(account) -> Path\n- get_file_path(account, source, id, ext) -> Path\n- write_content(path, content) -> Path\n- cleanup_temp()\n- init() — called on server start\n\n## Acceptance Criteria\n\n- [ ] Creates folder structure on first use\n- [ ] Returns paths (never content)\n- [ ] Cleans temp/ on startup\n- [ ] Handles multiple accounts (future-proof)", "done": "When complete"}, "created_at": "2026-01-23T07:24:52.622867Z", "created_by": "spm1001", "order": 1, "parent": "mise-52d", "waiting_for": null}
{"id": "mise-6fk", "title": "Gmail search format=full may be too heavy", "type": "action", "status": "done", "brief": {"why": "Changed Gmail search from format=metadata to format=full to get attachment names. This fetches entire message bodies just to extract attachment metadata. For threads with large attachments, this bloats memory. Consider: format=metadata + separate attachment query, or accept the tradeoff.", "what": "## Options\n1. Keep format=full — simpler, but heavier payloads\n2. Use format=metadata, then batch-fetch just the parts with attachments\n3. Accept the tradeoff and document it\n\n## Measurement needed\n- Search latency before/after change\n- Memory usage for 20 results with large attachments", "done": "When complete"}, "created_at": "2026-01-25T19:18:08.993636Z", "created_by": "spm1001", "order": 1, "parent": "mise-4mj", "waiting_for": null, "done_at": "2026-02-08T18:28:06Z"}
{"id": "mise-6kh", "title": "Replaced v1 MCP with mise-en-space in production", "type": "outcome", "status": "done", "brief": {"why": "Migrated from beads", "what": "## What Success Looks Like\n\nClaude Code uses mise-en-space instead of mcp-google-workspace. Real workflows work: search → triage → fetch → read. No regressions from v1.\n\n## Acceptance Criteria\n\n- [ ] mise-en-space registered in claude.json\n- [ ] Search deposits to file (token efficient)\n- [ ] Sheets bug fixed (chart sheets filtered)\n- [ ] MCP resources verified in real session\n- [ ] Real workflow tested end-to-end\n\n## Children\n\n- mise-br7: Replace v1 in claude.json (final step)\n- mise-lft: Search deposits to file\n- mise-bpm: Sheets chart sheet bug\n- mise-k80: Test MCP resources\n\n## Why This Matters\n\nUntil v1 is replaced, mise-en-space is a parallel experiment. This epic gates the actual ship.", "done": "When complete"}, "created_at": "2026-01-25T19:48:54.458432Z", "created_by": "spm1001", "order": 2, "done_at": "2026-02-01T10:58:22Z"}
{"id": "mise-6ku", "title": "Claude can search, fetch, AND create across all Workspace content", "type": "outcome", "status": "done", "done_at": "2026-01-25T19:48:43.782962Z", "brief": {"why": "Complete the MCP tool surface. All 3 verbs (search, fetch, create) work across all content types (Drive, Gmail, Contacts).", "what": "## Success Criteria\n\n- [ ] `search(source=\"drive\")` returns file metadata\n- [ ] `search(source=\"gmail\")` returns thread metadata  \n- [ ] `search(source=\"contacts\")` returns people\n- [ ] `fetch(file_id)` works for all supported types\n- [ ] `create(content, type=\"doc\")` creates Google Doc\n- [ ] `create(content, type=\"sheet\")` creates Google Sheet\n- [ ] MCP resources expose capabilities documentation\n\n## Execution Order\n\n1. **mise-3ab** Wire create tool (highest value gap)\n2. **mise-bmk** Implement contacts search (completes search)\n3. **mise-tld** Wire resources (self-documenting)\n\n## Why This Matters\n\nWithout create, Claude can only read. With all 3 verbs, Claude becomes a full Workspace participant — can research, synthesize, AND produce artifacts.", "done": "When complete"}, "created_at": "2026-01-24T20:16:18.445658Z", "created_by": "spm1001", "order": 2}
{"id": "mise-6zd", "title": "Fix silent thumbnail failures in slides adapter", "type": "action", "status": "done", "done_at": "2026-01-25T15:25:55.720724Z", "brief": {"why": "Slides adapter silently swallows all thumbnail fetch exceptions. Missing thumbnails not reported to user.", "what": "## Problem\n\nIn adapters/slides.py `_fetch_thumbnails()`:\n```python\nexcept Exception:\n    pass  # Silent failure - line 112\n```\n\nWhen a thumbnail fails to fetch:\n- No warning added to result\n- No logging\n- User doesn't know slide N is missing\n- Manifest shows has_thumbnails=true even with gaps\n\n## Impact\n\n- Debugging is hard (why is slide 7 missing?)\n- User may think extraction succeeded fully\n- Inconsistent with other extractors (which populate warnings)\n\n## Approach\n\n1. Catch specific exceptions:\n   - HttpError 403: permission denied\n   - HttpError 404: thumbnail not available\n   - Timeout: slow network\n   \n2. Populate warnings for failures:\n   ```python\n   warnings.append(f\"Slide {i}: thumbnail unavailable ({error_type})\")\n   ```\n\n3. Track failed slides in manifest:\n   ```json\n   {\n     \"has_thumbnails\": true,\n     \"thumbnail_failures\": [7, 12]\n   }\n   ```\n\n4. Consider retry for transient failures (already have retry decorator)\n\n## Acceptance Criteria\n\n- [ ] Thumbnail failures logged with reason\n- [ ] warnings list populated for failed slides\n- [ ] Manifest includes thumbnail_failures array\n- [ ] Test with mock that simulates failure\n- [ ] Pass is replaced with explicit handling", "done": "When complete"}, "created_at": "2026-01-24T20:09:32.338581Z", "created_by": "spm1001", "order": 1, "parent": "mise-52d", "waiting_for": null}
{"id": "mise-8g5", "title": "Test warning behaviors systematically", "type": "action", "status": "done", "done_at": "2026-01-25T17:36:55.159627Z", "brief": {"why": "Warnings implementation lacks dedicated tests. Need coverage for: empty sheets trigger warning, unknown element types trigger warning, HTML conversion fallback triggers warning, truncation triggers warning. Currently behavior exists but is undertested.", "what": "See title", "done": "When complete"}, "created_at": "2026-01-23T18:07:56.044601Z", "created_by": "spm1001", "order": 1, "parent": "mise-52d", "waiting_for": null}
{"id": "mise-8v5", "title": "Find Gemini video summary API endpoint", "type": "action", "status": "done", "done_at": "2026-01-24T11:18:20.078483Z", "brief": {"why": "The Gemini side panel in Drive shows AI summaries of videos — more useful than raw transcript. Need to find the API endpoint that powers this.", "what": "## Context\n\nThe transcript (/timedtext) is raw ASR output. The Gemini summary is processed/distilled — includes:\n- Summary of key points\n- Action items\n- Key topics\n\n## Approach\n\n1. Open video in Drive with Gemini panel visible\n2. Use network intercept to capture requests when panel loads\n3. Look for endpoints containing: gemini, gen_lang, ent-assist, aiplatform\n4. The summary might be streamed (chunked response)\n\n## Why This Matters\n\nFor a sous-chef MCP, the summary is much more valuable than transcript:\n- Token-efficient (distilled, not verbatim)\n- Structured (topics, actions)\n- LLM-ready", "done": "When complete"}, "created_at": "2026-01-24T08:54:00.309936Z", "created_by": "spm1001", "order": 1, "waiting_for": null}
{"id": "mise-9az", "title": "Test infrastructure: sanitized fixtures + adapter mocking", "type": "action", "status": "done", "done_at": "2026-01-23T18:04:14.690475Z", "brief": {"why": "Integration tests use real MIT budget data and hit real APIs. Need: (1) sanitized test fixtures with fake data, (2) mocking infrastructure so adapter unit tests don't need real credentials/APIs.", "what": "## Problem\n\n1. **Real data in fixtures** — test spreadsheet contains actual MIT budget data. If repo goes public or fixtures are shared, data leaks.\n\n2. **No mocking** — adapter tests hit real APIs. Slow, quota-consuming, fragile (test spreadsheet could be deleted).\n\n## Approach\n\n### Sanitized Fixtures\n- Create fake spreadsheet data that exercises same edge cases (multi-tab, special chars, empty cells)\n- Store as JSON in fixtures/sheets/\n- Consider: should integration tests use fixtures or real API? Maybe both (unit uses fixtures, integration uses real)\n\n### Adapter Mocking\n- Mock the Google API service objects\n- Adapters receive mock service, return canned responses\n- Unit tests: fast, no credentials needed\n- Integration tests: real API, needs credentials\n\n## Questions to Resolve\n- Where do mocks live? `tests/mocks/`?\n- Do we use pytest fixtures or a mocking library (unittest.mock, responses)?\n- Should adapters accept service as parameter (dependency injection) for easier mocking?\n\n## Acceptance Criteria\n- [ ] No real company data in committed fixtures\n- [ ] Adapter unit tests run without credentials\n- [ ] Integration tests still work with real API\n- [ ] Clear separation: unit (mocked) vs integration (real)", "done": "When complete"}, "created_at": "2026-01-23T12:27:46.778517Z", "created_by": "spm1001", "order": 1, "waiting_for": null}
{"id": "mise-9jl", "title": "Deposit folder cleanup strategy", "type": "action", "status": "open", "brief": {"why": "mise-fetch/ folders accumulate over time with no cleanup. Need TTL-based or manual cleanup.", "what": "## Problem\n\nEvery fetch creates a folder in mise-fetch/:\n```\nmise-fetch/\n├── slides--deck-1--abc123/\n├── doc--notes--def456/\n├── ...hundreds more over time\n```\n\nNo cleanup = unbounded growth.\n\n## Options\n\n### Option A: TTL-based auto-cleanup\n```python\ndef cleanup_old_deposits(max_age_days: int = 7):\n    \"\"\"Delete deposits older than N days.\"\"\"\n```\n\nPros: Automatic, no user action\nCons: May delete something user wants\n\n### Option B: Manual cleanup command\n```bash\n# MCP tool\nmise cleanup --older-than 7d\n\n# Or expose in manifest\n\"cleanup_hint\": \"safe to delete after 2026-01-31\"\n```\n\nPros: User controls\nCons: Requires user action\n\n### Option C: Versioned deposits\n```\nmise-fetch/\n├── slides--deck-1--abc123--20260124T1200/\n├── slides--deck-1--abc123--20260125T0900/\n```\n\nPros: History preserved\nCons: Even more growth\n\n## Recommendation\n\nStart with Option B (manual) + manifest hint:\n1. Add `cleanup_after` timestamp to manifest.json\n2. Document cleanup approach\n3. Consider auto-cleanup later based on usage\n\n## Acceptance Criteria\n\n- [ ] manifest.json includes cleanup_after field\n- [ ] workspace/manager.py has cleanup function\n- [ ] Documentation explains cleanup\n- [ ] Test cleanup with mock deposits", "done": "When complete"}, "created_at": "2026-01-24T20:10:12.905212Z", "created_by": "spm1001", "order": 1, "parent": "mise-52d", "waiting_for": null}
{"id": "mise-9t8", "title": "Fix streaming fallback memory issue for large PDFs", "type": "action", "status": "done", "done_at": "2026-01-25T17:54:39.602587Z", "brief": {"why": "Large PDF Drive conversion still loads entire file into memory after streaming download. A 500MB PDF streams to disk, then OOMs during conversion.", "what": "## Problem\n\n`_fetch_and_extract_pdf_large()` in adapters/pdf.py:\n1. Streams download to temp file (good)\n2. Tries markitdown from disk (good)\n3. If markitdown fails, reads entire file into memory for Drive conversion (bad)\n\n```python\npdf_bytes = tmp_path.read_bytes()  # <-- defeats streaming purpose\nconversion_result = convert_via_drive(file_bytes=pdf_bytes, ...)\n```\n\n## Approach\n\nOption 1: Stream temp file directly to Drive upload\n- `MediaFileUpload` instead of `MediaInMemoryUpload`\n- Requires refactoring conversion.py\n\nOption 2: For large files, skip Drive fallback entirely\n- Document limitation: \"PDFs >X MB use markitdown only\"\n- Simple but reduces extraction quality\n\n## Acceptance Criteria\n- [ ] 500MB PDF doesn't OOM during Drive fallback\n- [ ] Or: Large PDF fallback is gracefully disabled with warning", "done": "When complete"}, "created_at": "2026-01-25T17:28:38.500682Z", "created_by": "spm1001", "order": 1, "waiting_for": null}
{"id": "mise-Bivulu", "type": "outcome", "title": "Create purpose-built test doc with rich comments", "brief": {"why": "Current test_doc_with_comments_id reuses test_doc_id — works but lacks dedicated edge cases", "what": "Create Google Doc in test folder with: threaded replies, resolved/unresolved mix, @mentions, long anchor quotes, empty anchors (like DOCX)", "done": "Fixture exists with predictable comment patterns for regression testing"}, "status": "done", "order": 17, "created_at": "2026-01-31T18:30:00Z", "created_by": "spm1001", "done_at": "2026-02-01T10:58:31Z"}
{"id": "mise-BoVuMo", "type": "action", "title": "Investigate wrong project folder paths in test Claudes", "brief": {"why": "During skill-forge subagent testing (late Jan 2026), test Claudes deposited files to wrong locations — mise-fetch/ appeared in the MCP server's cwd instead of the test project's cwd. Likely the known base_path/MCP-cwd scoping issue documented in CLAUDE.md, but not confirmed.", "what": "1. Reproduce wrong-path issue in a skill-forge test run 2. Identify root cause (base_path, cwd, MCP scoping) 3. Fix or document the correct pattern", "done": "Test Claudes deposit to the expected location, root cause documented"}, "status": "open", "parent": "mise-jy3", "order": 14, "created_at": "2026-01-31T22:07:47Z", "created_by": "spm1001", "waiting_for": null, "updated_at": "2026-02-15T19:32:08Z"}
{"id": "mise-DiCewi", "type": "outcome", "title": "Test and exercise new API services", "brief": {"why": "Activity, Tasks, Calendar, Labels APIs added but not yet used in tool layer", "what": "Build search_activity tool using Activity API, test Tasks/Calendar integration, explore Labels for organizational metadata", "done": "At least one new tool using the expanded services, with integration test"}, "status": "done", "order": 18, "created_at": "2026-01-31T18:30:07Z", "created_by": "spm1001", "done_at": "2026-02-01T10:58:31Z"}
{"id": "mise-DiZaje", "type": "action", "title": "Activity fixture capture", "brief": {"why": "Activity adapter unit tests only cover models, not parsing logic. Need fixtures/activity/ with real API responses to test _parse_actor, _parse_target, _parse_comment_action.", "what": "Add capture_activity_fixtures() to scripts/capture_fixtures.py. Capture sample comment activities and file activities. Sanitize email addresses.", "done": "fixtures/activity/comment_activities.json and file_activities.json exist with realistic test data"}, "status": "done", "parent": "mise-3uu", "order": 6, "created_at": "2026-02-01T10:59:20Z", "created_by": "spm1001", "waiting_for": null, "tactical": {"steps": ["Read existing capture script and activity adapter to understand API shape", "Add capture_activity_fixtures() to capture_fixtures.py", "Capture real activity data and sanitize", "Verify fixtures load and are realistic"], "current": 4}, "done_at": "2026-02-09T10:04:02Z"}
{"id": "mise-FutuHe", "type": "action", "title": "Distill bakeoff findings into README", "brief": {"why": "Bakeoff comparison docs (Official vs Organic MCP) are sitting in Test Fixtures. Public repo needs a compelling README showing what mise-en-space does and why.", "what": "Read the 4 bakeoff gdocs, synthesize key findings, write README.md for the public repo", "done": "README.md exists with architecture summary, usage examples, and bakeoff learnings"}, "status": "done", "order": 3, "created_at": "2026-02-01T09:41:55Z", "created_by": "spm1001", "parent": "mise-naviho", "waiting_for": null, "tactical": {"steps": ["Read bakeoff docs", "Draft README", "Review and tighten"], "current": 3}, "done_at": "2026-02-07T21:17:04Z"}
{"id": "mise-GenoSi", "type": "outcome", "title": "Activity API integration tests", "brief": {"why": "Activity adapter has no real API tests — only model unit tests. Need to verify retry wiring, pagination behavior, and error handling against real API.", "what": "Create tests/integration/test_activity.py with tests for search_comment_activities and get_file_activities. Use real credentials, mark with @pytest.mark.integration.", "done": "Integration tests exist, can run with uv run pytest -m integration"}, "status": "done", "order": 15, "created_at": "2026-01-31T17:58:13Z", "created_by": "spm1001", "done_at": "2026-02-01T10:58:31Z"}
{"id": "mise-HoWeKe", "type": "action", "title": "Test and exercise new API services", "brief": {"why": "Activity, Tasks, Calendar, Labels APIs added but not yet used in tool layer", "what": "Build search_activity tool using Activity API, test Tasks/Calendar integration, explore Labels for organizational metadata", "done": "At least one new tool using the expanded services, with integration test"}, "status": "open", "parent": "mise-kecigu", "order": 4, "created_at": "2026-02-01T10:59:22Z", "created_by": "spm1001", "waiting_for": null, "updated_at": "2026-02-15T19:26:58Z"}
{"id": "mise-Katuwe", "type": "action", "title": "Add mise-fetch cleanup mechanism", "brief": {"why": "Deposited files in mise-fetch/ accumulate indefinitely. No automatic cleanup.", "what": "Per-directory TTL cleanup. With base_path, deposits scatter across project directories — each mise-fetch/ is self-contained, so cleanup is local not centralised. Options: (1) TTL flag on fetch (--cleanup-older-than 7d) (2) standalone 'do(operation=cleanup)' via the do verb (3) external cron (find ~/Repos/*/mise-fetch -mtime +7). Workspace manager already has list_deposit_folders() + parse_folder_name() infrastructure.", "done": "Cleanup mechanism implemented and tested"}, "status": "done", "parent": "mise-jy3", "order": 4, "created_at": "2026-02-01T10:59:44Z", "created_by": "spm1001", "waiting_for": null, "done_at": "2026-02-09T23:15:31Z"}
{"id": "mise-KewiCi", "type": "outcome", "title": "Phase 2: Cookie authentication for web fetch", "brief": {"why": "Authenticated sites (docs behind login, corporate wikis) need cookie injection. CDP adapter already exists for video summaries.", "what": "Integrate CDP cookie exfiltration with web adapter. Add cookie forwarding when auth required. Test with authenticated site.", "done": "fetch() works on auth-required pages when chrome-debug running"}, "status": "done", "order": 24, "created_at": "2026-02-01T15:23:07Z", "created_by": "spm1001", "done_at": "2026-02-06T20:11:52Z"}
{"id": "mise-Lovobo", "type": "outcome", "title": "Integration test for Gmail attachment extraction", "brief": {"why": "Current tests are unit-level; need to verify download → extract → deposit flow with real thread", "what": "Create test fixture with PDF attachment, write integration test that fetches thread and verifies PDF content is deposited", "done": "Integration test passes with real Gmail API call"}, "status": "done", "order": 9, "created_at": "2026-01-29T23:25:53Z", "created_by": "spm1001", "done_at": "2026-02-01T10:58:29Z"}
{"id": "mise-NiKuki", "type": "action", "title": "Surface action items from Workspace APIs", "brief": {"why": "Action items scattered across 3 sources: (1) Tasks API, (2) Activity API (comments mentioning you), (3) Assigned tasks in Docs — buried with no central view.", "what": "1. Explore all 3 sources for action item surfacing 2. Tasks API → sync to todoist-gtd or just surface? 3. Activity API → filter for mentions, extract action items 4. Docs assigned tasks → can we find these? 5. Decide: sous-chef surfaces, pipes to todoist-gtd, or both", "done": "Clear decision on each source: ignore, surface via fetch enrichment, or pipe to todoist-gtd"}, "status": "open", "parent": "mise-kecigu", "order": 3, "created_at": "2026-02-01T10:59:43Z", "created_by": "spm1001", "waiting_for": null, "updated_at": "2026-02-15T19:26:57Z"}
{"id": "mise-PepuZa", "type": "outcome", "title": "Wire pre-exfil lookup in fetch_gmail", "brief": {"why": "Pre-exfil optimization skips Gmail download for already-indexed files", "what": "Call lookup_exfiltrated() in fetch_gmail before downloading attachments. If file exists in Drive with Message ID match, fetch from Drive instead of Gmail.", "done": "fetch_gmail checks pre-exfil folder, uses Drive file when available"}, "status": "done", "order": 8, "created_at": "2026-01-29T23:25:48Z", "created_by": "spm1001", "done_at": "2026-02-01T10:58:29Z"}
{"id": "mise-Recebe", "type": "action", "title": "Parameterize Apps Script year functions", "brief": {"why": "backfillDriveLinks2023/2024/2025 are copy-paste boilerplate. Adding 2026 required code edits. Should be config-driven like attachment backfill.", "what": "1. Add years to BACKFILL_YEARS config array 2. Single backfillDriveLinksForYear(year) function 3. Remove year-specific boilerplate 4. Update any triggers", "done": "Adding a new year is config change only, no code duplication"}, "status": "open", "parent": "mise-tagemu", "order": 2, "created_at": "2026-01-29T22:34:56Z", "created_by": "spm1001", "waiting_for": null}
{"id": "mise-Sadulo", "type": "outcome", "title": "Calendar context in search results", "brief": {"why": "Docs don't exist in isolation. Meeting context explains why a doc matters — who was in the meeting, when it happened, what other docs were linked.", "what": "1. Add source='calendar' option to search, or enrich Drive results with calendar context\n2. Query calendar.events() for recent events with attachments\n3. Cross-reference file IDs with search results\n4. Add meeting context to result metadata: 'This doc was attached to Q4 Planning meeting on Jan 15'", "done": "Search results include meeting context when available"}, "status": "done", "order": 14, "created_at": "2026-01-31T17:49:13Z", "created_by": "spm1001", "done_at": "2026-02-01T10:58:33Z"}
{"id": "mise-Sojuci", "type": "outcome", "title": "Add mise-fetch cleanup mechanism", "brief": {"why": "Deposited files in mise-fetch/ accumulate indefinitely. No automatic cleanup.", "what": "Options: (1) age-based cleanup in workspace/manager.py (2) max folder count (3) manual cleanup skill/command", "done": "Cleanup mechanism implemented and tested"}, "status": "done", "order": 21, "created_at": "2026-01-31T20:49:46Z", "created_by": "spm1001", "done_at": "2026-02-01T10:58:33Z"}
{"id": "mise-Tojoni", "type": "action", "title": "Probe: does mise need a coupled skill?", "brief": {"why": "v1 had a detailed skill; v2 is simpler. Test MCP-only for a week and note friction points.", "what": "Evidence gathered from mem (15+ sessions), field reports, handoffs, and 4 exemplar skill+tool pairings. Findings: (1) Test Claudes skip skill and make mistakes — keyword soup, miss comments, forget base_path. (2) MANDATORY gate (CSO 80) needed for skill to load. (3) Skill value concentrates in research mode (exploration loop), low for simple fetch. (4) Design: MANDATORY, ~150 line SKILL.md coaching research workflow, references/ for mechanical detail. Decision: yes, mise needs coupled skill.", "done": "Decision made: no skill needed / create minimal skill / port v1 skill"}, "status": "done", "parent": "mise-tuguzi", "order": 2, "created_at": "2026-02-01T10:59:54Z", "created_by": "spm1001", "waiting_for": null, "done_at": "2026-02-09T15:16:58Z"}
{"id": "mise-Vujali", "type": "action", "title": "Deprecate google-workspace skill", "brief": {"why": "Two Workspace skills exist: google-workspace (v1 MCP) and mise (v2 MCP). This causes confusion about which to use.", "what": "1. Add deprecation notice to google-workspace SKILL.md pointing to mise 2. Update filing skill references 3. Consider removing after transition period", "done": "google-workspace skill marked deprecated; references updated to mise"}, "status": "done", "parent": "mise-tuguzi", "order": 1, "created_at": "2026-01-31T22:07:29Z", "created_by": "spm1001", "waiting_for": null, "done_at": "2026-02-09T15:17:26Z"}
{"id": "mise-Zoluca", "type": "outcome", "title": "Probe: does mise need a coupled skill?", "brief": {"why": "v1 had a detailed skill; v2 is simpler. Test MCP-only for a week and note friction points.", "what": "Use mise tools directly for a week. Track: (1) What patterns repeat? (2) What context is missing? (3) Would a skill help?", "done": "Decision made: no skill needed / create minimal skill / port v1 skill"}, "status": "done", "order": 20, "created_at": "2026-01-31T20:49:39Z", "created_by": "spm1001", "done_at": "2026-02-01T10:58:34Z"}
{"id": "mise-baJije", "type": "action", "title": "Replace v1 in claude.json", "brief": {"why": "Final step to ship mise-en-space. v1 still registered, this swaps it.", "what": "1. Update ~/.claude.json mcpServers 2. Point to mise-en-space server.py 3. Test real Claude session 4. Keep v1 commented for rollback", "done": "mise-en-space registered, Claude can call search/fetch/create, real workflow tested"}, "status": "done", "parent": "mise-6kh", "order": 1, "created_at": "2026-01-29T22:14:17Z", "created_by": "spm1001", "waiting_for": null, "done_at": "2026-01-31T20:50:04Z"}
{"id": "mise-beriji", "type": "outcome", "title": "Callers can create Google Sheets from deposited data", "brief": {"why": "The MCP can fetch and search Sheets but can't create them. Callers (Claudes) generate tabular data from queries, transformations, and analysis — that data sits on disk with nowhere to go. The deposit-then-publish pattern (mise-do/) keeps the context window lean and gives humans an inspection checkpoint before publishing.", "what": "1. Benchmark creation paths (done — Drive CSV upload wins decisively) 2. mise-do/ directory convention and manifest schema 3. CSV tick-prefix for text columns (leading zeros) 4. Wire do(create, sheet) via Drive CSV upload — ~10 lines, same pattern as doc creation 5. Post-creation manifest enrichment", "done": "do(operation=create, doc_type=sheet, source=mise-do/...) creates a Google Sheet from deposited CSV via Drive import. Type detection delegated to Google (94% accuracy). Manifest enriched as receipt. Tick-prefix handles leading-zero edge case."}, "status": "done", "order": 40, "created_at": "2026-02-16T13:28:33Z", "created_by": "spm1001", "updated_at": "2026-02-16T14:19:36Z", "done_at": "2026-02-18T14:17:03Z"}
{"id": "mise-bicesi", "type": "action", "title": "do() operations return consistent response shape", "brief": {"why": "CreateResult is a dataclass with retrofitted cues dict. do_move returns a raw dict. As more operations land, inconsistency makes it harder to know which pattern to follow.", "what": "1. Decide: dataclass per operation vs unified DoResult vs raw dicts everywhere 2. Implement chosen pattern 3. Update tests", "done": "All do() operations follow the same return pattern — future operations have a clear template to copy"}, "status": "done", "parent": "mise-hijute", "order": 9, "created_at": "2026-02-15T22:37:52Z", "created_by": "spm1001", "waiting_for": null, "tactical": {"steps": ["Normalise create to return raw dict with 'operation' key, drop CreateResult dataclass", "Add 'operation' to create response, keep 'type' for backwards compat", "Update server.py — remove .to_dict() call on create path", "Update tests for new create response shape", "Verify all 6 operations share: file_id, title, web_link, operation, cues"], "current": 5, "session": "/home/modha/Repos/mise-en-space"}, "updated_at": "2026-02-18T21:47:35Z", "done_at": "2026-02-18T21:47:35Z"}
{"id": "mise-biruri", "type": "outcome", "title": "Review mise passe bons given new passe outcomes", "brief": {"why": "Passe refactored its outcomes (Feb 2026). mise has 4 bons referencing passe: sipefe (403 fallback), peneku (login form detection), pufuti (paywall branch test), cifike (mock audit). These may need updating given passe's new error handling, shared session, and touch interaction outcomes.", "what": "1. Read sipefe, peneku, pufuti, cifike briefs 2. Check if any are addressed by passe-baboki (error handling) or passe-rulete (shared session) 3. Update or close as appropriate", "done": "All mise bons referencing passe are current and aligned with passe's refactored outcomes"}, "status": "open", "order": 44, "created_at": "2026-02-18T20:58:48Z", "created_by": "spm1001"}
{"id": "mise-bituzo", "type": "action", "title": "Real email signature stripping gap", "brief": {"why": "Real ITV email signatures (name/title/org/links block) aren't caught by talon stripper. Discovered when testing with real_thread.json fixture — quoted original is stripped but multi-line corporate signature passes through.", "what": "1. Analyze real signature patterns from fixture 2. Evaluate talon_signature.py rules 3. Add pattern for multi-line corporate sigs or accept as known limitation", "done": "Either signature is stripped from real fixture reply, or documented as accepted limitation with test"}, "status": "done", "parent": "mise-3uu", "order": 19, "created_at": "2026-02-09T09:53:55Z", "created_by": "spm1001", "waiting_for": null, "tactical": {"steps": ["Check real fixture signature pattern", "Add test documenting limitation", "Close as accepted limitation"], "current": 3}, "done_at": "2026-02-09T11:44:06Z"}
{"id": "mise-bl9", "title": "Round-trip extractor tests with real fixtures", "type": "action", "status": "done", "done_at": "2026-01-25T17:35:34.483094Z", "brief": {"why": "Test that extractors produce expected content from real API fixtures, not just 'does it crash' smoke tests. Should verify known content appears in output.", "what": "## Approach\n\nFor each real fixture, add tests that check specific expected content:\n- real_multi_tab.json → verify tab titles, some known text appears\n- real_spreadsheet.json → verify cell values round-trip\n- real_gmail.json → verify message bodies extracted, signatures stripped\n\nThis catches regressions where extractor 'works' but produces wrong output.", "done": "When complete"}, "created_at": "2026-01-23T15:48:54.268436Z", "created_by": "spm1001", "order": 1, "parent": "mise-52d", "waiting_for": null}
{"id": "mise-bmk", "title": "Implement contacts search", "type": "action", "status": "done", "done_at": "2026-01-25T16:57:22.882997Z", "brief": {"why": "Search tool returns 'Contacts search not yet implemented' for source='contacts'. Complete the search surface.", "what": "## Current State\n\nIn tools/search.py:79-80:\n```python\nelif source == \"contacts\":\n    return {\"error\": True, \"message\": \"Contacts search not yet implemented\"}\n```\n\nPeople API utilities exist in v1 (mcp-google-workspace) but not wired here.\n\n## Scope\n\n1. Add contacts adapter\n2. Define ContactResult model\n3. Wire to search tool\n4. Test with real contacts\n\n## Approach\n\n### 1. Model (models.py)\n\n```python\n@dataclass\nclass ContactResult:\n    resource_name: str  # people/c123...\n    display_name: str\n    email_addresses: list[str]\n    phone_numbers: list[str]\n    organization: str | None\n    job_title: str | None\n    source: Literal[\"directory\", \"contacts\"]\n```\n\n### 2. Adapter (adapters/contacts.py)\n\n```python\ndef search_contacts(query: str, max_results: int = 10) -> list[ContactResult]:\n    \"\"\"\n    Search BOTH organization directory AND personal contacts.\n    \n    - Directory: searchDirectoryPeople (requires directory.readonly)\n    - Contacts: searchContacts (requires contacts.readonly)\n    \"\"\"\n```\n\nPort from v1's lookup_contact() which already handles both sources.\n\n### 3. Wire to search.py\n\n```python\nelif source == \"contacts\":\n    results = search_contacts(query, max_results)\n    return [r.to_dict() for r in results]\n```\n\n## Acceptance Criteria\n\n- [ ] ContactResult model defined\n- [ ] adapters/contacts.py with search function\n- [ ] search(source=\"contacts\") returns results\n- [ ] Searches both directory and personal contacts\n- [ ] Integration test with real contacts", "done": "When complete"}, "created_at": "2026-01-24T20:09:44.378821Z", "created_by": "spm1001", "order": 1, "parent": "mise-52d", "waiting_for": null}
{"id": "mise-bpm", "title": "Sheets adapter should filter out non-GRID sheets", "type": "action", "status": "done", "done_at": "2026-01-25T20:13:57.052776Z", "brief": {"why": "fetch_spreadsheet crashes when spreadsheet has chart sheets (sheetType=OBJECT). Should filter to only GRID sheets like capture_fixtures.py does.", "what": "## Original Bug\nsheets.fetch_spreadsheet crashes when spreadsheet has chart sheets (sheetType=OBJECT).\nChart sheets don't have cell ranges → API error \"Invalid range: 'Chart1'\"\n\n## Status (Jan 2026)\n**Absorbed into mise-w6f (chart rendering).**\n\nThe fix is no longer \"filter out chart sheets\" — it's \"render chart sheets as images.\" \nmise-w6f handles both:\n1. Filter GRID sheets for cell values\n2. Render ALL charts (from GRID and OBJECT sheets) as PNGs\n\n## Do not close separately\nThis bead is resolved when mise-w6f is complete.", "done": "When complete"}, "created_at": "2026-01-25T19:14:54.267196Z", "created_by": "spm1001", "order": 1, "parent": "mise-6kh", "waiting_for": null}
{"id": "mise-br7", "title": "Replace v1 in claude.json", "type": "action", "status": "open", "brief": {"why": "Swap mcp-google-workspace for mise-en-space in Claude config. Final ship step.", "what": "## Steps\n\n1. Update ~/.claude.json mcpServers entry\n2. Point to mise-en-space server.py\n3. Test with real Claude session\n4. Verify search + fetch workflow works\n\n## Rollback\n\nKeep v1 config commented, easy to revert if issues.\n\n## Acceptance Criteria\n\n- [ ] mise-en-space registered in claude.json\n- [ ] Claude can call search, fetch, create, help\n- [ ] Real workflow tested (find doc, fetch, read)", "done": "When complete"}, "created_at": "2026-01-23T07:25:54.786555Z", "created_by": "spm1001", "order": 1, "parent": "mise-52d", "waiting_for": null}
{"id": "mise-busafe", "type": "action", "title": "Activity API search source", "brief": {"why": "Action items discovery needs efficient cross-file search. Activity API can find all comment events in one call vs N+1 queries through comments endpoint.", "what": "1. Add source='activity' option to search tool 2. Query activity.query(filter='detail.action_detail_case:COMMENT') 3. Filter to mentionedUsers containing current user 4. Return files with open action items", "done": "search(source='activity') returns files with action items for current user"}, "status": "open", "parent": "mise-kecigu", "order": 1, "created_at": "2026-02-01T10:59:40Z", "created_by": "spm1001", "waiting_for": null, "updated_at": "2026-02-15T19:26:56Z"}
{"id": "mise-bx8", "title": "Escape search query input", "type": "action", "status": "done", "done_at": "2026-01-24T21:55:52.70237Z", "brief": {"why": "Search queries passed directly to Drive/Gmail APIs without escaping. Security risk and correctness issue.", "what": "## Problem\n\nIn tools/search.py:\n```python\nquery = f\"fullText contains '{query}'\"  # line 61\n```\n\nUser input with quotes breaks the query or could inject malicious Drive query operators.\n\n## Scope\n\n1. Drive search - fullText contains clause\n2. Gmail search - query passed directly\n3. Contacts search (when implemented)\n\n## Approach\n\n1. Create `validation.py:escape_drive_query(query: str) -> str`\n   - Escape single quotes: `'` → `\\'`\n   - Escape backslashes: `\\` → `\\\\`\n   \n2. Create `validation.py:validate_gmail_query(query: str) -> str`\n   - Validate against known Gmail operators\n   - Reject/escape dangerous patterns\n   \n3. Update search.py to use these functions\n\n4. Add tests with malicious inputs:\n   - Quotes: `test' OR name contains 'secret`\n   - Operators: `mimeType:application/pdf`\n   - Unicode edge cases\n\n## Acceptance Criteria\n\n- [ ] Drive queries with quotes don't break\n- [ ] Gmail queries with special chars handled\n- [ ] Tests cover injection attempts\n- [ ] No user input reaches API unescaped", "done": "When complete"}, "created_at": "2026-01-24T20:08:51.56354Z", "created_by": "spm1001", "order": 1, "parent": "mise-52d", "waiting_for": null}
{"id": "mise-cacuri", "type": "action", "title": "Surface formula count in xlsx/sheet fetch cues", "brief": {"why": "When fetching xlsx or Google Sheets, formulae are silently flattened to values in the CSV deposit. Caller has no idea the spreadsheet contained logic, not just data. A Claude creating a Sheet from fetched data will lose all formulae without knowing.", "what": "1. In the Sheets API fetch path (adapters/sheets.py), request FORMULA valueRenderOption alongside FORMATTED_VALUE 2. Count cells where formula differs from displayed value 3. Surface formula_count in fetch cues (e.g. formula_count: 47) 4. Add formula_count to manifest.json extras 5. Unit test with fixture containing formulae", "done": "Fetching a spreadsheet with formulae shows formula_count in cues. Fetching one without shows formula_count: 0. Unit test confirms."}, "status": "done", "parent": "mise-geguwo", "order": 2, "created_at": "2026-02-17T20:45:39Z", "created_by": "spm1001", "waiting_for": null, "tactical": {"steps": ["Add FORMULA valueRenderOption call in adapters/sheets.py to count formula cells", "Surface formula_count in fetch cues and manifest for both Google Sheets and XLSX paths", "Unit tests with formula fixture"], "current": 3, "session": "/home/modha/Repos/mise-en-space"}, "updated_at": "2026-02-17T21:33:05Z", "done_at": "2026-02-17T21:33:05Z"}
{"id": "mise-cecuzu", "type": "action", "title": "Gmail search results arrive in relevance order", "brief": {"why": "Gmail batch callbacks (adapters/gmail.py search_threads, lines 245-301) return results in server callback order, not the relevance order from threads().list(). Titans review Epimetheus #13 (Feb 2026). Low user impact currently but technically wrong — search results should be deterministic and relevance-sorted.", "what": "1. In search_threads, capture original thread order from threads().list() response 2. Index batch callback results by thread_id 3. Reorder final results list to match original relevance ranking 4. Add test confirming ordering is preserved", "done": "Gmail search results returned in same order as threads().list() response, with test"}, "status": "done", "parent": "mise-jy3", "order": 12, "created_at": "2026-02-09T21:21:08Z", "created_by": "spm1001", "waiting_for": null, "tactical": {"steps": ["In search_threads, capture original thread order from threads().list() response", "Index batch callback results by thread_id", "Reorder final results list to match original relevance ranking", "Add test confirming ordering is preserved"], "current": 4}, "done_at": "2026-02-09T23:09:45Z"}
{"id": "mise-cedije", "type": "action", "title": "Truncation guard for inline attachment text", "brief": {"why": "No longer needed — inline embedding was removed entirely (a0a7a45), so there is nothing to truncate. The separate .pdf.md file is the single source for extracted attachment text.", "what": "1. Add max_inline_chars parameter to the embedding loop in fetch_gmail (default ~10000 chars) 2. When exceeded, truncate with note pointing to the separate .md file 3. Test with synthetic large content", "done": "content.md stays readable even with large/malformed PDF attachments. Full text available in separate file."}, "status": "done", "parent": "mise-4mj", "order": 9, "created_at": "2026-02-08T19:50:57Z", "created_by": "spm1001", "waiting_for": null, "done_at": "2026-02-08T21:12:21Z"}
{"id": "mise-cejepo", "type": "action", "title": "Forced-browser deposits get real titles from pre_extracted_content H1", "brief": {"why": "When use_browser=True, html='' so extract_title returns nothing. Folder names are always web--web-page--hash. The markdown from passe typically starts with # Title which could be used.", "what": "1. In tools/fetch/web.py, after pre_extracted_content path, extract title from first H1 line 2. Fall back to URL-derived title if no H1 3. Add test for H1 extraction", "done": "Forced-browser deposits have real titles in folder names and metadata, not 'web-page'"}, "status": "done", "parent": "mise-reriri", "order": 3, "created_at": "2026-02-13T10:59:50Z", "created_by": "spm1001", "waiting_for": null, "updated_at": "2026-02-15T20:09:58Z", "tactical": {"steps": ["In tools/fetch/web.py, after pre_extracted_content path, extract title from first H1 line", "Fall back to URL-derived title if no H1", "Add test for H1 extraction"], "current": 3, "session": "/home/modha/Repos/mise-en-space"}, "done_at": "2026-02-15T20:09:58Z"}
{"id": "mise-cetoha", "type": "action", "title": "Refactor do() input dispatch to per-operation validation", "brief": {"why": "do() has 11 optional params with most irrelevant per operation. content means different things for create vs replace_text. Next operation addition will make the signature confusing and error-prone.", "what": "1. Decide approach: kwargs dispatch, per-operation param dicts, or typed operation classes 2. Implement chosen pattern with clear param validation per operation 3. Update tests 4. Ensure MCP tool description stays clear for Claude discovery", "done": "Adding a new do() operation has a clear template. Invalid param combinations caught early with helpful errors."}, "status": "done", "parent": "mise-hijute", "order": 11, "created_at": "2026-02-18T21:51:31Z", "created_by": "spm1001", "waiting_for": null, "tactical": {"steps": ["Add DoResult dataclass to models.py", "Add OPERATIONS constant to tools/__init__.py", "Move validation into do_move, return DoResult", "Move validation into do_prepend/do_append/do_replace_text, return DoResult", "Move validation + source resolution into do_overwrite, return DoResult", "Move validation + source resolution into do_create, return DoResult at boundary", "Replace elif chain with dispatch dict in server.py", "Update tests for new signatures + add dispatch/consistency tests", "Update CLAUDE.md recipe and docs_do() resource"], "current": 9, "session": "/home/modha/Repos/mise-en-space"}, "updated_at": "2026-02-18T23:13:53Z", "updated_by": "stepped", "done_at": "2026-02-18T23:13:53Z"}
{"id": "mise-cewowe", "type": "action", "title": "Shared fixture loader in conftest.py", "brief": {"why": "Every adapter test file does its own json.loads(Path(...).read_text()). Duplicated, no type coercion, no validation that fixture exists.", "what": "Add load_fixture(category, name) to tests/conftest.py that returns raw dict. Optionally returns typed dataclass via existing conftest converters. Refactor test files to use it.", "done": "All test files using fixtures import from conftest. Missing fixture gives clear error at collection time, not runtime."}, "status": "done", "parent": "mise-3uu", "order": 14, "created_at": "2026-02-09T07:20:23Z", "created_by": "spm1001", "waiting_for": null, "tactical": {"steps": ["Replace inline json.loads in test_docs_adapter.py with load_fixture", "Replace inline json.loads in test_gmail_adapter.py with load_fixture", "Check test_negative_paths.py — separate malformed path, may need a load_malformed_fixture or just leave", "Verify all tests pass"], "current": 4}, "done_at": "2026-02-09T09:42:34Z"}
{"id": "mise-chf", "title": "Port sheets extractor — prove the pattern", "type": "action", "status": "done", "done_at": "2026-01-23T07:34:27.980862Z", "brief": {"why": "Simplest extractor (138 lines). Port to pure function to validate architecture.", "what": "## Why First\n\n- 138 lines, single function\n- Clear input → output\n- If this works, others follow same pattern\n\n## Porting Steps\n\n1. Copy extraction logic from v1 tools/sheets.py\n2. Remove get_sheets_service() call\n3. Change signature: extract_sheets(response: dict) -> str\n4. Add type hints\n5. Write unit test with fixture\n\n## Acceptance Criteria\n\n- [ ] extractors/sheets.py exists with pure function\n- [ ] Unit test with mocked API response passes\n- [ ] No imports from adapters or tools", "done": "When complete"}, "created_at": "2026-01-23T07:24:11.833893Z", "created_by": "spm1001", "order": 1, "parent": "mise-52d", "waiting_for": null}
{"id": "mise-cibufu", "type": "action", "title": "Structural quality gate catches flattened PDF tables", "brief": {"why": "markitdown's char-count gate (500) passes data-heavy table PDFs with total structure loss (Trivago PDF: 4,874 chars, zero row/column structure). Drive fallback preserves tables via Google Doc conversion but never triggers because the char threshold is a quantity check, not a quality check. Affects all PDF sources: Drive, Gmail attachments, web URLs.", "what": "1. Add _looks_like_flattened_tables() heuristic to adapters/pdf.py (three-signal: short_ratio, sentence_ratio, numeric_ratio) 2. Modify extract_pdf_content() to call heuristic after char-count passes, fall through to Drive on failure 3. Add synthetic near-threshold fixtures (e.g. 65% short_ratio, borderline numeric) to complement the real rate-card exemplar 4. Add ~9 unit tests in test_pdf.py — refactor TestFlattenedTableDetection to call actual heuristic function 5. Verify against rate-card fixture (thread 19b26c2d09b1291d) 6. Update CLAUDE.md design decisions", "done": "Flattened-table PDFs trigger Drive fallback with warning in cues. Normal text PDFs still use fast markitdown path (no false positives on prose, code, poetry, lists). Trivago PDF deposit preserves row/column structure. All existing PDF tests pass."}, "status": "done", "parent": "mise-wocidi", "order": 1, "created_at": "2026-02-13T14:26:25Z", "created_by": "spm1001", "waiting_for": null, "tactical": {"steps": ["Add _looks_like_flattened_tables() heuristic to adapters/pdf.py (three-signal: short_ratio, sentence_ratio, numeric_ratio)", "Modify extract_pdf_content() to call heuristic after char-count passes, fall through to Drive on failure", "Add synthetic near-threshold fixtures (e.g. 65% short_ratio, borderline numeric) to complement the real rate-card exemplar", "Add ~9 unit tests in test_pdf.py — refactor TestFlattenedTableDetection to call actual heuristic function", "Verify against rate-card fixture (thread 19b26c2d09b1291d)", "Update CLAUDE.md design decisions"], "current": 6, "session": "/home/modha/Repos/mise-en-space"}, "updated_at": "2026-02-15T21:39:34Z", "done_at": "2026-02-15T21:39:34Z"}
{"id": "mise-cifike", "type": "action", "title": "Audit passe mocks across test_web.py", "brief": {"why": "The pidihu bug was caused by missing _is_passe_available mock. Any test touching auth detection, CAPTCHA, or JS-rendering paths could have the same issue — silently passing because passe is available on the dev machine.", "what": "1. Grep test_web.py for all tests that exercise code paths reaching _is_passe_available 2. Verify each has explicit mock (True or False) 3. Fix any that rely on ambient environment", "done": "Every test in test_web.py that touches a passe-fallback code path has an explicit _is_passe_available mock"}, "status": "done", "parent": "mise-jy3", "order": 17, "created_at": "2026-02-18T14:37:02Z", "created_by": "spm1001", "waiting_for": null, "tactical": {"steps": ["Grep test_web.py for all tests that exercise code paths reaching _is_passe_available", "Verify each has explicit mock (True or False)", "Fix any that rely on ambient environment"], "current": 3, "session": "/home/modha/Repos/mise-en-space"}, "updated_at": "2026-02-18T20:11:42Z", "done_at": "2026-02-18T20:11:42Z"}
{"id": "mise-cilodu", "type": "action", "title": "CoreGraphics rendering polish (macOS-only)", "brief": {"why": "Three small improvements to _render_via_coregraphics identified during review. All macOS-only, untestable on Linux. Bundle as one pass on a Mac.", "what": "1. Replace temp PNG files with in-memory NSMutableData (CGImageDestinationCreateWithData) — removes 100 filesystem round-trips per PDF 2. Handle page rotation: check CGPDFPageGetRotationAngle, swap width_px/height_px for 90°/270° so PageImage metadata matches visual orientation 3. Replace assert file_bytes is not None with ValueError (assert stripped by python -O)", "done": "All three applied in _do_coregraphics_render. Tested manually on Mac with benchmark script. No temp .png files created during rendering."}, "status": "open", "parent": "mise-jy3", "order": 18, "created_at": "2026-02-18T21:11:29Z", "created_by": "spm1001", "waiting_for": null}
{"id": "mise-cinomi", "type": "action", "title": "Add status banner to sheet-creation-design.md", "brief": {"why": "Design doc advocates deposit-then-publish (source=path) but shipped code takes inline content. Next Claude will think source= is implemented.", "what": "Add banner at top: Status: Partially implemented — inline content works, deposit-then-publish (source param) tracked in mise-dinadi", "done": "Design doc has clear status showing what is and isn't implemented"}, "status": "done", "parent": "mise-beriji", "order": 8, "created_at": "2026-02-16T17:59:05Z", "created_by": "spm1001", "waiting_for": null, "done_at": "2026-02-18T14:04:33Z"}
{"id": "mise-cohato", "type": "action", "title": "Parallelize Drive + Gmail search", "brief": {"why": "Search with both sources runs sequentially (~2.3s). Drive and Gmail are independent API calls that could run concurrently, cutting latency roughly in half to ~1s.", "what": "1. Use concurrent.futures.ThreadPoolExecutor (or asyncio.to_thread) to run search_drive_files and search_gmail_threads in parallel 2. Merge results after both complete 3. Benchmark to confirm improvement", "done": "search('query') with both sources completes in ≤1.5s average (down from 2.3s)"}, "status": "done", "parent": "mise-weduje", "order": 1, "created_at": "2026-02-09T06:29:58Z", "created_by": "spm1001", "waiting_for": null, "tactical": {"steps": ["Use concurrent.futures.ThreadPoolExecutor (or asyncio.to_thread) to run search_drive_files and search_gmail_threads in parallel", "Merge results after both complete", "Benchmark to confirm improvement"], "current": 3}, "done_at": "2026-02-09T06:42:01Z"}
{"id": "mise-cokuhi", "type": "action", "title": "Test overwrite source-path happy path", "brief": {"why": "Overwrite supports deposit-then-publish via source param (reads content.md from folder) but only the validation error path (missing file) is tested. The happy path — mock a deposit folder, call do_overwrite with source, verify it reads content.md — is untested.", "what": "1. Add unit test: create temp dir with content.md, call do_overwrite(source=path), verify content passed to batchUpdate 2. Add unit test: source with headings, verify styles applied", "done": "Overwrite source-path covered by unit tests. Both plain text and heading-styled content tested."}, "status": "open", "parent": "mise-jy3", "order": 20, "created_at": "2026-02-18T21:51:40Z", "created_by": "spm1001", "waiting_for": null}
{"id": "mise-cunufu", "type": "outcome", "title": "Cross-source search ergonomics", "brief": {"why": "When bouncing between Drive and Gmail to find context, tools don't hint at linkage opportunities", "what": "MCP resource documenting patterns, metadata enrichment for exfil'd files, expose email context", "done": "All acceptance criteria met"}, "status": "done", "order": 2, "created_at": "2026-01-29T21:12:14Z", "created_by": "spm1001", "done_at": "2026-01-29T21:14:57Z"}
{"id": "mise-curuci", "type": "action", "title": "Survey meeting note patterns in Drive", "brief": {"why": "Need to understand variety: Google Meet transcripts vs manual notes vs Fireflies vs Notion imports", "what": "Sample 10-20 meeting notes, document common patterns and section types", "done": "Have categorization of meeting note formats with example snippets"}, "status": "done", "parent": "mise-fetifo", "order": 1, "created_at": "2026-01-30T12:49:21Z", "created_by": "spm1001", "waiting_for": null, "done_at": "2026-01-30T14:04:33Z"}
{"id": "mise-cuviCi", "type": "action", "title": "Add description field to Drive metadata", "brief": {"why": "Enables detection of exfil'd files via Message ID in description", "what": "Add description to FILE_METADATA_FIELDS, surface in results", "done": "Description field accessible in search/fetch results"}, "status": "done", "parent": "mise-cunufu", "order": 2, "created_at": "2026-01-29T21:12:25Z", "created_by": "spm1001", "waiting_for": null, "done_at": "2026-01-29T21:13:22Z"}
{"id": "mise-da6", "title": "Fix sheets adapter to use batchGet", "type": "action", "status": "done", "done_at": "2026-01-23T12:12:01.497189Z", "brief": {"why": "Current sheets adapter makes N+1 API calls (1 metadata + N tabs). Should use values().batchGet() to fetch all tabs in one call.", "what": "## Current (N+1 calls)\n\n```python\nfor sheet_name in sheet_names:\n    values_response = service.spreadsheets().values().get(\n        spreadsheetId=spreadsheet_id,\n        range=f\"'{sheet_name}'\",\n    ).execute()\n```\n\n## Should Be (2 calls)\n\n```python\nranges = [f\"'{name}'\" for name in sheet_names]\nbatch_response = service.spreadsheets().values().batchGet(\n    spreadsheetId=spreadsheet_id,\n    ranges=ranges,\n    valueRenderOption=\"FORMATTED_VALUE\",\n).execute()\n```\n\n## Acceptance Criteria\n\n- [ ] Uses batchGet instead of per-tab get\n- [ ] Integration test still passes\n- [ ] Quota usage reduced (verify with rate limit test)", "done": "When complete"}, "created_at": "2026-01-23T12:11:05.193664Z", "created_by": "spm1001", "order": 1, "waiting_for": null}
{"id": "mise-dezezi", "type": "action", "title": "AGENTS.md for visiting Claudes", "brief": {"why": "Team Claudes working in this repo need to know: don't modify architecture, file issues not PRs, how to run tests, layer rules", "what": "1. Expand existing AGENTS.md with repo conventions and architecture summary 2. Add test commands and quality gates 3. Add issue filing guidance for Claude agents 4. Document pre-ship/post-ship outcome split", "done": "A teammate's Claude can clone, orient, and file a good bug report without breaking conventions"}, "status": "done", "parent": "mise-naviho", "order": 6, "created_at": "2026-02-07T18:50:36Z", "created_by": "spm1001", "waiting_for": null, "done_at": "2026-02-07T22:54:35Z"}
{"id": "mise-dinadi", "type": "action", "title": "mise-do/ directory convention and manifest schema", "brief": {"why": "All do() operations need a staging area. The deposit-then-publish pattern keeps caller context windows lean (file path vs inline content), gives humans an inspection checkpoint (read/edit the draft before publishing), and makes intermediate work survive session crashes. This is the write-side equivalent of mise-fetch/. Applies to docs AND sheets — the source parameter routes by doc_type to the right file extension. Callers deposit markdown for docs, CSV for sheets, same pattern.", "what": "1. Rename mise-fetch/ to mise/ — single bidirectional workspace. Fetch deposits there, do reads from there. No copy step for fetch→transform→create flows. 2. Update workspace/manager.py — deposit folder creation, naming convention ({type}--{slug}--{qualifier}/) 3. Define manifest schema: pre-creation (title, doc_type, folder_id) and post-creation enrichment (status, file_id, web_link, created_at) 4. Add source parameter to do() tool signature — reads content from deposit 5. For doc_type=doc: reads content.md, passes to existing _create_doc 6. For doc_type=sheet: reads content.csv, uploads via Drive CSV import 7. Inline content param stays for backwards compat 8. Post-creation: enrich manifest as receipt 9. Update all references from mise-fetch/ to mise/", "done": "do(operation=create) accepts source= pointing at a mise-do/ deposit folder. Works for both doc (reads content.md) and sheet (reads content.csv). Manifest enriched post-creation. Inline content still works for backwards compat. Convention documented."}, "status": "done", "parent": "mise-beriji", "order": 2, "created_at": "2026-02-16T13:28:42Z", "created_by": "spm1001", "waiting_for": null, "updated_at": "2026-02-17T11:11:58Z", "tactical": {"steps": ["Rename mise-fetch/ to mise/ — single bidirectional workspace. Fetch deposits there, do reads from there. No copy step for fetch→transform→create flows.", "Update workspace/manager.py — deposit folder creation, naming convention ({type}--{slug}--{qualifier}/)", "Define manifest schema: pre-creation (title, doc_type, folder_id) and post-creation enrichment (status, file_id, web_link, created_at)", "Add source parameter to do() tool signature — reads content from deposit", "For doc_type=doc: reads content.md, passes to existing _create_doc", "For doc_type=sheet: reads content.csv, uploads via Drive CSV import", "Inline content param stays for backwards compat", "Post-creation: enrich manifest as receipt", "Update all references from mise-fetch/ to mise/"], "current": 9, "session": "/home/modha/Repos/mise-en-space"}, "done_at": "2026-02-17T11:11:58Z"}
{"id": "mise-dinidi", "type": "action", "title": "httpx mock helper for web adapter tests", "brief": {"why": "test_web.py has ~20 repetitions of the httpx.Client context manager mock pattern (mock_client_cls.return_value.__enter__/exit). This is verbose and non-obvious. A fixture or helper would reduce boilerplate and make tests more readable.", "what": "1. Create a pytest fixture or helper that yields a mock httpx client with the context manager wired up 2. Refactor test_web.py to use the helper 3. Consider respx library as alternative", "done": "No repeated __enter__/__exit__ boilerplate in test_web.py, all tests still pass"}, "status": "done", "parent": "mise-3uu", "order": 16, "created_at": "2026-02-09T08:09:35Z", "created_by": "spm1001", "waiting_for": null, "tactical": {"steps": ["Create a pytest fixture or helper that yields a mock httpx client with the context manager wired up", "Refactor test_web.py to use the helper", "Consider respx library as alternative"], "current": 3}, "done_at": "2026-02-09T08:34:29Z"}
{"id": "mise-dvs", "title": "Fix mypy errors across codebase", "type": "action", "status": "done", "done_at": "2026-01-25T17:11:55.514772Z", "brief": {"why": "Ran mypy on entire codebase, found 44 errors. Fixed critical one (missing Any import). Remaining errors need attention.", "what": "## Error Summary (44 total)\n- tools/fetch.py: 25 errors (type conflicts, missing generics)\n- server.py: 6 errors (Optional types, missing generics)\n- adapters/: 8 errors (Literal mismatches, missing generics)\n- extractors/: 1 error (Any return)\n\n## Key Issues\n1. **Return type mismatches** — Functions return FetchResult but typed as dict\n2. **Missing Optional** — `sources: list[str] = None` needs `| None`\n3. **Missing generics** — `dict` instead of `dict[str, Any]`\n4. **Literal mismatches** — string vs Literal type conflicts\n\n## Quick Fixes Done\n- Added `Any` import to tools/create.py\n\n## Acceptance Criteria\n- [ ] `uv run mypy` passes with 0 errors\n- [ ] Types are accurate, not just silenced", "done": "When complete"}, "created_at": "2026-01-25T17:00:03.005754Z", "created_by": "spm1001", "order": 1, "waiting_for": null}
{"id": "mise-dyd", "title": "mise-en-space handles edge cases and errors gracefully", "type": "outcome", "status": "done", "done_at": "2026-01-25T19:48:44.598541Z", "brief": {"why": "Robustness and security. No silent failures, no injection vectors, warnings surfaced to user.", "what": "## Success Criteria\n\n- [ ] Search queries with quotes/special chars don't break or inject\n- [ ] Thumbnail fetch failures reported (not silently swallowed)\n- [ ] Table rowSpan handled correctly\n- [ ] Temp files cleaned up (or cleanup documented)\n- [ ] All warnings flow through to manifest.json\n\n## Execution Order\n\n1. **mise-bx8** Escape search query input (security - do first)\n2. **mise-2h2** Handle rowSpan in table parsing (correctness)\n3. **mise-6zd** Fix silent thumbnail failures (observability)\n4. **mise-mii** Handle orphaned temp files (cleanup)\n\n## Why This Matters\n\nSilent failures erode trust. Claude thinks extraction succeeded when it didn't. Security issues (query injection) could expose data. Robustness = reliability.", "done": "When complete"}, "created_at": "2026-01-24T20:16:26.882981Z", "created_by": "spm1001", "order": 2}
{"id": "mise-e0e", "title": "Wire search tool", "type": "action", "status": "done", "done_at": "2026-01-23T22:13:09.483681Z", "brief": {"why": "Unified search across Drive + Gmail. Returns metadata only, no files written.", "what": "## Signature\n\nsearch(query: str, sources: list[str] = ['drive', 'gmail'], max_results: int = 20) -> dict\n\n## Response Shape\n\n{\n  \"drive_results\": [\n    {\"id\": \"...\", \"name\": \"...\", \"mimeType\": \"...\", \"modified\": \"...\", \"url\": \"...\"}\n  ],\n  \"gmail_results\": [\n    {\"thread_id\": \"...\", \"subject\": \"...\", \"snippet\": \"...\", \"date\": \"...\", \"from\": \"...\"}\n  ]\n}\n\n## Key Behaviors\n\n- Separate lists per source (not merged)\n- Drive: fullText match IS the triage (no snippets needed)\n- Gmail: snippet from API\n- Includes exfiltrated attachments (they're in Drive)\n- Contacts deferred for now\n\n## URL Handling\n\nAccept Gmail URLs, convert to thread ID:\nhttps://mail.google.com/mail/u/0/#inbox/18f4a... → thread_id\n\n## Acceptance Criteria\n\n- [ ] Searches Drive and Gmail in parallel\n- [ ] Returns separate result lists\n- [ ] Handles Gmail URLs\n- [ ] max_results works per source", "done": "When complete"}, "created_at": "2026-01-23T07:25:02.686984Z", "created_by": "spm1001", "order": 1, "parent": "mise-52d", "waiting_for": null}
{"id": "mise-e5o", "title": "Create pre-exfil test fixture", "type": "action", "status": "open", "brief": {"why": "Email with attachment that's been exfiltrated to Drive. Needed to test pre-exfil detection.", "what": "## What's Needed\n\n1. Find an email with attachment in user's Gmail\n2. Verify the attachment exists in \"Email Attachments\" folder\n3. Note: email thread ID, attachment filename, Drive file ID\n4. Add to EXPERIMENTS.md fixtures table\n\n## Test Case\n\nfetch(attachment_id) should:\n1. Detect pre-exfiltrated version exists\n2. Fetch from Drive (not Gmail)\n3. Return path to Drive-sourced file", "done": "When complete"}, "created_at": "2026-01-23T07:30:06.326602Z", "created_by": "spm1001", "order": 1, "parent": "mise-52d", "waiting_for": null}
{"id": "mise-eqi", "title": "Add negative path tests", "type": "action", "status": "done", "done_at": "2026-01-25T17:23:35.075557Z", "brief": {"why": "No tests for malformed input, missing fields, or error conditions. Extractors need resilience testing.", "what": "## Problem\n\nCurrent tests only cover happy paths:\n- Valid fixtures → expected output\n- No tests for: null fields, missing objectId, empty content, malformed structures\n\n## Gap Analysis\n\n### Extractors\n\n| Extractor | Missing Tests |\n|-----------|---------------|\n| docs.py | Missing inlineObject reference, null textRun content |\n| sheets.py | Empty spreadsheet, null cell values, malformed rows |\n| slides.py | Missing objectId, null shape content, empty tables |\n| gmail.py | Decode errors, missing body parts, malformed MIME |\n\n### Adapters\n\n| Adapter | Missing Tests |\n|---------|---------------|\n| All | 429 rate limit response |\n| All | 5xx server error |\n| All | Network timeout |\n| All | Invalid credentials (401) |\n\n### Validation\n\n| Function | Missing Tests |\n|----------|---------------|\n| extract_drive_file_id | Malformed URLs, empty string |\n| extract_gmail_id | Invalid web IDs, empty string |\n| convert_gmail_web_id | Edge case IDs |\n\n## Approach\n\n1. Create fixtures/malformed/ directory with bad inputs\n2. Test each extractor with malformed data\n3. Verify graceful degradation (warnings, not crashes)\n4. Test adapters with mocked error responses (see mise-h6m)\n\n## Acceptance Criteria\n\n- [ ] fixtures/malformed/ contains edge case fixtures\n- [ ] Each extractor has negative path tests\n- [ ] Extractors produce warnings not exceptions for recoverable errors\n- [ ] Tests document expected behavior for each error type", "done": "When complete"}, "created_at": "2026-01-24T20:10:39.234734Z", "created_by": "spm1001", "order": 1, "parent": "mise-52d", "waiting_for": null}
{"id": "mise-fejena", "type": "action", "title": "Forwarded messages preserved in Gmail thread extraction", "brief": {"why": "strip_signature_and_quotes() unconditionally strips ALL >-prefixed lines and 'On ... wrote:' attributions. Correct for reply quotes (redundant content) but destroys forwarded messages (unique content from outside the thread). Additionally, message/rfc822 MIME parts are completely invisible to the parser. Real impact: Peter Fergusson's key internal analysis was absent from VOD measurement thread deposit (19be6358cbc99eca, message 10/10).", "what": "1. Add split_forward_sections() to extractors/talon_signature.py — scan for forward markers (Gmail: '---------- Forwarded message ---------', Apple: 'Begin forwarded message:') BEFORE stripping. Split body into own_content (gets stripped) and forwarded_sections (preserved). Handle >-prefixed forwarded content after markers. ForwardedSection dataclass with attribution + body.\n2. Modify strip_signature_and_quotes() pipeline — call split_forward_sections first, run existing 3-pass stripping on own_body only, reassemble with forwarded sections using '--- Forwarded message ---' delimiter. Edge case: reply-to-forward (> ---------- Forwarded message) inside reply quotes won't match regex (anchored to ^), correctly stripped.\n3. Add ForwardedMessage dataclass to models.py (from_address, date, subject, body_text). Add forwarded_messages: list[ForwardedMessage] field to EmailMessage (default empty list, non-breaking).\n4. Add parse_forwarded_messages(payload) to extractors/gmail.py — walk MIME tree for message/rfc822 parts, extract headers (From/Date/Subject) and body (text/plain, html fallback) from nested payload.\n5. Wire into adapters/gmail.py _build_message() — call parse_forwarded_messages alongside existing parse_attachments_from_payload.\n6. Update extract_message_content() in extractors/gmail.py — after signature stripping, append MIME-forwarded messages with attribution block.\n7. Add ~14 unit tests: TestForwardDetection (~8 tests: Gmail/Apple markers, preserved content, quoted forwards, reply-quotes-before-forward still stripped, multiple forwards, reply-to-forward edge case, no-marker unchanged) + TestRfc822Extraction (~6 tests: rfc822 part parsed, headers extracted, body text, html fallback, no-rfc822 empty, appended to content).\n8. Verify against real VOD measurement thread (19be6358cbc99eca) — Peter Fergusson's analysis visible in message 10/10.\n9. Update CLAUDE.md design decisions table.", "done": "Forwarded messages appear in content.md with clear '--- Forwarded message ---' delimiter and From/Date/Subject attribution. Reply quotes still stripped correctly. MIME message/rfc822 parts extracted and appended. Peter Fergusson's analysis visible in message 10/10 of VOD thread. All existing Gmail extraction tests pass."}, "status": "done", "parent": "mise-wocidi", "order": 2, "created_at": "2026-02-13T14:26:41Z", "created_by": "spm1001", "waiting_for": null, "tactical": {"steps": ["Add split_forward_sections() to extractors/talon_signature.py — scan for forward markers (Gmail: '---------- Forwarded message ---------', Apple: 'Begin forwarded message:') BEFORE stripping. Split body into own_content (gets stripped) and forwarded_sections (preserved). Handle >-prefixed forwarded content after markers. ForwardedSection dataclass with attribution + body.", "Modify strip_signature_and_quotes() pipeline — call split_forward_sections first, run existing 3-pass stripping on own_body only, reassemble with forwarded sections using '--- Forwarded message ---' delimiter. Edge case: reply-to-forward (> ---------- Forwarded message) inside reply quotes won't match regex (anchored to ^), correctly stripped.", "Add ForwardedMessage dataclass to models.py (from_address, date, subject, body_text). Add forwarded_messages: list[ForwardedMessage] field to EmailMessage (default empty list, non-breaking).", "Add parse_forwarded_messages(payload) to extractors/gmail.py — walk MIME tree for message/rfc822 parts, extract headers (From/Date/Subject) and body (text/plain, html fallback) from nested payload.", "Wire into adapters/gmail.py _build_message() — call parse_forwarded_messages alongside existing parse_attachments_from_payload.", "Update extract_message_content() in extractors/gmail.py — after signature stripping, append MIME-forwarded messages with attribution block.", "Add ~14 unit tests: TestForwardDetection (~8 tests: Gmail/Apple markers, preserved content, quoted forwards, reply-quotes-before-forward still stripped, multiple forwards, reply-to-forward edge case, no-marker unchanged) + TestRfc822Extraction (~6 tests: rfc822 part parsed, headers extracted, body text, html fallback, no-rfc822 empty, appended to content).", "Verify against real VOD measurement thread (19be6358cbc99eca) — Peter Fergusson's analysis visible in message 10/10.", "Update CLAUDE.md design decisions table."], "current": 9, "session": "/home/modha/Repos/mise-en-space"}, "updated_at": "2026-02-15T20:48:43Z", "done_at": "2026-02-15T20:48:43Z"}
{"id": "mise-fetifo", "type": "outcome", "title": "Meeting notes are distilled to high-signal summaries", "brief": {"why": "Meeting notes are context busters with high chaff — raw extraction wastes tokens and buries decisions", "what": "Extraction strategy that reduces meeting notes to structured signal: decisions, action items, key points", "done": "Can fetch a meeting note and get ~500 words of structured signal instead of 5000 words of raw content"}, "status": "done", "order": 12, "created_at": "2026-01-30T12:49:16Z", "created_by": "spm1001", "done_at": "2026-01-31T16:33:07Z"}
{"id": "mise-fiWiBe", "type": "outcome", "title": "Embed extracted PDF content in content.md", "brief": {"why": "Currently PDFs are deposited separately; Claude must Read them explicitly. Should embed text like Drive PDFs.", "what": "After extracting PDF attachment, append extracted markdown to thread content.md (like we do for Drive PDFs). Keep raw PDF file for reference.", "done": "content.md includes inline PDF text for all extracted attachments"}, "status": "done", "order": 11, "created_at": "2026-01-29T23:26:05Z", "created_by": "spm1001", "done_at": "2026-02-01T10:58:29Z"}
{"id": "mise-fuSepi", "type": "action", "title": "Fix cwd bug — deposits go to MCP dir not Claude's cwd", "brief": {"why": "MCP servers run as separate processes. Path.cwd() returns MCP's directory, not Claude's working directory. All deposits accumulate in mise-en-space/mise-fetch/ regardless of where Claude is working.", "what": "1. Add base_path parameter to search() and fetch() tools 2. Pass through to workspace manager 3. Default to cwd for backwards compat 4. Document in skill", "done": "Deposits appear in Claude's working directory when base_path passed; mise skill updated"}, "status": "done", "parent": "mise-naviho", "order": 1, "created_at": "2026-01-31T22:07:22Z", "created_by": "spm1001", "waiting_for": null, "tactical": {"steps": ["Add base_path parameter to search() and fetch() tools", "Pass through to workspace manager", "Default to cwd for backwards compat", "Document in skill"], "current": 4}, "done_at": "2026-02-07T20:58:44Z"}
{"id": "mise-fubari", "type": "action", "title": "Capture flattened PDF fixture from Trivago extraction", "brief": {"why": "The PDF heuristic (_looks_like_flattened_tables) needs a grounded test fixture. Currently 'verify against real artifact' requires live Gmail API access. A frozen fixture of the markitdown output enables repeatable unit tests and threshold validation.", "what": "1. Fetch Gmail thread 19b26c2d09b1291d and extract 'ITV UK - 2025 05 Nov.pdf' attachment via markitdown (force markitdown path, skip Drive fallback)\n2. Save raw markitdown output as fixtures/pdf/flattened_table_trivago.txt\n3. Reference from test_pdf.py TestFlattenedTableDetection tests", "done": "Fixture file exists at fixtures/pdf/flattened_table_rate_card.txt (sanitized from real data). Heuristic tests use it instead of synthetic data for at least one 'should trigger' test case."}, "status": "done", "parent": "mise-wocidi", "order": 3, "created_at": "2026-02-13T14:59:52Z", "created_by": "spm1001", "waiting_for": null, "tactical": {"steps": ["Fetch Gmail thread 19b26c2d09b1291d and extract 'ITV UK - 2025 05 Nov.pdf' attachment via markitdown (force markitdown path, skip Drive fallback)", "Save raw markitdown output as fixtures/pdf/flattened_table_trivago.txt", "Reference from test_pdf.py TestFlattenedTableDetection tests"], "current": 3, "session": "/Users/modha/Repos/mise-en-space"}, "done_at": "2026-02-13T15:11:32Z"}
{"id": "mise-fulawu", "type": "outcome", "title": "Phase 3: Full webctl browser integration", "brief": {"why": "JS-rendered pages (SPAs, Next.js) need browser rendering. webctl fallback exists but is untested.", "what": "Test browser fallback with real JS-heavy pages. Verify cookie forwarding works. Add integration tests.", "done": "fetch() extracts content from React/Next.js sites when webctl running"}, "status": "done", "order": 25, "created_at": "2026-02-01T15:23:19Z", "created_by": "spm1001", "done_at": "2026-02-06T20:11:52Z"}
{"id": "mise-g9i", "title": "Wire adapters — Google API wrappers", "type": "action", "status": "done", "done_at": "2026-01-23T18:32:00.02925Z", "brief": {"why": "Thin wrappers that call Google APIs and pass responses to extractors. Thread-safe.", "what": "## Pattern\n\nFrom V2.md research:\n- Use requestBuilder pattern for thread safety\n- Always use fields parameter for partial responses\n- **BATCH API CALLS** — whenever making multiple calls of the same type, batch them\n\n## Batching Requirements (MANDATORY)\n\n| Adapter | What to batch | How |\n|---------|---------------|-----|\n| sheets | Multiple tabs | `values().batchGet(ranges=[...])` |\n| gmail | Multiple messages in thread | `new_batch_http_request()` |\n| slides | Selected thumbnail fetches | `new_batch_http_request()` |\n| drive | Multiple file metadata | `new_batch_http_request()` |\n| docs | Inline image fetches | `new_batch_http_request()` if multiple |\n\n## Adapters Needed\n\nadapters/drive.py:\n- get_file_metadata(file_id)\n- export_file(file_id, mime_type) \n- search_files(query, max_results)\n\nadapters/gmail.py:\n- get_thread(thread_id) — batch fetches all messages\n- get_message(message_id)\n- search_threads(query, max_results)\n\nadapters/docs.py:\n- get_document(doc_id)\n\nadapters/sheets.py:\n- get_spreadsheet(spreadsheet_id) — uses batchGet for tabs\n\nadapters/slides.py:\n- get_presentation(presentation_id)\n- get_thumbnails(presentation_id, slide_ids) — batched\n\n## Acceptance Criteria\n\n- [ ] All adapters use fields parameter\n- [ ] All adapters use batch APIs where applicable (see table above)\n- [ ] Gmail uses batch endpoint internally\n- [ ] Sheets uses batchGet for multiple tabs\n- [ ] Thread-safe (requestBuilder pattern)\n- [ ] Integration test with real API passes", "done": "When complete"}, "created_at": "2026-01-23T07:24:44.671222Z", "created_by": "spm1001", "order": 1, "parent": "mise-52d", "waiting_for": null}
{"id": "mise-g9i.1", "title": "Write sheets adapter", "type": "action", "status": "done", "done_at": "2026-01-23T11:35:00.401404Z", "brief": {"why": "First real adapter. Calls Sheets API, assembles SpreadsheetData, proves end-to-end pattern. Use @with_retry decorator. Follow search_/fetch_/create_ naming.", "what": "See title", "done": "When complete"}, "created_at": "2026-01-23T09:03:40.499314Z", "created_by": "spm1001", "order": 1, "parent": "mise-g9i", "waiting_for": null}
{"id": "mise-gajori", "type": "action", "title": "Forage: native CDP browser fallback replacing webctl", "brief": {"why": "mise's web fetch needs browser rendering for JS-heavy and auth'd pages. Currently shells out to webctl (Playwright daemon, heavy deps). Chrome Debug on port 9222 already runs with user's Google SSO. mise already has websockets dep. A thin CDP adapter gives mise direct browser access — zero new deps, inherits auth, and chrome-log captures traffic as a free side effect.", "what": "1. Create adapters/cdp.py — thin CDP client (connect, navigate, wait, get HTML, manage tabs) 2. Wire into web adapter as browser fallback (HTTP+trafilatura fails → CDP+trafilatura) 3. Auto-start Chrome Debug if not running (shell out to chrome-debug/forage script) 4. Tab lifecycle — open, navigate, extract, close (don't litter Chrome Debug with orphan tabs) 5. Rename chrome-debug to forage across skill-chrome-log repo (script, app, commands) 6. Update mise skill to document Forage as the browser layer", "done": "mise fetch of a JS-rendered page uses CDP via Chrome Debug instead of webctl. Auth'd Google pages work via existing SSO session. chrome-log captures traffic from mise navigations. No webctl dependency."}, "status": "done", "order": 2, "created_at": "2026-02-06T20:11:41Z", "created_by": "spm1001", "parent": "mise-reriri", "waiting_for": null, "updated_at": "2026-02-15T20:03:50Z", "done_at": "2026-02-15T20:03:50Z"}
{"id": "mise-geguwo", "type": "outcome", "title": "xlsx fetch preserves formulae or warns about formula loss", "brief": {"why": "WARC workbook had 4004 formula cells encoding classification logic. mise fetch converted to CSV, losing all of them. User had to flag that formulae existed. Had to bypass mise entirely (Drive API + openpyxl) to read the actual logic.", "what": "Either: (1) deposit the raw .xlsx alongside content.csv, or (2) add formula_count to cues so the caller knows the CSV is lossy, or (3) both", "done": "Caller can read formulae from deposited xlsx, or at minimum knows to fetch the raw file separately"}, "status": "done", "order": 41, "created_at": "2026-02-16T16:39:04Z", "created_by": "spm1001", "done_at": "2026-02-17T21:48:17Z"}
{"id": "mise-gelopa", "type": "action", "title": "Port apps-script from mcp-google-workspace", "brief": {"why": "Email exfiltration (Gmail attachments → Drive) is infrastructure that enables mise-en-space's 'Drive = canonical surface' philosophy. Currently lives in mcp-google-workspace which is being wound down.", "what": "1. Copy mcp-google-workspace/apps-script/ to mise-en-space/apps-script/ 2. Update apps-script/README.md paths if needed 3. Add to setup checklist in README.md 4. Add prerequisite row in CLAUDE.md 5. Verify itv-appscript-deploy still works from new location", "done": "apps-script/ exists in mise-en-space, setup docs mention it as a required step, can deploy from new location"}, "status": "open", "parent": "mise-tagemu", "order": 1, "created_at": "2026-01-29T22:32:57Z", "created_by": "spm1001", "waiting_for": null}
{"id": "mise-gifiku", "type": "action", "title": "Integration test for Gmail attachment extraction", "brief": {"why": "Current tests are unit-level; need to verify download → extract → deposit flow with real thread", "what": "Create test fixture with PDF attachment, write integration test that fetches thread and verifies PDF content is deposited", "done": "Integration test passes with real Gmail API call"}, "status": "done", "parent": "mise-4mj", "order": 5, "created_at": "2026-02-01T10:59:01Z", "created_by": "spm1001", "waiting_for": null, "tactical": {"steps": ["Find a test thread with PDF attachment", "Write integration test that fetches thread and verifies PDF deposited", "Run test against real API"], "current": 3}, "done_at": "2026-02-09T06:14:03Z"}
{"id": "mise-gizige", "type": "action", "title": "Files can be moved between Drive folders", "brief": {"why": "v1 MCP had move_file capability. Users discovered gap when filing workflows broke. Subagent research confirmed this is a real need.", "what": "1. Add operation='move' handler in do tool 2. Accept source_id + dest_folder_id params 3. Port move logic from v1 (single-parent enforcement) 4. Return post-action cues (new parent folder, web_link) 5. Test with filing workflow", "done": "Files can be moved between Drive folders; filing skill workflows work"}, "status": "done", "parent": "mise-hijute", "order": 2, "created_at": "2026-01-31T22:07:37Z", "created_by": "spm1001", "waiting_for": null, "tactical": {"steps": ["Add operation='move' handler in do tool", "Accept source_id + dest_folder_id params", "Port move logic from v1 (single-parent enforcement)", "Return post-action cues (new parent folder, web_link)", "Test with filing workflow"], "current": 5, "session": "/home/modha/Repos/mise-en-space"}, "updated_at": "2026-02-15T22:22:10Z", "done_at": "2026-02-15T22:22:10Z"}
{"id": "mise-gocojo", "type": "action", "title": "Migrate remaining adapter tests to mock_api_chain", "brief": {"why": "4 adapter test files still use raw MagicMock chaining: test_sheets_adapter (7), test_docs_adapter (3), test_gmail_adapter (3), test_charts_adapter (4). Inconsistent patterns make tests harder to maintain.", "what": "1. Refactor test_sheets_adapter.py 2. Refactor test_docs_adapter.py 3. Refactor test_gmail_adapter.py 4. Refactor test_charts_adapter.py", "done": "All service-chain .execute.return_value/.side_effect patterns migrated to mock_api_chain. 4 non-service patterns remain intentionally: 2 batch callback mocks (Gmail) and 2 local mock_request objects (Slides) — these are structurally different from service chains."}, "status": "done", "parent": "mise-3uu", "order": 18, "created_at": "2026-02-09T08:52:01Z", "created_by": "spm1001", "waiting_for": null, "tactical": {"steps": ["Refactor test_sheets_adapter.py", "Refactor test_docs_adapter.py", "Refactor test_gmail_adapter.py", "Refactor test_charts_adapter.py"], "current": 4}, "done_at": "2026-02-09T09:25:17Z"}
{"id": "mise-gofeBo", "type": "action", "title": "Decide v1 skill strategy", "brief": {"why": "mcp-google-workspace/skill-google-workspace still exists but references v1 tools. Either archive it or create a mise-specific skill.", "what": "After skill probe completes, decide: archive v1 skill / create mise skill / no skill needed", "done": "v1 skill archived or replaced"}, "status": "done", "parent": "mise-tuguzi", "order": 3, "created_at": "2026-02-01T10:59:54Z", "created_by": "spm1001", "waiting_for": null, "done_at": "2026-02-09T15:23:09Z"}
{"id": "mise-gtg", "title": "Factor Office file extraction into adapter", "type": "action", "status": "done", "done_at": "2026-01-24T20:28:57.425329Z", "brief": {"why": "DOCX/XLSX/PPTX extraction via Drive conversion is hardcoded in fetch.py. Should be adapter for testability.", "what": "## Current State\n\nIn tools/fetch.py:\n- `fetch_office()` handles DOCX, XLSX, PPTX\n- Uses upload → convert → export → delete pattern\n- Creates `_mise_temp_{file_id}` files in Drive\n\n## Problem\n\n1. Not testable without Drive access\n2. Pattern duplicates PDF's Drive conversion logic\n3. Temp file cleanup is best-effort (orphans accumulate)\n\n## Approach\n\nCreate `adapters/office.py`:\n\n```python\n@dataclass\nclass OfficeExtractionResult:\n    content: str\n    source_type: Literal[\"docx\", \"xlsx\", \"pptx\"]\n    converted_type: Literal[\"doc\", \"sheet\", \"slides\"]\n    warnings: list[str]\n\ndef extract_office_content(\n    file_path: Path,\n    service: Resource,\n    source_mime: str\n) -> OfficeExtractionResult:\n    \"\"\"\n    Extract Office file via Drive conversion.\n    \n    1. Upload to Drive with conversion\n    2. Export as text/markdown\n    3. Delete temp file (best effort)\n    \"\"\"\n```\n\n## Shared Infrastructure\n\nBoth PDF and Office use Drive conversion. Consider:\n- `adapters/drive_conversion.py` for shared upload/convert/export/delete\n- Used by both pdf.py and office.py\n\n## Acceptance Criteria\n\n- [ ] adapters/office.py exists with typed interface\n- [ ] fetch.py imports from adapter\n- [ ] Shared conversion logic factored out\n- [ ] Unit tests with mocked Drive responses\n- [ ] Temp file cleanup properly logged/warned", "done": "When complete"}, "created_at": "2026-01-24T20:09:22.044973Z", "created_by": "spm1001", "order": 1, "parent": "mise-52d", "waiting_for": null}
{"id": "mise-gugucu", "type": "action", "title": "Create actions under mise-geguwo for formula awareness and raw xlsx deposit", "brief": {"why": "WARC workbook had 4004 formula cells silently flattened to CSV. Outcome exists but has no children to execute.", "what": "1. Formula count in cues — second batchGet with FORMULA render, count =cells, surface in cues.formula_count 2. Deposit raw xlsx alongside content.csv — export native xlsx, keep original filename 3. format=raw on fetch — general escape hatch for any file type", "done": "Three actions filed under mise-geguwo with full briefs"}, "status": "done", "parent": "mise-geguwo", "order": 1, "created_at": "2026-02-16T17:58:51Z", "created_by": "spm1001", "waiting_for": null, "done_at": "2026-02-17T20:46:19Z"}
{"id": "mise-h6m", "title": "Adapter mocking infrastructure", "type": "action", "status": "done", "done_at": "2026-01-25T15:49:18.642701Z", "brief": {"why": "Adapter tests currently need real Google credentials. Add mocking infrastructure so tests run without hitting real APIs. Enables CI, faster tests, no credential management.", "what": "See title", "done": "When complete"}, "created_at": "2026-01-23T18:04:03.309992Z", "created_by": "spm1001", "order": 1, "parent": "mise-52d", "waiting_for": null}
{"id": "mise-hadabu", "type": "action", "title": "Mock chain validation catches renamed API methods", "brief": {"why": "mock_api_chain centralizes setup but MagicMock still silently creates new attributes. A renamed adapter method won't fail tests — it returns MagicMock instead of fixture data.", "what": "1. Test unittest.mock.seal() after mock setup in one adapter test file 2. If seal works with Google API chaining, add seal_service() helper to tests/helpers.py 3. Retrofit one test class as proof of concept", "done": "A test that renames a mock chain (e.g., files.get → files.get_media) fails immediately instead of passing silently"}, "status": "done", "parent": "mise-3uu", "order": 17, "created_at": "2026-02-09T08:51:58Z", "created_by": "spm1001", "waiting_for": null, "tactical": {"steps": ["Test unittest.mock.seal() after mock setup in one adapter test file", "If seal works with Google API chaining, add seal_service() helper to tests/helpers.py", "Retrofit one test class as proof of concept"], "current": 3}, "done_at": "2026-02-09T09:15:33Z"}
{"id": "mise-hafojo", "type": "action", "title": "Implement image deposit validation", "brief": {"why": "Large images and unsupported formats deposited by fetch_gmail crash the Claude API with a hard 400 that poisons the conversation", "what": "1. Add MAX_IMAGE_BYTES (4.5MB), MAX_IMAGE_DIMENSION_PX (8000), SUPPORTED_IMAGE_MIME_TYPES constants 2. Pre-download size check using att.size in fetch_gmail loop 3. Post-download PIL dimension + format check in _deposit_attachment_content 4. Add skipped_images to manifest/metadata analogous to skipped_office 5. Update _is_extractable_attachment to exclude non-API-supported formats 6. Write unit tests for the validation logic", "done": "fetch_gmail with an image att > 4.5MB or > 8000px or non-JPEG/PNG/GIF/WebP MIME type produces skipped_images in metadata instead of depositing; existing tests pass"}, "status": "done", "parent": "mise-wemuve", "order": 1, "created_at": "2026-02-19T15:01:57Z", "created_by": "spm1001", "waiting_for": null, "tactical": {"steps": ["Add MAX_IMAGE_BYTES (4.5MB), MAX_IMAGE_DIMENSION_PX (8000), SUPPORTED_IMAGE_MIME_TYPES constants", "Pre-download size check using att.size in fetch_gmail loop", "Post-download PIL dimension + format check in _deposit_attachment_content", "Add skipped_images to manifest/metadata analogous to skipped_office", "Update _is_extractable_attachment to exclude non-API-supported formats", "Write unit tests for the validation logic"], "current": 6, "session": "/Users/modha/Repos/mise-en-space"}, "updated_at": "2026-02-19T15:06:33Z", "done_at": "2026-02-19T15:06:33Z"}
{"id": "mise-halomi", "type": "action", "title": "Verify XLSX multi-tab fetch after MCP restart", "brief": {"why": "XLSX→Sheets API path (mise-lofeho) is code-complete and unit-tested but unverified live. MCP server needs restart to pick up changes. First real multi-tab xlsx fetch will confirm the upload_and_convert → fetch_spreadsheet → extract_sheets_content → delete_temp pipeline works end-to-end.", "what": "1. Restart MCP server 2. Fetch OHID Survey xlsx (1ZCmdo3OE46hROf2Mr8_irqWeaIjbK_Md) 3. Verify content.csv has === Sheet: === headers for both tabs 4. Check temp Google Sheet was deleted", "done": "OHID Survey deposits both tabs with valid per-sheet CSV. No orphaned temp files in Drive."}, "status": "done", "parent": "mise-wocidi", "order": 7, "created_at": "2026-02-15T21:10:35Z", "created_by": "spm1001", "waiting_for": null, "tactical": {"steps": ["Restart MCP server", "Fetch OHID Survey xlsx (1ZCmdo3OE46hROf2Mr8_irqWeaIjbK_Md)", "Verify content.csv has === Sheet: === headers for both tabs", "Check temp Google Sheet was deleted"], "current": 4, "session": "/home/modha/Repos/mise-en-space"}, "updated_at": "2026-02-15T21:30:55Z", "done_at": "2026-02-15T21:30:55Z"}
{"id": "mise-hefoVu", "type": "outcome", "title": "Support text/plain files in fetch", "brief": {"why": "Plain text files like code.txt return 'unsupported type' - should just download and deposit", "what": "Add text/plain handler to fetch routing, download file, write to deposit folder", "done": "fetch('text-file-id') works for .txt, .csv, .json, etc"}, "status": "done", "order": 4, "created_at": "2026-01-29T21:47:24Z", "created_by": "spm1001", "done_at": "2026-01-29T22:20:56Z"}
{"id": "mise-hijute", "type": "outcome", "title": "Claude can act on Workspace, not just read it", "brief": {"why": "The MCP currently fetches and searches but can't organise. Users hit this in filing workflows (move), after creating docs (share/open), and when managing folders. The 3rd verb ('do') is designed but unbuilt.", "what": "1. Rename create→do with operation param 2. Move files between folders 3. Post-action cues on create 4. Rename files 5. Share/permission operations", "done": "do(operation=create|move|rename|share) works, with post-action cues on all operations"}, "status": "open", "parent": null, "order": 28, "created_at": "2026-02-09T23:15:48Z", "created_by": "spm1001"}
{"id": "mise-hiwoti", "type": "action", "title": "Corporate signatures with <3 URLs leak through strip_trailing_contact_block", "brief": {"why": "Ross Partington's ITV corporate signature (name, title, phone, 2 URLs) survives stripping because _strip_trailing_contact_block requires 3+ URLs. Affects many ITV internal threads. Compound quality gap — every thread with corporate sigs has noise.", "what": "1. Lower URL threshold or add alternative detection (title+phone pattern) 2. Test against real corporate signatures from ITV threads 3. Ensure no false positives on content with casual URLs", "done": "Ross Partington's ITV signature stripped from VOD measurement thread messages. No false positives on content-with-links tests."}, "status": "done", "parent": "mise-wocidi", "order": 6, "created_at": "2026-02-15T21:09:43Z", "created_by": "spm1001", "waiting_for": null, "tactical": {"steps": ["Lower URL threshold or add alternative detection (title+phone pattern)", "Test against real corporate signatures from ITV threads", "Ensure no false positives on content with casual URLs"], "current": 3, "session": "/home/modha/Repos/mise-en-space"}, "updated_at": "2026-02-15T21:29:41Z", "done_at": "2026-02-15T21:29:41Z"}
{"id": "mise-hnq", "title": "Add capture_slides_fixture.py script", "type": "action", "status": "done", "done_at": "2026-01-23T18:04:16.594043Z", "brief": {"why": "Consistent fixture capture workflow for slides, matching capture_fixtures.py pattern. Should fetch presentation and optionally sanitize.", "what": "See title", "done": "When complete"}, "created_at": "2026-01-23T17:31:18.063965Z", "created_by": "spm1001", "order": 1, "waiting_for": null}
{"id": "mise-i93", "title": "Reverse-engineer Drive video transcript API", "type": "action", "status": "done", "done_at": "2026-01-24T11:18:24.906334Z", "brief": {"why": "The /timedtext endpoint returns ASR transcripts in json3 format. Need to figure out: (1) Drive file ID → internal video ID mapping, (2) how authpayload is generated, (3) the redirect flow that establishes the session.", "what": "## Discovery (Jan 2026)\n\nEndpoint: `https://drive.google.com/u/0/timedtext?id={vid}&fmt=json3`\n\nSee: `bakeoff/video_transcript_discovery.md`\n\n## Approach\n\n1. Use webctl or Playwright to intercept network requests\n2. Trace the redirect chain (302 → 200)\n3. Find where authpayload comes from\n4. Test if Drive API exposes internal video ID\n\n## Alternative: claude-in-chrome\n\nThe Anthropic claude-in-chrome extension might provide dev tools access more naturally than webctl.", "done": "When complete"}, "created_at": "2026-01-24T08:53:49.194025Z", "created_by": "spm1001", "order": 1, "waiting_for": null}
{"id": "mise-jinaro", "type": "action", "title": "Wire do(create, sheet) via Drive CSV upload", "brief": {"why": "Benchmark proved Drive CSV upload is the right path: 94% type detection, 6s for 5K rows, ~10 lines of code. Same pattern as existing _create_doc but with text/csv mimetype. No formatting pass needed — bold/freeze are one-click in the UI. No new dependencies.", "what": "1. Add _create_sheet to tools/create.py — read CSV from deposit, upload with text/csv mimetype targeting google-apps.spreadsheet 2. Route doc_type=sheet through new path in do_create 3. Return CreateResult with web_link and cues (same as doc creation) 4. Handle errors: missing deposit, empty CSV 5. Unit test with mock Drive API", "done": "do(operation=create, doc_type=sheet, source=path) creates a Google Sheet from deposited CSV. Same response shape as doc creation. Errors handled gracefully."}, "status": "done", "parent": "mise-beriji", "order": 4, "created_at": "2026-02-16T13:29:02Z", "created_by": "spm1001", "waiting_for": null, "updated_at": "2026-02-16T16:53:50Z", "tactical": {"steps": ["Add _create_sheet to tools/create.py with text/csv mimetype", "Route doc_type=sheet through new path in do_create", "Update tests — replace not_implemented assertion, add sheet creation tests", "Run full test suite"], "current": 4, "session": "/home/modha/Repos/mise-en-space"}, "done_at": "2026-02-16T16:53:50Z"}
{"id": "mise-jiseti", "type": "action", "title": "Gmail compose/draft scope in mise OAuth", "brief": {"why": "Gmail compose scope is a prerequisite for the broader 'act on Workspace' outcome. Current mise token is read-only for Gmail — can search and fetch threads but can't create drafts or send. Discovered during GCloud audit when trying to draft an email from findings. The scope gap blocks any Gmail write operation in the do() verb.", "what": "1. Add gmail.compose scope to OAuth flow in auth.py 2. Update token refresh to include new scope 3. Test draft creation via gmail.users().drafts().create() 4. Document the new capability", "done": "Can create Gmail drafts via mise's OAuth token. Existing read-only flows still work. Token refresh handles the new scope without re-auth (or re-auth is documented if unavoidable)."}, "status": "open", "order": 4, "created_at": "2026-02-12T08:32:26Z", "created_by": "spm1001", "parent": "mise-hijute", "waiting_for": null, "updated_at": "2026-02-15T19:32:09Z"}
{"id": "mise-jizako", "type": "action", "title": "Build shared mock helper and refactor fetch_sheet tests", "brief": {"why": "7-8 @patch decorators per test, adding a field means touching every test method", "what": "1. Study current TestFetchSheet boilerplate to understand the common mock shape 2. Create helper in tests/helpers.py that builds mocked sheet context with defaults 3. Refactor TestFetchSheet tests to use the helper 4. Run tests to verify no regressions", "done": "Tests pass, adding a new field to sheet fetch path doesn't require N test updates"}, "status": "done", "parent": "mise-maduza", "order": 1, "created_at": "2026-02-18T13:48:38Z", "created_by": "spm1001", "waiting_for": null, "tactical": {"steps": ["Study current TestFetchSheet boilerplate to understand the common mock shape", "Create helper in tests/helpers.py that builds mocked sheet context with defaults", "Refactor TestFetchSheet tests to use the helper", "Run tests to verify no regressions"], "current": 4, "session": "/home/modha/Repos/mise-en-space"}, "updated_at": "2026-02-18T13:52:13Z", "done_at": "2026-02-18T13:52:13Z"}
{"id": "mise-jodeja", "type": "action", "title": "Pre-exfil attachment detection", "brief": {"why": "Drive indexes PDF content via fullText, Gmail doesn't. Background extractor saves attachments to Drive. Fetch should use indexed copy when available.", "what": "1. Config for Email Attachments folder ID (env var or discover) 2. Query Drive for attachment by name in folder 3. Use Drive copy if found, else Gmail fallback", "done": "Looks up attachments in Drive folder, uses Drive copy when found, graceful fallback when not"}, "status": "done", "parent": "mise-4mj", "order": 2, "created_at": "2026-01-29T22:14:34Z", "created_by": "spm1001", "waiting_for": null, "done_at": "2026-02-08T18:40:32Z"}
{"id": "mise-jonaha", "type": "action", "title": "Wire pre-exfil lookup in fetch_gmail", "brief": {"why": "Pre-exfil optimization skips Gmail download for already-indexed files", "what": "Call lookup_exfiltrated() in fetch_gmail before downloading attachments. If file exists in Drive with Message ID match, fetch from Drive instead of Gmail.", "done": "fetch_gmail checks pre-exfil folder, uses Drive file when available"}, "status": "done", "parent": "mise-4mj", "order": 4, "created_at": "2026-02-01T10:59:00Z", "created_by": "spm1001", "waiting_for": null, "tactical": {"steps": ["Trace current attachment handling in fetch_gmail/fetch tool", "Add lookup_exfiltrated() call before attachment download", "Route matched attachments through Drive fetch instead of Gmail download", "Test with unit test (mock lookup_exfiltrated)", "Verify graceful fallback when no exfil folder"], "current": 5}, "done_at": "2026-02-08T18:44:43Z"}
{"id": "mise-jonofu", "type": "action", "title": "Multi-tab sheet creation via Sheets API hybrid path", "brief": {"why": "Current CSV upload creates a single tab. Callers with multi-tab data (e.g. fetched xlsx with 3 tabs, or BigQuery results for different dimensions) can't create a proper workbook. itv-linkedin-analytics proved the Sheets API write path works well — gspread pattern of create/find tab + values().update(USER_ENTERED) handles formulae and multiple tabs.", "what": "1. Add write functions to adapters/sheets.py: create_spreadsheet(), update_values(spreadsheet_id, range, values, value_input_option=USER_ENTERED), add_sheet(spreadsheet_id, title) 2. New creation path in tools/create.py: CSV upload for tab 1 (existing, 94% type detection), then addSheet + values().batchUpdate for additional tabs 3. Convention for multi-tab source deposits: multiple content_*.csv files, manifest lists tab order 4. Formulae in CSV cells starting with = are preserved via USER_ENTERED 5. Unit tests with multi-tab source folder 6. Benchmark: timing for 3-tab workbook creation", "done": "do(operation=create, doc_type=sheet, source=path) with a multi-tab deposit creates a Google Sheet with multiple named tabs. Formulae in cells work. Single-tab deposits still use the fast CSV upload path."}, "status": "done", "parent": "mise-beriji", "order": 10, "created_at": "2026-02-17T20:46:14Z", "created_by": "spm1001", "waiting_for": null, "tactical": {"steps": ["Add Sheets API write functions to adapters/sheets.py: add_sheet(), update_sheet_values()", "Update _create_sheet() in tools/create.py to detect multi-tab deposits and use hybrid path: CSV upload for tab 1, Sheets API for additional tabs", "Unit tests: multi-tab source deposit creates multi-tab sheet, single-tab unchanged, formulae preserved via USER_ENTERED", "Integration smoke test timing (optional, needs credentials)"], "current": 4, "session": "/home/modha/Repos/mise-en-space"}, "updated_at": "2026-02-17T21:23:15Z", "done_at": "2026-02-17T21:23:15Z"}
{"id": "mise-jowawe", "type": "action", "title": "Dead code sweep", "brief": {"why": "Unused functions, stale forward declarations, dead extractor variants add cognitive load and mislead future Claudes", "what": "1. Remove unwired functions (do_search_activity, do_fetch_comments, workspace inspection trio, dead gmail extractor _extract_drive_links) 2. Remove test-only adapter functions (fetch_message, export_file, get_sapisid) or mark clearly 3. Remove unused validation helpers (is_valid_email, normalize_email) 4. Remove forward-declared services (tasks, calendar, labels) 5. Fix _parse_email_context visibility (underscore prefix)", "done": "No production function is uncalled, no forward declarations for unbuilt features, imports clean"}, "status": "done", "parent": "mise-niraci", "order": 2, "created_at": "2026-02-18T22:05:44Z", "created_by": "spm1001", "waiting_for": null, "tactical": {"steps": ["Remove unwired functions (do_search_activity, do_fetch_comments, workspace inspection trio, dead gmail extractor _extract_drive_links)", "Remove test-only adapter functions (fetch_message, export_file, get_sapisid) or mark clearly", "Remove unused validation helpers (is_valid_email, normalize_email)", "Remove forward-declared services (tasks, calendar, labels)", "Fix _parse_email_context visibility (underscore prefix)"], "current": 5, "session": "/home/modha/Repos/mise-en-space"}, "updated_at": "2026-02-18T22:16:32Z", "done_at": "2026-02-18T22:16:32Z"}
{"id": "mise-jusoga", "type": "action", "title": "Resize oversized images before deposit rather than skipping", "brief": {"why": "Current behaviour skips images exceeding 4.5MB or 8000px and leaves a note in metadata. This is not mise-en-place — the chef gets a rejection note instead of prepped ingredients. Valid images that are simply too large should be scaled down to a usable size.", "what": "1. Add resize logic: if max(w,h) > 1568px, scale to 1568px long edge (Anthropics own optimal threshold — API downscales internally above this anyway, no quality benefit) 2. Keep original format (JPEG stays JPEG, PNG stays PNG — avoids size inflation and quality loss of PNG conversion) 3. If size still > 4.5MB after resize (rare edge case): convert PNG to JPEG as fallback 4. Deposit resized bytes, note original dimensions and scale factor in metadata 5. Only skip if PIL cannot open the bytes at all (genuine MIME mismatch — cannot be fixed) 6. Update skipped_images logic and tests accordingly", "done": "fetch_gmail with a 2746x1908 PNG deposits a 1568px-wide version; metadata notes original_dimensions and scaled_to; session is not poisoned; PIL-unreadable bytes still skip with reason"}, "status": "open", "parent": "mise-jy3", "order": 23, "created_at": "2026-02-20T08:32:00Z", "created_by": "spm1001", "waiting_for": null}
{"id": "mise-jutike", "type": "action", "title": "Create purpose-built test doc with rich comments", "brief": {"why": "Current test_doc_with_comments_id reuses test_doc_id — works but lacks dedicated edge cases", "what": "Create Google Doc in test folder with: threaded replies, resolved/unresolved mix, @mentions, long anchor quotes, empty anchors (like DOCX)", "done": "Fixture exists with predictable comment patterns for regression testing"}, "status": "done", "parent": "mise-3uu", "order": 7, "created_at": "2026-02-01T10:59:21Z", "created_by": "spm1001", "waiting_for": null, "tactical": {"steps": ["Create Google Doc in test folder with sections to anchor comments on", "Add comments via Drive API: threaded replies, resolved+unresolved, @mention, long anchor, empty anchor", "Capture fixture with capture_fixtures.py", "Verify fixture has predictable comment patterns for regression"], "current": 4}, "done_at": "2026-02-09T11:26:29Z"}
{"id": "mise-jvz", "title": "Wire fetch tool — with pre-exfil detection", "type": "action", "status": "done", "done_at": "2026-01-23T22:33:09.699182Z", "brief": {"why": "Fetch content to filesystem. Auto-detects ID type, checks for pre-exfiltrated attachments.", "what": "## Signature\n\nfetch(id: str) -> dict\n\n## Response Shape\n\n{\n  \"path\": \"~/.mcp-workspace/account/drive/abc123.md\",\n  \"format\": \"markdown\",\n  \"metadata\": {\"title\": \"...\", \"mimeType\": \"...\", \"modified\": \"...\"}\n}\n\n## ID Auto-Detection\n\n1. If looks like Gmail URL → extract thread ID\n2. If looks like Drive URL → extract file ID\n3. If hex-ish (194ac68f...) → Gmail thread/message\n4. Otherwise → Drive file ID\n\n## Pre-Exfiltration Detection\n\nBefore downloading Gmail attachment:\n1. Check \"Email Attachments\" folder for matching file\n2. Match by: filename + size + approximate date\n3. If found → fetch from Drive (already indexed, searchable)\n4. If not → download from Gmail (3x faster than Drive for same binary)\n\n## Routing by Type\n\n- Google Doc → docs extractor → markdown\n- Google Sheet → sheets extractor → CSV  \n- Google Slides → slides extractor → structured text\n- Gmail thread → gmail extractor → cleaned text\n- PDF → markitdown → markdown\n- Office → Drive conversion → appropriate extractor\n- Binary → download to workspace, return path\n\n## Acceptance Criteria\n\n- [ ] Auto-detects ID type\n- [ ] Handles Gmail URLs\n- [ ] Checks pre-exfil folder before Gmail download\n- [ ] Routes to correct extractor\n- [ ] Returns path, not content\n\n## URL Handling (REQUIRED)\n\nAccept Gmail URLs and convert to thread ID:\n- https://mail.google.com/mail/u/0/#inbox/18f4a... → extract thread_id\n- Port the URL extraction logic from v1 validation.py\n", "done": "When complete"}, "created_at": "2026-01-23T07:25:15.084528Z", "created_by": "spm1001", "order": 1, "parent": "mise-52d", "waiting_for": null}
{"id": "mise-jy3", "title": "PDF extraction decisions are made and test infrastructure is solid", "type": "outcome", "status": "open", "brief": {"why": "v1 had PDF features we rushed past (thumbnails, visual extraction). This outcome ensures we consciously decide rather than accidentally omit. Also covers test infrastructure gaps discovered during blind testing.", "what": "1. PDF visual extraction strategy decided — always text, always images, hybrid, or user parameter (ogf) 2. v1 PDF thumbnails evaluated — port, decline, or defer with rationale (wonaru) 3. Test Claudes deposit to correct locations — root cause identified and fixed (BoVuMo)", "done": "PDF extraction strategy documented in CLAUDE.md with decision rationale. v1 thumbnails either ported or declined (not silently omitted). Test Claude deposits land in expected directories."}, "created_at": "2026-01-24T20:16:53.534947Z", "created_by": "spm1001", "order": 5, "updated_at": "2026-02-15T19:31:58Z"}
{"id": "mise-k6g", "title": "Factor PDF extraction into adapter", "type": "action", "status": "done", "done_at": "2026-01-24T20:28:57.330853Z", "brief": {"why": "PDF hybrid extraction logic (markitdown → Drive fallback) is hardcoded in fetch.py. Should be an adapter for testability and reuse.", "what": "## Current State\n\nIn tools/fetch.py:\n- `fetch_pdf()` function handles all PDF logic\n- MARKITDOWN_MIN_CHARS = 500 hardcoded constant\n- Hybrid strategy: try markitdown, fall back to Drive if <500 chars\n\n## Problem\n\n1. Not testable without real files (no adapter abstraction)\n2. Threshold not parameterized\n3. Logic entangled with file writing (workspace concerns)\n\n## Approach\n\nCreate `adapters/pdf.py`:\n\n```python\n@dataclass\nclass PdfExtractionResult:\n    content: str\n    method: Literal[\"markitdown\", \"drive\"]\n    char_count: int\n    warnings: list[str]\n\ndef extract_pdf_content(\n    file_path: Path | None = None,\n    file_id: str | None = None,\n    service: Resource | None = None,\n    min_chars_threshold: int = 500\n) -> PdfExtractionResult:\n    \"\"\"\n    Hybrid PDF extraction.\n    \n    If file_path provided: use markitdown directly\n    If file_id provided: download then markitdown, fall back to Drive conversion\n    \"\"\"\n```\n\nMove from fetch.py:\n- `_extract_with_markitdown()` \n- `_extract_with_drive_conversion()`\n- Threshold constant\n\n## Acceptance Criteria\n\n- [ ] adapters/pdf.py exists with typed interface\n- [ ] fetch.py imports from adapter (thin wiring only)\n- [ ] min_chars_threshold is parameterized\n- [ ] Unit tests with mocked markitdown responses\n- [ ] Integration test with real PDF", "done": "When complete"}, "created_at": "2026-01-24T20:09:12.256743Z", "created_by": "spm1001", "order": 1, "parent": "mise-52d", "waiting_for": null}
{"id": "mise-k80", "title": "Test MCP resources in real Claude session", "type": "action", "status": "done", "brief": {"why": "MCP resources are registered but not verified in actual use. Need to confirm Claude can read mise:// URIs.", "what": "## Test Plan\n\n1. Start mise-en-space MCP (replace v1 or run alongside)\n2. In Claude session, request resource: \"Read mise://docs/fetch\"\n3. Verify content returns correctly\n4. Test all 5 resources:\n   - mise://docs/overview\n   - mise://docs/search\n   - mise://docs/fetch\n   - mise://docs/create\n   - mise://docs/workspace\n\n## Acceptance Criteria\n- [ ] Claude can read each resource\n- [ ] Content matches what's in server.py\n- [ ] No errors or empty responses", "done": "When complete"}, "created_at": "2026-01-25T17:28:40.189716Z", "created_by": "spm1001", "order": 1, "parent": "mise-6kh", "waiting_for": null, "done_at": "2026-01-31T22:06:46Z"}
{"id": "mise-k97", "title": "Investigate Google Groups content access", "type": "action", "status": "done", "done_at": "2026-01-24T21:27:09.680923Z", "brief": {"why": "Research how to fetch Google Groups conversation archives. Individual Gmail inboxes aren't reliable (subscriptions vary, digest mode, membership changes). Need to access group archives directly.", "what": "## Goal\n\nEnable `fetch(group_id)` or `search(source=\"groups\")` to access Google Groups discussions.\n\n## Research Questions\n\n1. **Groups Migration API** — Can it read/export posts without admin?\n   - Docs: https://developers.google.com/admin-sdk/groups-migration\n   - Test: Try to list/export from a group we have access to\n\n2. **groups.google.com scraping** — What's the DOM structure?\n   - Auth: Does browser session (webctl) work?\n   - Pagination: How are threads listed?\n   - Content: Can we get full thread content?\n\n3. **Cloud Identity API** — Any content access?\n   - Docs: https://cloud.google.com/identity/docs/groups\n\n4. **Workspace admin approaches** — What's possible with elevated access?\n\n## Acceptance Criteria\n\n- [ ] Document which API/approach works (or confirm none do)\n- [ ] If viable: create follow-up bead with implementation design\n- [ ] If not viable: document why and close\n\n## Test Groups\n\n- MIT team groups (if accessible)\n- Public Google Groups (for unauthenticated testing)", "done": "When complete"}, "created_at": "2026-01-24T20:40:48.224167Z", "created_by": "spm1001", "order": 1, "parent": "mise-6ku", "waiting_for": null}
{"id": "mise-kaRuro", "type": "action", "title": "Sanitized test fixtures", "brief": {"why": "Test fixtures contain real data (ITV budget figures, email addresses). Required before open-sourcing or sharing.", "what": "Create sanitized fixture versions with fake data that preserves structure", "done": "All fixtures sanitized, no real PII/business data, structure preserved for tests"}, "status": "done", "parent": "mise-3uu", "order": 4, "created_at": "2026-01-29T22:14:41Z", "created_by": "spm1001", "waiting_for": null, "done_at": "2026-02-09T07:20:08Z"}
{"id": "mise-kabaGi", "type": "action", "title": "Deposit folder cleanup strategy", "brief": {"why": "mise-fetch/ folders accumulate with no cleanup — unbounded growth over time.", "what": "1. Add cleanup_after timestamp to manifest.json 2. Add cleanup function to workspace/manager.py 3. Document cleanup approach", "done": "manifest.json has cleanup_after field, cleanup function exists, documented"}, "status": "done", "parent": "mise-jy3", "order": 3, "created_at": "2026-01-29T22:14:45Z", "created_by": "spm1001", "waiting_for": null, "done_at": "2026-02-09T23:15:31Z"}
{"id": "mise-kafolu", "type": "action", "title": "Slim CLAUDE.md and move decision history", "brief": {"why": "638 lines / ~10k tokens loaded every session — 60% is post-hoc documentation of implemented code, not guidance", "what": "1. Create docs/decisions.md with full decision history 2. Slim CLAUDE.md to essential every-session guidance (~250 lines) 3. Add missing recipes (new content type, new do operation) 4. Move implementation details to code comments where appropriate", "done": "CLAUDE.md under 300 lines, docs/decisions.md exists with full history, recipes for common extension tasks present"}, "status": "done", "parent": "mise-niraci", "order": 1, "created_at": "2026-02-18T22:05:39Z", "created_by": "spm1001", "waiting_for": null, "tactical": {"steps": ["Create docs/decisions.md with full decision history", "Slim CLAUDE.md to essential every-session guidance (~250 lines)", "Add missing recipes (new content type, new do operation)", "Move implementation details to code comments where appropriate"], "current": 4, "session": "/home/modha/Repos/mise-en-space"}, "updated_at": "2026-02-18T22:09:17Z", "done_at": "2026-02-18T22:09:17Z"}
{"id": "mise-kamotu", "type": "action", "title": "Add unit tests for cues output", "brief": {"why": "cues block added to all 15 fetch return sites but no tests assert on content. Existing 992 tests pass because they predate cues. Untested code riding on tested code.", "what": "1. Test fetch_doc returns cues with correct open_comment_count 2. Test fetch_gmail includes participants list 3. Test email_context is null (not absent) when no email trail 4. Test files list matches actual deposit contents", "done": "At least 4 new tests covering cues contract for doc, gmail, search preview"}, "status": "done", "parent": "mise-jy3", "order": 7, "created_at": "2026-02-09T20:44:17Z", "created_by": "spm1001", "waiting_for": null, "tactical": {"steps": ["Fix 4 critical-path items from titans review", "Fix cherry-picked lower-priority items", "Add unit tests for cues output (kamotu)", "Add unit tests for search preview output"], "current": 4}, "done_at": "2026-02-09T21:08:28Z"}
{"id": "mise-kecigu", "type": "outcome", "title": "Search surfaces richer context beyond Drive and Gmail", "brief": {"why": "Search currently returns Drive files and Gmail threads. But context lives in Activity API (who commented, who edited), Calendar (meeting context for docs), and Tasks (action items). Expanding search sources turns mise from a file finder into a context finder.", "what": "1. Activity API as search source (busafe) 2. Calendar context enrichment (siLugo) 3. Surface action items across APIs (NiKuki) 4. Test and exercise new API services (HoWeKe) 5. Sharper blind test: preview as triage replacement (milako)", "done": "search() can query activity and calendar sources, results include meeting context and action item signals"}, "status": "open", "order": 37, "created_at": "2026-02-15T19:26:48Z", "created_by": "spm1001"}
{"id": "mise-kineza", "type": "action", "title": "Benchmark and document expected latencies", "brief": {"why": "Speed is a nagging concern — unclear if it's MCP startup, search latency, extraction time, or all three. Team will notice if it's slow but won't diagnose why.", "what": "1. Measure MCP server startup time 2. Benchmark search (Drive, Gmail, both) 3. Benchmark fetch for each content type (doc, sheet, slides, PDF, web) — run live, don't reuse old numbers 4. Document expected latencies in README or skill 5. Identify worst offenders and file optimisation items if needed", "done": "README or skill has 'what to expect' section with typical latencies. Worst-case paths identified."}, "status": "done", "parent": "mise-naviho", "order": 7, "created_at": "2026-02-07T18:57:14Z", "created_by": "spm1001", "waiting_for": null, "tactical": {"steps": ["Measure MCP server startup time", "Benchmark search (Drive, Gmail, both)", "Benchmark fetch for each content type (doc, sheet, slides, PDF, web)", "Document expected latencies in README or skill", "Identify worst offenders and file optimisation items if needed"], "current": 5}, "done_at": "2026-02-09T06:30:00Z"}
{"id": "mise-kivati", "type": "action", "title": "Investigate slides thumbnail parallelism", "brief": {"why": "Slides fetch for 7 slides takes ~6s. Thumbnails are fetched sequentially (~0.5s each). Google disabled HTTP batch for editor APIs in 2022, but concurrent individual requests might still work.", "what": "1. Test concurrent.futures.ThreadPoolExecutor for parallel getThumbnail calls 2. Verify Google doesn't rate-limit concurrent thumbnail requests 3. If parallel works, measure improvement 4. If rate-limited, document the constraint", "done": "Either: thumbnail fetch is parallel and slides benchmark drops by 40%+, or documented that Google rate-limits concurrent requests."}, "status": "done", "parent": "mise-weduje", "order": 3, "created_at": "2026-02-09T06:37:12Z", "created_by": "spm1001", "waiting_for": null, "tactical": {"steps": ["Test concurrent.futures.ThreadPoolExecutor for parallel getThumbnail calls", "Verify Google doesn't rate-limit concurrent thumbnail requests", "If parallel works, measure improvement", "If rate-limited, document the constraint"], "current": 4}, "done_at": "2026-02-09T13:42:42Z"}
{"id": "mise-kojiti", "type": "action", "title": "Add symlink setup to README", "brief": {"why": "Contributors cloning fresh need to create ~/.claude/skills/mise symlink manually", "what": "Add setup instructions to README for skill symlink", "done": "README has clear instructions for linking skill after clone"}, "status": "done", "order": 4, "created_at": "2026-02-01T20:05:48Z", "created_by": "spm1001", "parent": "mise-naviho", "waiting_for": null, "done_at": "2026-02-07T21:17:07Z"}
{"id": "mise-kuwija", "type": "action", "title": "Surgical edits: prepend, append, insert text at index", "brief": {"why": "The simpler of the two edit modes. Users need to prepend exec summaries, append notes, or insert at specific points without replacing the whole doc. Docs API batchUpdate supports insertText and replaceAllText natively.", "what": "1. Add insertText operation to do() tool (index-based insert) 2. Add replaceAllText for find-and-replace 3. Convenience wrappers: prepend (index 1) and append (index end) 4. Test round-trip: create → fetch → surgical edit → fetch again", "done": "Can prepend, append, and insert text into existing Google Docs via mise. Content at other positions is preserved."}, "status": "done", "parent": "mise-hijute", "order": 7, "created_at": "2026-02-15T19:30:30Z", "created_by": "spm1001", "waiting_for": null, "updated_at": "2026-02-18T21:39:55Z", "tactical": {"steps": ["Implement tools/edit.py with _insert_text helper, _get_doc_end_index", "Add do_prepend, do_append, do_replace_text operations", "Wire three operations into do() router in server.py", "Unit tests for all three operations", "Integration test: create → prepend → append → replace_text → verify"], "current": 5, "session": "/home/modha/Repos/mise-en-space"}, "done_at": "2026-02-18T21:39:55Z"}
{"id": "mise-lahero", "type": "action", "title": "Implement single-attachment fetch API", "brief": {"why": "Office files are skipped in eager extraction (5-10s each). Need explicit fetch path.", "what": "Add fetch(message_id, attachment=filename) API to download and extract a specific Office attachment on demand", "done": "Can fetch individual PPTX/DOCX/XLSX from Gmail with extraction"}, "status": "done", "parent": "mise-4mj", "order": 6, "created_at": "2026-02-01T10:59:02Z", "created_by": "spm1001", "waiting_for": null, "tactical": {"steps": ["Add attachment param to server.py", "Add fetch_attachment + routing in tools/fetch.py", "Update skipped_office_hint", "Write unit tests", "Update CLAUDE.md design decision"], "current": 5}, "done_at": "2026-02-08T21:36:21Z"}
{"id": "mise-lajeke", "type": "action", "title": "Existing Google Docs can be edited and overwritten in-place", "brief": {"why": "During GCloud audit, needed to prepend an exec summary to an existing doc. Mise's create tool makes new docs but can't edit them. Had to write a throwaway Python script using the Docs API directly via mise's token. The token already has auth/documents scope — the capability is there, just not exposed.", "what": "Two modes: 1. Surgical edits — prepend/append/insert text at index (batchUpdate with insertText/replaceText) 2. Wholesale markdown replacement — delete all content, insertText new, re-apply heading styles 3. Sheets and Slides editing for parity (stretch) 4. Update skill docs to reflect new capabilities", "done": "Can edit an existing Google Doc through mise — both surgical inserts and full content replacement. File ID, sharing, location preserved. Heading styles applied on overwrite."}, "status": "done", "order": 5, "created_at": "2026-02-12T08:47:20Z", "created_by": "spm1001", "updated_at": "2026-02-15T22:05:21Z", "parent": "mise-hijute", "waiting_for": null, "done_at": "2026-02-15T22:05:35Z"}
{"id": "mise-lakono", "type": "action", "title": "Route non-PDF binary web content to extractors", "brief": {"why": "Web URLs serving DOCX/XLSX/PPTX (SharePoint downloads, presigned S3 URLs, file hosting) still hit trafilatura HTML extraction and produce garbage. Only application/pdf is handled.", "what": "1. Add Office MIME types to BINARY_CONTENT_TYPES in adapters/web.py 2. Add routing in fetch_web() per content-type — application/vnd.openxmlformats-officedocument.* maps to Office extractors 3. Wire _fetch_web_office() similar to _fetch_web_pdf() — save raw_bytes to temp file, call fetch_and_extract_office with appropriate OfficeType 4. Test with mocked responses for each Office type", "done": "fetch('https://example.com/report.docx') extracts markdown. fetch('https://example.com/data.xlsx') extracts CSV. Unit tests for each Office MIME type."}, "status": "done", "parent": "mise-jy3", "order": 6, "created_at": "2026-02-07T16:18:00Z", "created_by": "spm1001", "waiting_for": null, "tactical": {"steps": ["Add Office MIME types to BINARY_CONTENT_TYPES in adapters/web.py", "Add routing in fetch_web() per content-type — application/vnd.openxmlformats-officedocument.* maps to Office extractors", "Wire _fetch_web_office() similar to _fetch_web_pdf() — save raw_bytes to temp file, call fetch_and_extract_office with appropriate OfficeType", "Test with mocked responses for each Office type"], "current": 4}, "done_at": "2026-02-07T18:23:25Z"}
{"id": "mise-lakusu", "type": "action", "title": "Arc tactical stale-state visibility", "brief": {"why": "arc done doesn't clear tactical steps. Next arc work on a different action fails until manual arc work --clear. Hit twice during mise-tuguzi session — friction that breaks flow.", "what": "1. Diagnose whether arc done should auto-clear tactical 2. Fix in arc CLI (clear on done, or clear-and-activate on work) 3. Remove need for manual --clear workaround", "done": "arc done X && arc work Y succeeds without manual --clear step"}, "status": "done", "parent": null, "order": 2, "created_at": "2026-02-09T06:48:32Z", "created_by": "spm1001", "waiting_for": null, "done_at": "2026-02-09T16:46:52Z"}
{"id": "mise-legani", "type": "outcome", "title": "Tighten tick-prefix leading-zero assertion in integration test", "brief": {"why": "test_create_sheet_leading_zeros accepts both '00412' and \"'00412\" — loose assertion because Drive CSV import behaviour for tick-prefix isn't fully documented", "what": "1. Run integration test and inspect actual Google return value 2. Tighten assertion to match what Google actually does 3. Document the behaviour in CLAUDE.md design decisions", "done": "Integration test asserts the exact form Google returns, not a loose in-check"}, "status": "done", "order": 43, "created_at": "2026-02-18T14:20:13Z", "created_by": "spm1001", "done_at": "2026-02-18T20:13:44Z"}
{"id": "mise-lft", "title": "Search deposits results to file instead of inline JSON", "type": "action", "status": "done", "brief": {"why": "Search should deposit results to mise-fetch/search--{query}/results.json instead of returning JSON inline. More token-efficient for calling Claude, especially when firing multiple searches.", "what": "## Approach\n\n1. Create deposit folder: `mise-fetch/search--{slugified-query}--{timestamp}/`\n2. Write results.json with drive_results + gmail_results\n3. Return {path, drive_count, gmail_count} instead of full JSON\n\n## Already done\n- contentSnippet in Drive results ✅\n- attachment_names in Gmail results ✅", "done": "When complete"}, "created_at": "2026-01-25T19:14:20.312794Z", "created_by": "spm1001", "order": 2, "parent": "mise-naviho", "waiting_for": null, "done_at": "2026-02-07T21:11:03Z"}
{"id": "mise-loMuzu", "type": "outcome", "title": "Activity fixture capture", "brief": {"why": "Activity adapter unit tests only cover models, not parsing logic. Need fixtures/activity/ with real API responses to test _parse_actor, _parse_target, _parse_comment_action.", "what": "Add capture_activity_fixtures() to scripts/capture_fixtures.py. Capture sample comment activities and file activities. Sanitize email addresses.", "done": "fixtures/activity/comment_activities.json and file_activities.json exist with realistic test data"}, "status": "done", "order": 16, "created_at": "2026-01-31T17:58:21Z", "created_by": "spm1001", "done_at": "2026-02-01T10:58:31Z"}
{"id": "mise-lodipa", "type": "outcome", "title": "Implement single-attachment fetch API", "brief": {"why": "Office files are skipped in eager extraction (5-10s each). Need explicit fetch path.", "what": "Add fetch(message_id, attachment=filename) API to download and extract a specific Office attachment on demand", "done": "Can fetch individual PPTX/DOCX/XLSX from Gmail with extraction"}, "status": "done", "order": 10, "created_at": "2026-01-29T23:25:59Z", "created_by": "spm1001", "done_at": "2026-02-01T10:58:29Z"}
{"id": "mise-lofeho", "type": "action", "title": "XLSX fetch returns only first tab; Sheets fetch merges tabs into invalid CSV", "brief": {"why": "Fetching OHID Survey xlsx (1ZCmdo3OE46hROf2Mr8_irqWeaIjbK_Md, 2 tabs) returned only tab 1. The xlsx path uses Drive CSV export which is single-sheet only. Converting to native Sheets returns both tabs but concatenated with === Sheet: === headers interleaved in a .csv file — not valid CSV. Two bugs, same root cause: xlsx conversion doesn't use the Sheets API path.", "what": "1. After xlsx→Google Sheet conversion, use fetch_spreadsheet() + extract_sheets_content() instead of Drive CSV export 2. Decide multi-tab deposit format: separate .csv per sheet, or single file with clear delimiters 3. Clean up conversion.py to support the hybrid path (upload via Drive, read via Sheets API, delete temp) 4. Test with OHID Survey fixture (2-tab xlsx)", "done": "Fetching a multi-tab xlsx deposits all tabs with valid per-sheet CSV. No === Sheet: === lines inside .csv files — either separate files or a format that tools can parse."}, "status": "done", "parent": "mise-wocidi", "order": 5, "created_at": "2026-02-15T19:27:34Z", "created_by": "spm1001", "waiting_for": null, "tactical": {"steps": ["After xlsx→Google Sheet conversion, use fetch_spreadsheet() + extract_sheets_content() instead of Drive CSV export", "Decide multi-tab deposit format: separate .csv per sheet, or single file with clear delimiters", "Clean up conversion.py to support the hybrid path (upload via Drive, read via Sheets API, delete temp)", "Test with OHID Survey fixture (2-tab xlsx)"], "current": 4, "session": "/home/modha/Repos/mise-en-space"}, "updated_at": "2026-02-15T20:53:01Z", "done_at": "2026-02-15T20:53:01Z"}
{"id": "mise-lorafi", "type": "action", "title": "Contributing guide and GitHub issue templates", "brief": {"why": "Team (mostly Claude users) needs clear path to contribute, report bugs, and request features. Contributing agents need posture guidance: file issues not PRs, don't modify architecture, layer rules", "what": "1. CONTRIBUTING.md aimed at Claude agents (works for humans too) covering: posture, test commands, layer rules, filing good bug reports 2. GitHub issue templates (bug, feature, question) 3. Branch protection on main", "done": "A teammate's Claude can clone, orient, run tests, and file an actionable bug report following conventions"}, "status": "done", "parent": "mise-naviho", "order": 5, "created_at": "2026-02-07T18:50:32Z", "created_by": "spm1001", "waiting_for": null, "tactical": {"steps": ["CONTRIBUTING.md aimed at Claude agents (works for humans too) covering: posture, test commands, layer rules, filing good bug reports", "GitHub issue templates (bug, feature, question)", "Branch protection on main"], "current": 3}, "done_at": "2026-02-07T22:58:27Z"}
{"id": "mise-lufofi", "type": "action", "title": "PIL validation applied to all external-source image deposits", "brief": {"why": "fetch_attachment (gmail.py:648) and fetch_image_file (drive.py:563) deposit image bytes from external sources without any PIL validation. The thread fetch path (fetch_gmail) now validates, but these two paths bypass it entirely — a Word file renamed .png deposited via fetch_attachment would still poison the session.", "what": "1. Extract PIL validation logic from _deposit_attachment_content into a shared helper (e.g. validate_image_bytes in extractors/image.py) 2. Apply to fetch_attachment image path (~line 648 in tools/fetch/gmail.py) 3. Apply to fetch_image_file in tools/fetch/drive.py (~line 563) 4. Invalid bytes go to skipped metadata with reason; valid bytes deposit unchanged 5. Tests for both paths", "done": "fetch_attachment and fetch_image_file with DOCX-renamed-.png input deposit nothing and return skipped metadata with reason; tests pass"}, "status": "done", "parent": "mise-jy3", "order": 22, "created_at": "2026-02-20T08:31:48Z", "created_by": "spm1001", "waiting_for": null, "tactical": {"steps": ["Extract PIL validation logic from _deposit_attachment_content into a shared helper (e.g. validate_image_bytes in extractors/image.py)", "Apply to fetch_attachment image path (~line 648 in tools/fetch/gmail.py)", "Apply to fetch_image_file in tools/fetch/drive.py (~line", "4. Invalid bytes go to skipped metadata with reason; valid bytes deposit unchanged", "Tests for both paths"], "current": 5, "session": "/Users/modha/Repos/mise-en-space"}, "updated_at": "2026-02-20T08:46:08Z", "done_at": "2026-02-20T08:46:08Z"}
{"id": "mise-lusome", "type": "action", "title": "Activity API integration tests", "brief": {"why": "Activity adapter has no real API tests — only model unit tests. Need to verify retry wiring, pagination behavior, and error handling against real API.", "what": "Create tests/integration/test_activity.py with tests for search_comment_activities and get_file_activities. Use real credentials, mark with @pytest.mark.integration.", "done": "Integration tests exist, can run with uv run pytest -m integration"}, "status": "done", "parent": "mise-3uu", "order": 5, "created_at": "2026-02-01T10:59:19Z", "created_by": "spm1001", "waiting_for": null, "tactical": {"steps": ["Read existing integration test patterns", "Write integration tests for search_comment_activities and get_file_activities", "Run tests against real API and verify"], "current": 3}, "done_at": "2026-02-09T10:18:13Z"}
{"id": "mise-maduza", "type": "outcome", "title": "Reduce fetch_sheet test boilerplate with shared mock helper", "brief": {"why": "Every fetch_sheet test needs 7-8 @patch decorators and matching mock_* params. Adding formula_count required touching 7 tests. A shared fixture would reduce this fragility.", "what": "1. Create a helper/fixture that builds a properly-configured mock SpreadsheetData with sensible defaults 2. Refactor TestFetchSheet tests to use it 3. Verify no regressions", "done": "Adding a new field to SpreadsheetData doesn't require updating N test methods"}, "status": "done", "order": 42, "created_at": "2026-02-17T21:40:18Z", "created_by": "spm1001", "done_at": "2026-02-18T13:52:21Z"}
{"id": "mise-mafoce", "type": "action", "title": "Mock helper for Google API tests", "brief": {"why": "MagicMock chaining (mock_service.files().get().execute.return_value) is brittle — if adapter code changes call pattern, mock silently returns MagicMock instead of fixture data. Multiple test files duplicate this pattern.", "what": "Create tests/helpers.py with mock_google_service() that takes method chain + response, validates the chain at test time. Refactor existing adapter tests to use it.", "done": "All adapter test files use shared helper. A refactored adapter call pattern causes a test failure, not a silent pass."}, "status": "done", "parent": "mise-3uu", "order": 13, "created_at": "2026-02-09T07:20:16Z", "created_by": "spm1001", "waiting_for": null, "tactical": {"steps": ["Survey existing Google API mock patterns across test files", "Create tests/helpers.py with mock_google_service helper", "Refactor adapter tests to use the helper"], "current": 3}, "done_at": "2026-02-09T08:44:10Z"}
{"id": "mise-mavoze", "type": "outcome", "title": "Mise skill works on Pi and CLI-only environments", "brief": {"why": "Mise skill assumes MCP tools (mcp__mise__search/fetch/create) exist. On Pi, CLI-only setups, or sessions without MCP configured, Claude gets guidance it can't act on. But the knowledge — search syntax, Gmail operators, exploration patterns, comment-checking discipline — is valuable everywhere.", "what": "1. Split skill into MCP-dependent guidance (tool invocations, base_path) and universal knowledge (search syntax, Gmail operators, comment checking, exploration patterns) 2. Add fallback patterns: curl + Google APIs with token from token.json, or direct Python scripts 3. Create a lightweight 'headless mise' mode that the skill detects (no MCP tools available) 4. Test on Pi with CLI-only Claude session", "done": "Claude on Pi can search Drive/Gmail and fetch content using skill guidance, without MCP tools installed"}, "status": "open", "parent": null, "order": 35, "created_at": "2026-02-09T15:48:43Z", "created_by": "spm1001", "updated_at": "2026-02-15T19:26:30Z"}
{"id": "mise-mawoju", "type": "action", "title": "Docs can be overwritten in-place from markdown without human copy-paste", "brief": {"why": "During GCloud audit, updating an existing Google Doc required pulling as markdown, editing locally, opening in Sublime, and having the user manually paste it back. The Docs API supports full content replacement via batchUpdate (delete all + insertText), and mise's token already has auth/documents scope. This is a different shape from mise-lajeke (surgical edits) — this is wholesale markdown-to-doc replacement, preserving the file ID and sharing.", "what": "1. Add an overwrite/replace tool to mise MCP that takes markdown content and a doc ID 2. Implementation: use Docs API batchUpdate to delete range(1, end) then insertText the new content at index 1 3. Optionally re-apply heading styles by parsing markdown headers (## → Heading 2 etc.) 4. Ensure shared drive docs work (supportsAllDrives) 5. Test round-trip: create → fetch → edit markdown → overwrite → fetch again", "done": "Can replace the full content of an existing Google Doc via mise MCP tool, from markdown, without human intervention. File ID, sharing, and location are preserved. Heading styles are applied. Works on shared drives."}, "status": "done", "order": 6, "created_at": "2026-02-12T08:55:47Z", "created_by": "spm1001", "parent": "mise-hijute", "waiting_for": null, "updated_at": "2026-02-18T21:34:10Z", "tactical": {"steps": ["Add an overwrite/replace tool to mise MCP that takes markdown content and a doc ID", "Implementation: use Docs API batchUpdate to delete range(1, end) then insertText the new content at index 1", "Optionally re-apply heading styles by parsing markdown headers (## → Heading 2 etc.)", "Ensure shared drive docs work (supportsAllDrives)", "Test round-trip: create → fetch → edit markdown → overwrite → fetch again"], "current": 5, "session": "/home/modha/Repos/mise-en-space"}, "done_at": "2026-02-18T21:34:10Z"}
{"id": "mise-mecebu", "type": "action", "title": "Activity fixtures survive re-capture without manual PII cleanup", "brief": {"why": "Activity fixtures contain file titles and team drive names that the sanitizer doesn't catch. Re-running capture_fixtures.py --sanitize leaves real data in activity fixtures — manual find-replace was needed for the initial capture.", "what": "Add activity-specific sanitization patterns to sanitize_fixtures.py: file title replacement, team drive name replacement, people ID normalization. Verify by re-running capture + sanitize and checking output.", "done": "uv run python scripts/capture_fixtures.py --sanitize produces activity fixtures with no real file titles, team drive names, or unsanitized people IDs"}, "status": "done", "parent": "mise-3uu", "order": 20, "created_at": "2026-02-09T10:41:17Z", "created_by": "spm1001", "waiting_for": null, "tactical": {"steps": ["Add file/folder title sanitization to sanitize_fixtures.py", "Re-capture and sanitize activity fixtures", "Verify no real data remains"], "current": 3}, "done_at": "2026-02-09T11:56:08Z"}
{"id": "mise-menifo", "type": "action", "title": "Shared constant for extraction_failed cue marker", "brief": {"why": "tools/fetch/web.py detects extraction failure by string-matching '*Content extraction failed for' against extractor output. If extractors/web.py changes the stub wording, the cue silently breaks. Cross-layer test catches it now but a shared constant would be better.", "what": "1. Add EXTRACTION_FAILED_MARKER constant to models.py or extractors/web.py 2. Import in both extractor and tool layer 3. Verify cross-layer test still passes", "done": "Both layers reference the same constant. Changing the marker string requires changing it in one place only."}, "status": "done", "parent": "mise-reriri", "order": 4, "created_at": "2026-02-13T10:59:58Z", "created_by": "spm1001", "waiting_for": null, "updated_at": "2026-02-15T20:07:33Z", "tactical": {"steps": ["Add EXTRACTION_FAILED_MARKER constant to models.py or extractors/web.py", "Import in both extractor and tool layer", "Verify cross-layer test still passes"], "current": 3, "session": "/home/modha/Repos/mise-en-space"}, "done_at": "2026-02-15T20:07:33Z"}
{"id": "mise-mii", "title": "Handle orphaned _mise_temp_* files", "type": "action", "status": "done", "done_at": "2026-01-25T15:46:57.651295Z", "brief": {"why": "If Drive delete fails during PDF/Office conversion, orphan temp files accumulate. Options: (1) Catch delete errors and log for manual cleanup, (2) Add cleanup job that finds _mise_temp_* files older than 1 hour, (3) Use Drive's trash instead of permanent delete.", "what": "See title", "done": "When complete"}, "created_at": "2026-01-24T19:56:14.172851Z", "created_by": "spm1001", "order": 1, "parent": "mise-52d", "waiting_for": null}
{"id": "mise-milako", "type": "action", "title": "Sharper blind test: preview as triage replacement", "brief": {"why": "Wijupo blind test used a comprehensive-research prompt which naturally requires reading full files. That tested preview accuracy but not whether preview alone enables triage decisions. This action validates search enrichment by testing: can Claude pick which results to fetch from preview alone, without reading deposited files? Lives under search enrichment (mise-kecigu) because it's the acceptance test for that outcome's value proposition.", "what": "1. Design prompt that asks 'which of these should I fetch' not 'fetch everything and summarise' 2. Run blind test with fresh Claude 3. Observe: does preview alone drive fetch decisions without reading deposited file?", "done": "Blind test with triage prompt confirms preview sufficient for pick-and-fetch workflow"}, "status": "open", "parent": "mise-kecigu", "order": 5, "created_at": "2026-02-09T23:19:57Z", "created_by": "spm1001", "waiting_for": null, "updated_at": "2026-02-15T19:39:16Z"}
{"id": "mise-miloru", "type": "outcome", "title": "Audit backlog for actions already solved by adjacent work", "brief": {"why": "gajori was fully solved by passe's evolution but stayed open. Other actions may be similarly superseded — adjacent work, external tool updates, or scope absorbed by completed items.", "what": "1. Read all open actions across outcomes 2. For each, check if the --done criteria is already met by existing code 3. Close any that are done, note any that are partially done", "done": "All open actions reflect real remaining work, not stale intentions"}, "status": "done", "order": 38, "created_at": "2026-02-15T20:37:51Z", "created_by": "spm1001", "done_at": "2026-02-15T22:04:54Z"}
{"id": "mise-mkb", "title": "Wire help tool — self-documentation", "type": "action", "status": "done", "done_at": "2026-01-23T07:43:25.311309Z", "brief": {"why": "Self-documentation tool. Returns usage info for the MCP.", "what": "## Signature\n\nhelp(topic: str = None) -> str\n\n## Topics\n\n- None → overview of all verbs\n- \"search\" → search usage and examples\n- \"fetch\" → fetch usage, format routing\n- \"create\" → create usage\n- \"formats\" → what formats are supported\n\n## Acceptance Criteria\n\n- [ ] Default help is concise overview\n- [ ] Topic-specific help exists for each verb\n- [ ] Examples included", "done": "When complete"}, "created_at": "2026-01-23T07:25:30.899606Z", "created_by": "spm1001", "order": 1, "parent": "mise-52d", "waiting_for": null}
{"id": "mise-muceki", "type": "action", "title": "Add mise://docs/cross-source resource", "brief": {"why": "Document the bounce-between-sources pattern and filename: operator", "what": "New MCP resource with cross-source workflow patterns", "done": "Resource accessible and accurate"}, "status": "done", "parent": "mise-cunufu", "order": 1, "created_at": "2026-01-29T21:12:20Z", "created_by": "spm1001", "waiting_for": null, "done_at": "2026-01-29T21:13:13Z"}
{"id": "mise-muhifu", "type": "action", "title": "CSV parser with tick-prefix for text columns", "brief": {"why": "Drive CSV import auto-detects types brilliantly (94% accuracy, benchmark Feb 2026). The only intervention needed is the tick prefix for columns the manifest marks as text — prevents leading-zero stripping on IDs, phone numbers etc. No type inference needed; Google handles it.", "what": "1. If manifest has columns with type=text, prepend tick to numeric-looking values in those columns 2. Pass through all other values unchanged — Google's import handles numbers, currency, dates, percentages, formulas 3. Unit test: tick-prefixed values survive round-trip", "done": "Closed — caller responsibility, not mise code. Tick-prefix convention documented in mise-wucasa (skill) instead."}, "status": "done", "parent": "mise-beriji", "order": 3, "created_at": "2026-02-16T13:28:53Z", "created_by": "spm1001", "waiting_for": null, "updated_at": "2026-02-16T17:49:44Z", "done_at": "2026-02-16T17:49:40Z"}
{"id": "mise-muwetu", "type": "action", "title": "Parallel search integration test", "brief": {"why": "Parallel search (mise-cohato) only has unit test coverage. Unit tests mock adapters so they can't catch threading issues — token refresh races, httplib2 thread safety, or futures not actually running concurrently.", "what": "1. Write integration test that times search with both sources 2. Assert total time < 2x slower single source (proves concurrency) 3. Run 5 times to check for intermittent thread issues", "done": "Integration test proves parallel search completes in under 1.5s and doesn't race on auth"}, "status": "done", "parent": "mise-3uu", "order": 12, "created_at": "2026-02-09T06:48:26Z", "created_by": "spm1001", "waiting_for": null, "tactical": {"steps": ["Write integration test that times search with both sources", "Assert total time < 2x slower single source (proves concurrency)", "Run 5 times to check for intermittent thread issues"], "current": 3}, "done_at": "2026-02-09T10:23:20Z"}
{"id": "mise-naviho", "type": "outcome", "title": "Ship to team", "brief": {"why": "Team has mixed technical ability. Need solid ergonomics, clear docs, and good contribution patterns before they and their Claudes start using it.", "what": "1. Fix deposit cwd bug 2. Search deposits to file 3. Rewrite README 4. Add symlink setup to README 5. Contributing guide + issue templates 6. AGENTS.md for visiting Claudes", "done": "Teammate can: install, search, fetch, read deposits in their cwd. Their Claude can: file bugs via GitHub issues following template."}, "status": "done", "order": 1, "created_at": "2026-02-07T18:50:23Z", "created_by": "spm1001", "done_at": "2026-02-09T06:30:15Z", "waiting_for": null}
{"id": "mise-navuna", "type": "action", "title": "Integration test with real 2-page PDF rendering", "brief": {"why": "All thumbnail tests are unit-level with mocks. Rendering code (poppler → PNG) is untested end-to-end. If poppler changes PNG output or pdf2image changes API, we find out in production.", "what": "1. Create/find a small 2-page PDF fixture in fixtures/pdf/ 2. Write integration test: render_pdf_pages(file_path=fixture) → verify 2 PageImage results with valid PNG bytes 3. Mark with @pytest.mark.integration (skipped in normal CI, needs poppler)", "done": "test_pdf_render_integration passes on Linux with poppler-utils installed. Fixture committed. PNG bytes start with valid PNG header."}, "status": "done", "parent": "mise-jy3", "order": 19, "created_at": "2026-02-18T21:15:08Z", "created_by": "spm1001", "waiting_for": null, "tactical": {"steps": ["Create/find a small 2-page PDF fixture in fixtures/pdf/", "Write integration test: render_pdf_pages(file_path=fixture) → verify 2 PageImage results with valid PNG bytes", "Mark with @pytest.mark.integration (skipped in normal CI, needs poppler)"], "current": 3, "session": "/home/modha/Repos/mise-en-space"}, "updated_at": "2026-02-18T21:20:17Z", "done_at": "2026-02-18T21:20:17Z"}
{"id": "mise-nfh", "title": "Port gmail extractor", "type": "action", "status": "done", "done_at": "2026-01-23T15:22:31.74465Z", "brief": {"why": "Port gmail.py extraction (1126 lines). Signature stripping, HTML→markdown, thread assembly.", "what": "## What to Port\n\nFrom v1 tools/gmail.py:\n- Signature stripping (talon library, 0.01ms per message)\n- HTML → markdown (markitdown, NOT Google Docs API)\n- Thread assembly from messages\n- Quoted reply removal\n\n## Key Learning (from V2.md)\n\nHTML→markdown was the bottleneck (10s for 64KB email).\nNow using markitdown: ~100ms. Keep this fix.\n\n## Signature\n\nextract_thread(thread_response: dict, messages: list[dict]) -> str\nextract_message(message_response: dict) -> str\n\n## Acceptance Criteria\n\n- [ ] extractors/gmail.py exists\n- [ ] Signature stripping works\n- [ ] HTML converted via markitdown (fast path)\n- [ ] Unit test with fixture passes", "done": "When complete"}, "created_at": "2026-01-23T07:24:28.743738Z", "created_by": "spm1001", "order": 1, "parent": "mise-52d", "waiting_for": null}
{"id": "mise-nihiwa", "type": "action", "title": "Integration test: Sheet creation with edge values", "brief": {"why": "Skill-forge subagent test verified what a Claude *would* do but never hit the API. CSV quoting, tick prefix, and formulae are documented but untested end-to-end.", "what": "1. Create a sheet with UK currency containing commas (£65,000) 2. Create a sheet with leading-zero IDs using tick prefix 3. Create a sheet with formulae (=SUM) 4. Verify all three render correctly in Google Sheets", "done": "Integration test passes: currency displays as currency, IDs keep leading zeros, formulae compute"}, "status": "done", "parent": "mise-beriji", "order": 13, "created_at": "2026-02-17T22:12:51Z", "created_by": "spm1001", "waiting_for": null, "done_at": "2026-02-18T14:16:48Z"}
{"id": "mise-niraci", "type": "outcome", "title": "Codebase is tidy after intense dev sessions", "brief": {"why": "Several fast-shipping sessions accumulated dead code, CLAUDE.md bloat, stale docs, layer debt — cognitive tax on every future session", "what": "1. Slim CLAUDE.md from 638 to ~250 lines, move decisions to docs/ 2. Dead code sweep (unused functions, stale forward declarations) 3. Fix stale MCP resource docs (docs_do) 4. Clean duplicated logic (source resolution, folder cues, warning aggregation)", "done": "CLAUDE.md under 300 lines, no dead code in production paths, MCP resource docs match shipped operations, duplicated patterns extracted to helpers"}, "status": "done", "order": 45, "created_at": "2026-02-18T22:05:32Z", "created_by": "spm1001", "done_at": "2026-02-18T22:19:18Z"}
{"id": "mise-nofifu", "type": "action", "title": "Test for HTML body with PDF Content-Type", "brief": {"why": "A misconfigured CDN could return content_type: application/pdf with an HTML body. Current code would pass HTML bytes to markitdown, get <500 chars, fall back to Drive conversion, and produce a confusing extraction error that doesn't mention the real problem.", "what": "1. Add test: mock WebData with content_type=application/pdf but raw_bytes containing HTML 2. Verify error message mentions content mismatch 3. Consider adding PDF magic bytes check (%PDF-) in _fetch_web_pdf before calling extract_pdf_content", "done": "Unit test covers misleading Content-Type. Error message is actionable."}, "status": "done", "parent": "mise-3uu", "order": 8, "created_at": "2026-02-07T16:23:13Z", "created_by": "spm1001", "waiting_for": null, "tactical": {"steps": ["Read _fetch_web_pdf in tools/fetch.py to understand current PDF routing", "Add test: HTML bytes with application/pdf Content-Type", "Add PDF magic bytes check (%PDF-) before extraction", "Run tests"], "current": 4}, "done_at": "2026-02-09T07:01:21Z"}
{"id": "mise-noveme", "type": "action", "title": "Mise skill reflects new do() operations", "brief": {"why": "Four new do() operations (overwrite, prepend, append, replace_text) shipped but the mise skill doesn't mention them. Claudes loading the skill won't discover these capabilities. The skill is the gate — if it doesn't teach the operations, they won't be used.", "what": "1. Add do() operations section to mise skill covering overwrite, prepend, append, replace_text 2. Include guidance on when to use overwrite vs surgical edits (overwrite destroys images/tables) 3. Show example invocations", "done": "A Claude loading the mise skill knows about all 6 do() operations and can choose the right one for the task"}, "status": "done", "parent": "mise-hijute", "order": 10, "created_at": "2026-02-18T21:51:23Z", "created_by": "spm1001", "waiting_for": null, "tactical": {"steps": ["Add do() operations section to mise skill covering overwrite, prepend, append, replace_text", "Include guidance on when to use overwrite vs surgical edits (overwrite destroys images/tables)", "Show example invocations"], "current": 3, "session": "/home/modha/Repos/mise-en-space"}, "updated_at": "2026-02-18T22:34:37Z", "done_at": "2026-02-18T22:34:37Z"}
{"id": "mise-nubice", "type": "outcome", "title": "xlsx fetch preserves formulae or warns about formula loss", "brief": {"why": "WARC workbook had 4004 formula cells encoding classification logic. mise fetch converted to CSV, losing all of them. User had to flag that formulae existed. Had to bypass mise entirely (Drive API + openpyxl) to read the actual logic.", "what": "Either: (1) deposit the raw .xlsx alongside content.csv, or (2) add formula_count to cues so the caller knows the CSV is lossy, or (3) both", "done": "Caller can read formulae from deposited xlsx, or at minimum knows to fetch the raw file separately"}, "status": "done", "order": 40, "created_at": "2026-02-16T16:33:27Z", "created_by": "spm1001", "done_at": "2026-02-16T16:39:08Z"}
{"id": "mise-ogf", "title": "PDF visual extraction strategy", "type": "action", "status": "done", "brief": {"why": "Decide when PDFs need page images vs text extraction. Unlike Slides, PDFs don't have semantic markers (charts, shapes). Options: always text, always images, hybrid based on text density, user parameter.", "what": "1. Benchmark rendering approaches (CG, pdf2image, PyMuPDF, Chrome CDP, Drive) 2. Choose always-render strategy with 100-page cap 3. Implement platform-adaptive rendering (CG macOS → pdf2image Linux) 4. Wire into all fetch paths (Drive, web, Gmail single-attachment) 5. Document decision in CLAUDE.md", "done": "Decision documented in CLAUDE.md. Implementation ships: render_pdf_pages() in adapters/pdf.py, _deposit_pdf_thumbnails() shared helper, all three fetch paths covered. 27→31 unit tests. Eager Gmail extraction intentionally skipped (filename collision in shared folder)."}, "created_at": "2026-01-24T22:24:12.022384Z", "created_by": "spm1001", "order": 1, "parent": "mise-jy3", "waiting_for": null, "updated_at": "2026-02-18T21:11:14Z", "done_at": "2026-02-18T21:11:19Z"}
{"id": "mise-p01", "title": "Port sheets.py — prove the pattern", "type": "action", "status": "done", "brief": {"why": "Smallest extractor (138 lines, 1 function). Port to pure function to validate architecture.", "what": "## Why sheets.py First\n\n- 138 lines, single function `read_sheets_as_csv()`\n- Clear input → output\n- If this works, docs.py/slides.py/gmail.py follow same pattern\n\n## Porting Steps\n\n1. Copy extraction logic from v1 tools/sheets.py\n2. Remove `get_sheets_service()` call\n3. Change signature: `extract_sheets_csv(sheets_response: dict) -> str`\n4. Write unit test with fixture\n5. Write integration test with real sheet\n\n## Acceptance Criteria\n\n- [ ] extractors/sheets.py exists\n- [ ] Pure function signature\n- [ ] Unit test passes\n- [ ] Integration test passes", "done": "When complete"}, "created_at": "2026-01-20T22:10:34.217867Z", "created_by": "spm1001", "order": 1, "waiting_for": null, "done_at": "2026-01-29T20:45:23Z"}
{"id": "mise-pajulu", "type": "action", "title": "Stabilise deposit convention before skill documents it", "brief": {"why": "mise-ponico (per-tab CSV split) changes the deposit structure. mise-wucasa (skill teaching Sheet workflow) documents the deposit structure for callers. If wucasa ships first, the skill describes a format that ponico will change. If ponico ships first, wucasa can document the final format.", "what": "1. Ensure mise-ponico lands before mise-wucasa 2. When writing wucasa skill section, reference per-tab convention not single-CSV 3. Update _SOURCE_FILENAME in tools/create.py if per-tab naming changes", "done": "Skill documents the stable deposit convention. No mismatch between what skill says and what code does."}, "status": "done", "parent": "mise-beriji", "order": 12, "created_at": "2026-02-17T20:55:07Z", "created_by": "spm1001", "waiting_for": null, "done_at": "2026-02-17T21:53:56Z"}
{"id": "mise-pejigo", "type": "outcome", "title": "Auth reflow doesn't fight Claude Code or Chrome Debug", "brief": {"why": "Current auth requires two sequential OAuth flows (gcloud then mise) that both try to launch new Chrome instances. From Claude Code: gcloud opens a separate Chrome that bounces forever (profile lock conflict with Chrome Debug), URLs with query strings get mangled by CDP's /json/new endpoint, and the whole thing takes ~5 minutes of manual wrangling.", "what": "Options to evaluate: 1. Single 'mise-reauth' command that chains gcloud+mise auth with --no-launch-browser (prints URL for user to open in existing Chrome Debug) 2. Move gcloud secret fetch into itv_google_auth so there's only one OAuth flow 3. Add pre-flight token check to mise skill that detects expiry before the first fetch fails. Open question: some of this logic belongs in itv_google_auth (the shared package) not mise specifically — but that package has no repo, it's vendored in the venv.", "done": "From a cold-token state, a single command or automatic prompt gets both gcloud and mise re-authed without launching rogue Chrome instances"}, "status": "open", "order": 32, "created_at": "2026-02-13T13:13:59Z", "created_by": "spm1001"}
{"id": "mise-peliwo", "type": "outcome", "title": "Decide v1 skill strategy", "brief": {"why": "mcp-google-workspace/skill-google-workspace still exists but references v1 tools. Either archive it or create a mise-specific skill.", "what": "After skill probe (mise-Zoluca) completes, decide: archive v1 skill / create mise skill / no skill needed", "done": "v1 skill archived or replaced"}, "status": "done", "order": 22, "created_at": "2026-01-31T20:49:52Z", "created_by": "spm1001", "done_at": "2026-02-01T10:58:34Z"}
{"id": "mise-peneku", "type": "action", "title": "403→passe fallback validates extracted content isn't a login form", "brief": {"why": "When 403 triggers passe fallback, passe may land on a login page and extract the form as 'content'. Claude downstream sees plausible-looking HTML-derived markdown and treats it as the real page.", "what": "1. After passe fallback returns, check content length and structure 2. Detect login form patterns (input type=password, 'Sign in', short page) 3. If login form detected, raise AUTH_REQUIRED instead of depositing", "done": "403→passe on a login page raises AUTH_REQUIRED with 'login form detected' message, not a deposit of form HTML"}, "status": "open", "parent": "mise-reriri", "order": 6, "created_at": "2026-02-15T20:37:44Z", "created_by": "spm1001", "waiting_for": null}
{"id": "mise-pidihu", "type": "action", "title": "Fix pre-existing test_paywall_raises failure in test_web.py", "brief": {"why": "test_paywall_raises has been failing and getting deselected. Broken tests erode confidence in the suite.", "what": "1. Read the test and understand what it expects 2. Check if the extractor/adapter behaviour changed 3. Fix the test or the code", "done": "uv run pytest tests/unit/test_web.py::TestFetchWebContentHTML::test_paywall_raises passes"}, "status": "done", "parent": "mise-jy3", "order": 15, "created_at": "2026-02-17T21:40:10Z", "created_by": "spm1001", "waiting_for": null, "done_at": "2026-02-18T14:36:45Z"}
{"id": "mise-podigo", "type": "action", "title": "Stabilise mimetype assertion in test_sheet_uses_csv_mimetype", "brief": {"why": "Oracle flagged brittle mock inspection pattern — call_args[1][media_body] if/else fallback. Could break with MagicMock internals.", "what": "Replace with clean _, kwargs = mock_service.files().create.call_args; assert kwargs[media_body].mimetype() == text/csv", "done": "Test uses direct kwargs unpacking, no if/else fallback"}, "status": "done", "parent": "mise-beriji", "order": 7, "created_at": "2026-02-16T17:58:58Z", "created_by": "spm1001", "waiting_for": null, "done_at": "2026-02-18T14:03:38Z"}
{"id": "mise-ponico", "type": "action", "title": "Split multi-tab spreadsheet deposits into per-tab CSV files", "brief": {"why": "A Claude missed that a single CSV contained multiple tabs separated by === Sheet: Name === headers. Per-tab files are unambiguous — each file is one tab, filename tells you which. This also enables selective reading (fetch tab 2 without reading tab 1).", "what": "1. Change xlsx and Google Sheets deposit to write one CSV per tab: content_sheet1.csv, content_sheet2.csv (or slugified tab names) 2. Keep a combined content.csv for backwards compat (or remove if we're confident per-tab is better) 3. Update manifest.json with tabs array listing filenames and sheet names 4. Update cues to show tab_count and tab names 5. Update extractors/sheets.py extract_sheets_content if needed 6. Unit tests with multi-tab fixture", "done": "Multi-tab spreadsheet deposits produce per-tab CSV files. Manifest lists tabs. Cues show tab_count. Single-tab spreadsheets still produce content.csv as before."}, "status": "done", "parent": "mise-geguwo", "order": 4, "created_at": "2026-02-17T20:46:00Z", "created_by": "spm1001", "waiting_for": null, "tactical": {"steps": ["Add extract_sheets_per_tab() to extractors/sheets.py — returns list of (tab_name, csv_content) tuples", "Update fetch_sheet() in tools/fetch/drive.py to write per-tab CSVs (content_tabname.csv) alongside combined content.csv", "Update XLSX deposit path in tools/fetch/drive.py to write per-tab CSVs (reuses same extractor)", "Enrich manifest.json with tabs array [{name, filename}] and update cues with tab_count + tab_names", "Update _SOURCE_FILENAME contract in tools/create.py to document per-tab convention", "Unit tests: multi-tab fixture produces per-tab files, single-tab still produces content.csv only, manifest tabs array, cues show tab info"], "current": 6, "session": "/home/modha/Repos/mise-en-space"}, "updated_at": "2026-02-17T21:10:45Z", "done_at": "2026-02-17T21:10:45Z"}
{"id": "mise-pucobo", "type": "action", "title": "Streaming raw xlsx deposit without double memory", "brief": {"why": "fetch_and_extract_office reads raw bytes via tmp_path.read_bytes() before unlink on the streaming path. A 200MB xlsx briefly doubles memory. The streaming path exists because files are large — we're partially defeating it.", "what": "1. On the streaming path, copy temp file to deposit folder directly (shutil.copy2) instead of read_bytes 2. On the small-file path, keep current write_bytes from memory 3. Update test to verify large-file path uses copy not read", "done": "Large xlsx files go to deposit via file copy, not memory. Peak memory stays at ~1x file size on the streaming path"}, "status": "done", "parent": "mise-beriji", "order": 14, "created_at": "2026-02-17T22:12:59Z", "created_by": "spm1001", "waiting_for": null, "tactical": {"steps": ["On the streaming path, copy temp file to deposit folder directly (shutil.copy2) instead of read_bytes", "On the small-file path, keep current write_bytes from memory", "Update test to verify large-file path uses copy not read"], "current": 3, "session": "/home/modha/Repos/mise-en-space"}, "updated_at": "2026-02-18T14:02:59Z", "done_at": "2026-02-18T14:02:59Z"}
{"id": "mise-pufuti", "type": "action", "title": "Test paywall-detected + passe-available fallback branch", "brief": {"why": "test_paywall_raises only covers the no-passe path. The passe-available branch (paywall detected → browser fallback) has no dedicated test — it was silently executing during the env-dependent failure.", "what": "1. Add test_paywall_falls_back_to_passe that mocks _is_passe_available=True and _fetch_with_passe, asserts WebData returned with render_method=passe and warning containing 'Paywall'", "done": "New test passes, covers the paywall+passe branch explicitly"}, "status": "done", "parent": "mise-jy3", "order": 16, "created_at": "2026-02-18T14:36:54Z", "created_by": "spm1001", "waiting_for": null, "tactical": {"steps": ["Add test_paywall_falls_back_to_passe that mocks _is_passe_available=True and _fetch_with_passe, asserts WebData returned with render_method=passe and warning containing 'Paywall'"], "current": 1, "session": "/home/modha/Repos/mise-en-space"}, "updated_at": "2026-02-18T20:09:37Z", "done_at": "2026-02-18T20:09:37Z"}
{"id": "mise-pujege", "type": "action", "title": "Update mise skill for Gmail deposit format", "brief": {"why": "Gmail deposits now use file pointers instead of inline text (a0a7a45). The mise skill's Gmail section doesn't mention that extracted attachments are separate files that need explicit Read. A calling Claude may not discover the .pdf.md files.", "what": "Update mise SKILL.md Gmail section to mention: 1. Extracted attachments are separate files (filename.md) 2. content.md has pointers, not full text 3. Read the .pdf.md file when you need attachment content", "done": "Mise skill documents the Gmail deposit structure including separate attachment files"}, "status": "done", "parent": "mise-4mj", "order": 10, "created_at": "2026-02-08T21:22:32Z", "created_by": "spm1001", "waiting_for": null, "tactical": {"steps": ["Extracted attachments are separate files (filename.md)", "content.md has pointers, not full text", "Read the .pdf.md file when you need attachment content"], "current": 3}, "done_at": "2026-02-09T06:11:25Z"}
{"id": "mise-puzuve", "type": "action", "title": "Deposit raw xlsx alongside CSV", "brief": {"why": "CSV is lossy — formulae, formatting, and cell types are gone. For roundtrip workflows (fetch xlsx → transform → republish), callers need the original file. The raw xlsx also lets callers upload to Drive without conversion loss.", "what": "1. In the xlsx fetch path (adapters/office.py), keep the downloaded xlsx bytes 2. Deposit as {filename}.xlsx alongside content.csv in the deposit folder 3. Add raw_file field to manifest.json (filename of the raw deposit) 4. Add raw_file to cues so caller knows it's available 5. Unit test: xlsx fetch deposits both content.csv and the raw .xlsx", "done": "Fetching an xlsx deposits both content.csv (for Claude to read) and the raw .xlsx (for roundtrip). Manifest and cues surface the raw file path."}, "status": "done", "parent": "mise-geguwo", "order": 3, "created_at": "2026-02-17T20:45:48Z", "created_by": "spm1001", "waiting_for": null, "tactical": {"steps": ["In the xlsx fetch path (adapters/office.py), keep the downloaded xlsx bytes", "Deposit as {filename}.xlsx alongside content.csv in the deposit folder", "Add raw_file field to manifest.json (filename of the raw deposit)", "Add raw_file to cues so caller knows it's available", "Unit test: xlsx fetch deposits both content.csv and the raw .xlsx"], "current": 5, "session": "/home/modha/Repos/mise-en-space"}, "updated_at": "2026-02-17T21:46:09Z", "done_at": "2026-02-17T21:46:09Z"}
{"id": "mise-q7j", "title": "Evaluate v1 PDF thumbnail extraction", "type": "action", "status": "open", "brief": {"why": "v1 (mcp-google-workspace) surfaces thumbnails as well as markdown for PDFs. Evaluate whether to port this logic.", "what": "## Context\n\nUser requested evaluation of v1's PDF handling which includes thumbnails, not just text extraction.\n\n## v1 Behavior (to investigate)\n\nNeed to check mcp-google-workspace for:\n1. How does v1 extract PDF thumbnails?\n2. What API/library does it use?\n3. Are these page thumbnails or embedded images?\n4. What's the quality/performance tradeoff?\n\n## Questions to Answer\n\n1. **Value**: Do PDF thumbnails add value for Claude's analysis?\n   - Charts/graphs embedded in PDFs\n   - Scanned documents with images\n   - Presentations exported to PDF\n\n2. **Feasibility**: What does extraction require?\n   - PyMuPDF (AGPL license - problematic)\n   - pdf2image + poppler (MIT, but heavy dependency)\n   - Drive thumbnail API (if available for PDFs?)\n   \n3. **Consistency**: Should PDFs match slides behavior?\n   - Slides: content.md + slide_*.png\n   - PDFs: content.md + page_*.png?\n\n4. **Cost**: Performance impact?\n   - Thumbnail generation time\n   - Storage overhead\n   - Token cost for Claude to analyze images\n\n## Research Tasks\n\n- [ ] Read v1's PDF handling code\n- [ ] Test v1 on sample PDFs\n- [ ] Compare output: v1 vs mise-en-space\n- [ ] Benchmark performance difference\n- [ ] Assess license implications\n\n## Decision Outcome\n\nAfter research, create follow-up bead:\n- If YES: \"Port v1 PDF thumbnail extraction\" with implementation plan\n- If NO: Close with rationale documented\n\n## Acceptance Criteria\n\n- [ ] v1 PDF code reviewed\n- [ ] Sample PDFs tested in both systems\n- [ ] Comparison documented (quality, speed, deps)\n- [ ] Decision made and rationale captured\n- [ ] Follow-up bead created OR this closed with \"won't do\"", "done": "When complete"}, "created_at": "2026-01-24T20:09:59.884865Z", "created_by": "spm1001", "order": 1, "parent": "mise-52d", "waiting_for": null}
{"id": "mise-qa6", "title": "Test hybrid PDF threshold logic", "type": "action", "status": "done", "done_at": "2026-01-25T17:39:24.848484Z", "brief": {"why": "Unit test for the <500 char fallback in fetch_pdf. Test cases: (1) markitdown returns 600 chars → no fallback, (2) markitdown returns 100 chars → Drive fallback triggered. Mock both paths.", "what": "See title", "done": "When complete"}, "created_at": "2026-01-24T19:56:12.495317Z", "created_by": "spm1001", "order": 1, "parent": "mise-52d", "waiting_for": null}
{"id": "mise-r0a", "title": "Extract V2.md into decisions summary", "type": "action", "status": "open", "brief": {"why": "V2.md is 466 lines of research. Extract key decisions into a digestible table format.", "what": "## Target Format\n\n| Topic | Decision | Rationale | Source |\n|-------|----------|-----------|--------|\n| Gmail HTML→MD | markitdown | 100ms vs 10s (Google Docs API) | V2.md Gmail section |\n| PDF extraction | PyMuPDF primary | 35x faster, AGPL acceptable for personal | V2.md PDF section |\n| ... | ... | ... | ... |\n\n## Location\n\nAdd to CLAUDE.md or create docs/DECISIONS.md\n\n## Why\n\nFuture Claude needs decisions quickly, not 466 lines of exploration.", "done": "When complete"}, "created_at": "2026-01-23T07:30:15.795777Z", "created_by": "spm1001", "order": 1, "parent": "mise-52d", "waiting_for": null}
{"id": "mise-racafe", "type": "action", "title": "Create pre-exfil test fixture", "brief": {"why": "Need email with attachment that's been exfiltrated to Drive to test pre-exfil detection logic.", "what": "1. Find email with attachment 2. Verify attachment in Email Attachments folder 3. Note thread ID, filename, Drive file ID 4. Add to fixtures", "done": "Test fixture documented with email thread ID, attachment filename, Drive file ID"}, "status": "done", "parent": "mise-4mj", "order": 3, "created_at": "2026-01-29T22:14:51Z", "created_by": "spm1001", "waiting_for": null, "done_at": "2026-02-08T18:55:04Z"}
{"id": "mise-rakaga", "type": "outcome", "title": "Headless detection tested on macOS", "brief": {"why": "_is_interactive() uses DISPLAY/WAYLAND_DISPLAY which behave differently on macOS — may incorrectly fall back to manual OAuth mode", "what": "Test auth flow on macOS, check whether headless detection triggers correctly", "done": "Auth flow picks correct mode on macOS without manual override"}, "status": "open", "order": 39, "created_at": "2026-02-16T07:48:22Z", "created_by": "spm1001"}
{"id": "mise-reriri", "type": "outcome", "title": "Web fetch is robust against hostile and auth-gated sites", "brief": {"why": "Web fetching touches hostile territory — tarpits, auth walls, JS-rendered SPAs, 403s on signed URLs. These are scattered across mise-jy3 and a standalone outcome but they're one concern: making web fetch resilient.", "what": "1. Hostile site defences (siJoRo) 2. Browser fallback via passe/CDP (gajori steps 3-6) 3. Forced-browser title extraction from H1 (cejepo) 4. Shared extraction_failed constant (menifo) 5. 403/auth-gated fallback to passe (sipefe)", "done": "Web fetch handles JS-rendered pages, hostile patterns, and auth-gated URLs gracefully — clear errors or automatic fallback in each case"}, "status": "done", "order": 36, "created_at": "2026-02-15T19:26:43Z", "created_by": "spm1001", "done_at": "2026-02-15T20:31:27Z"}
{"id": "mise-retara", "type": "action", "title": "Benchmark three Sheet creation paths", "brief": {"why": "Three viable paths exist: Drive CSV upload (simplest, reuses doc creation pattern), Sheets API spreadsheets.create (full formatting control), and openpyxl+upload (XLSX intermediary). Need timing and quality data to decide — not opinion. Drive CSV import discovered via about.get(fields=importFormats) — same native conversion as markdown→Doc.", "what": "1. Create test CSV with diverse cell types: plain text, integers, decimals, negative numbers, £ currency, $ currency, € currency, percentages (3.63% style), dates (ISO 2026-01-15, UK 15/01/2026, US 01/15/2026), booleans (TRUE/FALSE), formulas (=SUM, =AVERAGE), empty cells, long text strings, text that looks like numbers (phone numbers, postcodes, leading zeros like 007), commas in quoted values, unicode/emoji 2. Path A: Drive CSV upload via files.create with text/csv mimetype — check every cell type for correct detection 3. Path A+: CSV upload + batchUpdate for bold headers, freeze row, number formats — time the formatting pass 4. Path B: spreadsheets.create with full Spreadsheet resource — values + formatting in one call 5. Path C: openpyxl build XLSX + upload_and_convert 6. Read back every created Sheet via Sheets API to verify cell types (numberValue vs stringValue vs formulaValue) and effective formats 7. Document findings with timing data and a type-detection accuracy table per path", "done": "Timing data for all paths. Type detection accuracy table: for each cell type, did each path treat it correctly? Clear recommendation. Results in docs/sheet-creation-design.md."}, "status": "done", "parent": "mise-beriji", "order": 1, "created_at": "2026-02-16T13:29:21Z", "created_by": "spm1001", "waiting_for": null, "updated_at": "2026-02-16T14:14:01Z", "tactical": {"steps": ["Create test CSV with diverse cell types: plain text, integers, decimals, negative numbers, £ currency, $ currency, € currency, percentages (3.63% style), dates (ISO 2026-01-15, UK 15/01/2026, US 01/15/2026), booleans (TRUE/FALSE), formulas (=SUM, =AVERAGE), empty cells, long text strings, text that looks like numbers (phone numbers, postcodes, leading zeros like 007), commas in quoted values, unicode/emoji", "Path A: Drive CSV upload via files.create with text/csv mimetype — check every cell type for correct detection", "Path A+: CSV upload + batchUpdate for bold headers, freeze row, number formats — time the formatting pass", "Path B: spreadsheets.create with full Spreadsheet resource — values + formatting in one call", "Path C: openpyxl build XLSX + upload_and_convert", "Read back every created Sheet via Sheets API to verify cell types (numberValue vs stringValue vs formulaValue) and effective formats", "Document findings with timing data and a type-detection accuracy table per path"], "current": 7, "session": "/home/modha/Repos/mise-en-space"}, "done_at": "2026-02-16T14:14:01Z"}
{"id": "mise-ribiTo", "type": "outcome", "title": "Surface action items from Workspace APIs", "brief": {"why": "Action items scattered across 3 sources: (1) Tasks API (Google Tasks), (2) Activity API (comments mentioning you), (3) Assigned tasks in Docs (@mention checkboxes) — the bane of existence, buried with no central view.", "what": "1. Explore all 3 sources for action item surfacing\n2. Tasks API → sync to todoist-gtd or just surface?\n3. Activity API → filter for mentions, extract action items\n4. Docs assigned tasks → can we find these? (parse doc content for checkbox+@mention)\n5. Decide: sous-chef surfaces, pipes to todoist-gtd, or both", "done": "Clear decision on each source: ignore, surface via fetch enrichment, or pipe to todoist-gtd"}, "status": "done", "order": 19, "created_at": "2026-01-31T19:35:56Z", "created_by": "spm1001", "done_at": "2026-02-01T10:58:33Z"}
{"id": "mise-rilegu", "type": "action", "title": "Check mem indexing coverage for mise-en-space", "brief": {"why": "During Feb 8 session, mem search returned zero results for mise-en-space sessions despite them existing. Either indexer hasn't run recently or search terms need adjusting. This made decision archaeology harder than it should be.", "what": "1. Run mem status to check mise-en-space coverage 2. If under-indexed, run mem scan for this project 3. Verify that recent sessions (Jan-Feb 2026) are findable", "done": "mem search finds mise-en-space sessions reliably"}, "status": "done", "parent": "mise-3uu", "order": 11, "created_at": "2026-02-08T21:22:36Z", "created_by": "spm1001", "waiting_for": null, "tactical": {"steps": ["Run mem status to check mise-en-space coverage", "If under-indexed, run mem scan for this project", "Verify that recent sessions (Jan-Feb", "are findable"], "current": 4}, "done_at": "2026-02-09T10:30:46Z"}
{"id": "mise-rohali", "type": "action", "title": "Add unit tests for charts adapter", "brief": {"why": "adapters/charts.py only has integration tests. Need mocked unit tests for get_charts_from_spreadsheet() parsing, render_charts_as_pngs(), and error handling.", "what": "Add unit tests with mocked Slides/Drive services covering: parsing, rendering, missing contentUrl, failed PNG fetch", "done": "charts adapter has unit tests, can run without real API"}, "status": "done", "parent": "mise-3uu", "order": 3, "created_at": "2026-01-29T22:14:23Z", "created_by": "spm1001", "waiting_for": null, "done_at": "2026-02-09T06:59:55Z"}
{"id": "mise-romevu", "type": "action", "title": "Fix stale MCP resource docs and deduplicate do() wiring", "brief": {"why": "docs_do() resource still advertises coming operations that shipped — and source resolution is copy-pasted twice in server.py", "what": "1. Update docs_do() resource to document all 6 live operations with correct params 2. Extract source resolution helper 3. Fix Path.cwd() fallback (contradicts design)", "done": "Resource docs match shipped operations, source resolution is a single function, no cwd fallback"}, "status": "done", "parent": "mise-niraci", "order": 3, "created_at": "2026-02-18T22:05:47Z", "created_by": "spm1001", "waiting_for": null, "tactical": {"steps": ["Update docs_do() resource to document all 6 live operations with correct params", "Extract source resolution helper", "Fix Path.cwd() fallback (contradicts design)"], "current": 3, "session": "/home/modha/Repos/mise-en-space"}, "updated_at": "2026-02-18T22:19:12Z", "done_at": "2026-02-18T22:19:12Z"}
{"id": "mise-rosite", "type": "action", "title": "Pre-exfil filename matching may break with renamed files", "brief": {"why": "exfil_by_name matches Gmail attachment filename to Drive file name. But exfil script may add (1) suffixes for deduplication. Springer fixture already has this: '2026 GBP+Price+List (1).xlsx'", "what": "1. Investigate what the exfil script does to filenames 2. If it renames, add fuzzy matching (strip dedup suffixes) or match on Content Hash instead", "done": "Pre-exfil lookup finds files even when exfil script has renamed them"}, "status": "done", "parent": "mise-4mj", "order": 8, "created_at": "2026-02-08T19:17:18Z", "created_by": "spm1001", "waiting_for": null, "tactical": {"steps": ["Investigate what the exfil script does to filenames", "If it renames, add fuzzy matching (strip dedup suffixes) or match on Content Hash instead"], "current": 2}, "done_at": "2026-02-08T19:34:25Z"}
{"id": "mise-rufile", "type": "action", "title": "Capture fixtures for untested adapters", "brief": {"why": "7 adapters have no fixture data: activity, charts, genai, image, office, pdf, web. Tests mock API responses but don't verify against real data shapes.", "what": "1. Capture real API responses for each adapter 2. Sanitize PII 3. Add to fixtures/ directory 4. Wire into conftest.py", "done": "Every adapter has at least one real fixture in fixtures/"}, "status": "done", "parent": "mise-3uu", "order": 10, "created_at": "2026-02-07T18:50:43Z", "created_by": "spm1001", "waiting_for": null, "done_at": "2026-02-09T07:20:08Z"}
{"id": "mise-rukola", "type": "action", "title": "fetch.py is readable at any function, not just at file scope", "brief": {"why": "tools/fetch.py is 1611 lines — 15 fetch_* functions each repeating a 20-line deposit/manifest/cues sequence. Titans review (Feb 2026): both Metis and Prometheus flagged it independently. It's the longest file by 3x and every new content type adds ~100 lines of boilerplate. The file is readable at function level (once you find the function) but unreadable at file level — you can't hold the routing logic and the deposit logic in your head simultaneously.", "what": "1. Extract a deposit_pipeline() helper that handles the repeated write_content → write_manifest → _build_cues → FetchResult sequence — the 15 fetch sites each repeat this with minor variations (comment count, email_context, warnings source). 2. Consider tools/fetch/ package with __init__.py re-exporting do_fetch, sub-modules for routing (detect_id_type, fetch_drive dispatcher), gmail (fetch_gmail, fetch_attachment), web (fetch_web, _fetch_web_pdf, _fetch_web_office), drive types (fetch_doc/sheet/slides/pdf/office/text/image/video), and cues (_build_cues, _build_email_context_metadata). External interface unchanged. 3. Run full test suite — tests import individual functions so imports must be updated or re-exported.", "done": "tools/fetch.py split into modules, no module over 400 lines, 1031 tests still pass, do_fetch still importable from tools"}, "status": "done", "parent": "mise-jy3", "order": 11, "created_at": "2026-02-09T21:16:24Z", "created_by": "spm1001", "waiting_for": null, "done_at": "2026-02-09T22:27:42Z"}
{"id": "mise-sazizi", "type": "action", "title": "Move validates destination is a folder before moving", "brief": {"why": "do(operation=move) trusts caller to pass a folder ID. If a file ID is passed, Drive API creates a weird parent relationship or errors with an unclear message.", "what": "1. Add MIME type check on destination_folder_id before update call 2. Return clear error if not a folder 3. Test with file ID as destination", "done": "Passing a non-folder ID returns a clear invalid_input error before the move attempt"}, "status": "done", "parent": "mise-hijute", "order": 8, "created_at": "2026-02-15T22:37:45Z", "created_by": "spm1001", "waiting_for": null, "tactical": {"steps": ["Add MIME type check on destination_folder_id before update call", "Return clear error if not a folder", "Test with file ID as destination"], "current": 3, "session": "/home/modha/Repos/mise-en-space"}, "updated_at": "2026-02-18T21:30:42Z", "done_at": "2026-02-18T21:30:42Z"}
{"id": "mise-siJoRo", "type": "action", "title": "Web fetch handles hostile sites gracefully", "brief": {"why": "Some websites actively resist extraction — infinite redirects, tarpit responses, massive payloads, misleading Content-Types. Current web.py has timeouts but no systematic defence. A hostile site can hang or OOM the fetch.", "what": "1. Catalog hostile patterns seen in the wild (redirect loops, tarpits, size bombs, soft auth walls, CAPTCHA) 2. Add/verify defences for each (timeouts, size limits, redirect caps, clear detection) 3. Return structured error messages per pattern", "done": "Each hostile pattern produces a clear, actionable error message instead of hang, OOM, or corrupted output"}, "status": "done", "order": 1, "created_at": "2026-02-01T19:08:36Z", "created_by": "spm1001", "parent": "mise-reriri", "waiting_for": null, "updated_at": "2026-02-15T20:30:59Z", "tactical": {"steps": ["Catalog hostile patterns seen in the wild (redirect loops, tarpits, size bombs, soft auth walls, CAPTCHA)", "Add/verify defences for each (timeouts, size limits, redirect caps, clear detection)", "Return structured error messages per pattern"], "current": 3, "session": "/home/modha/Repos/mise-en-space"}, "done_at": "2026-02-15T20:30:59Z"}
{"id": "mise-siLugo", "type": "action", "title": "Calendar context in search results", "brief": {"why": "Docs don't exist in isolation. Meeting context explains why a doc matters — who was in the meeting, when it happened, what other docs were linked.", "what": "1. Add source='calendar' option to search, or enrich Drive results with calendar context 2. Query calendar.events() for recent events with attachments 3. Cross-reference file IDs with search results 4. Add meeting context to result metadata", "done": "Search results include meeting context when available"}, "status": "open", "parent": "mise-kecigu", "order": 2, "created_at": "2026-02-01T10:59:41Z", "created_by": "spm1001", "waiting_for": null, "updated_at": "2026-02-15T19:26:57Z"}
{"id": "mise-sipefe", "type": "action", "title": "Graceful fallback when fetch hits 403 on signed/authenticated URLs", "brief": {"why": "During NRDP key rotation, mise fetch got 403 on a CloudFront signed URL. The URL worked fine in Chrome (which had cookies/session). No guidance exists for bouncing to passe when mise can't access a URL but Chrome can.", "what": "1. When fetch returns 403/401, include suggestion in error message: 'URL may require browser auth — try passe read instead' 2. Document the mise→passe bounce pattern in the skill 3. Consider auto-fallback option", "done": "mise fetch on a 403 URL returns error with actionable passe fallback suggestion"}, "status": "done", "order": 5, "created_at": "2026-02-15T11:26:22Z", "created_by": "spm1001", "parent": "mise-reriri", "waiting_for": null, "updated_at": "2026-02-15T20:28:01Z", "tactical": {"steps": ["When fetch returns 403/401, include suggestion in error message: 'URL may require browser auth — try passe read instead'", "Document the mise→passe bounce pattern in the skill", "Consider auto-fallback option"], "current": 3, "session": "/home/modha/Repos/mise-en-space"}, "done_at": "2026-02-15T20:28:01Z"}
{"id": "mise-sireho", "type": "action", "title": "Cap files list for slides deposits", "brief": {"why": "cues.files lists every file in the deposit folder. A 43-slide deck with thumbnails produces ~45 filenames (~200 tokens). This is noisy and the individual slide_XX.png names add no decision value.", "what": "1. In _build_cues, detect thumbnail files by pattern (slide_*.png) 2. Replace individual filenames with summary: 'slide_01.png ... slide_43.png (43 thumbnails)' or just add thumbnail_count 3. Keep non-thumbnail files listed individually", "done": "Slides fetch cues.files is compact regardless of slide count"}, "status": "done", "parent": "mise-jy3", "order": 8, "created_at": "2026-02-09T20:44:22Z", "created_by": "spm1001", "waiting_for": null, "tactical": {"steps": ["In _build_cues, detect thumbnail files by pattern (slide_*.png)", "Replace individual filenames with summary: 'slide_01.png ... slide_43.png (43 thumbnails)' or just add thumbnail_count", "Keep non-thumbnail files listed individually"], "current": 3}, "done_at": "2026-02-09T22:45:14Z"}
{"id": "mise-sitovi", "type": "action", "title": "Create tool returns useful post-action signals", "brief": {"why": "Fetch and search now surface cues/preview so Claude doesn't need to read manifest.json. Create has no equivalent — after creating a doc, Claude doesn't know sharing status, folder location, or whether the doc was actually created where expected.", "what": "1. Audit what create currently returns 2. Identify what signals would change Claude's next action (share link? open in browser? move to folder?) 3. Design cues block for create responses 4. Implement and test", "done": "Create response includes actionable signals — at minimum: web_link, folder_name, sharing_status"}, "status": "done", "parent": "mise-hijute", "order": 3, "created_at": "2026-02-09T20:53:36Z", "created_by": "spm1001", "waiting_for": null, "tactical": {"steps": ["Audit what create currently returns", "Identify what signals would change Claude's next action (share link? open in browser? move to folder?)", "Design cues block for create responses", "Implement and test"], "current": 4, "session": "/home/modha/Repos/mise-en-space"}, "updated_at": "2026-02-15T22:28:13Z", "done_at": "2026-02-15T22:28:13Z"}
{"id": "mise-sokedi", "type": "action", "title": "Fix stale sheet integration test", "brief": {"why": "tests/integration/test_create_tool.py line 107-113 expects doc_type=sheet to return not_implemented. Sheet creation shipped in c51bfdd — this test now fails on real credentials.", "what": "1. Update test to expect successful sheet creation (like the doc test) 2. Add cleanup to delete the created sheet after test 3. Verify with uv run pytest tests/integration/test_create_tool.py -v -m integration", "done": "Integration test for sheet creation passes. Old not_implemented assertion removed."}, "status": "done", "parent": "mise-beriji", "order": 11, "created_at": "2026-02-17T20:54:57Z", "created_by": "spm1001", "waiting_for": null, "done_at": "2026-02-18T14:16:48Z"}
{"id": "mise-sujuKo", "type": "action", "title": "Add raw text unit tests", "brief": {"why": "Raw text handling (_is_raw_text, _format_raw_text) works but lacks explicit unit test coverage.", "what": "Add TestRawTextHandling class to test_web.py with tests for Content-Type detection, URL extension detection, and formatting for each file type.", "done": "Unit tests cover all raw text code paths"}, "status": "done", "order": 9, "created_at": "2026-02-01T15:23:28Z", "created_by": "spm1001", "parent": "mise-3uu", "waiting_for": null, "tactical": {"steps": ["Understand raw text code paths (_is_raw_text, _format_raw_text)", "Write TestRawTextHandling in test_web.py", "Run tests, verify coverage"], "current": 3}, "done_at": "2026-02-09T06:55:56Z"}
{"id": "mise-tagemu", "type": "outcome", "title": "Apps Script email extractor pipeline", "brief": {"why": "Email attachment extraction (Gmail → Drive) is infrastructure that runs in Google, separate from mise's codebase. Currently lives in mcp-google-workspace being wound down.", "what": "1. Port apps-script from v1 (gelopa) 2. Parameterize year functions (Recebe)", "done": "apps-script/ in mise-en-space, deployable, adding a new year is config-only"}, "status": "open", "order": 26, "created_at": "2026-02-08T19:22:07Z", "created_by": "spm1001"}
{"id": "mise-tasofe", "type": "action", "title": "Wire do(operation=create, doc_type=sheet) end-to-end", "brief": {"why": "The final wiring: tool parameter accepts source path, reads deposit, runs CSV parser + openpyxl assembly + upload+convert, enriches manifest, returns result with cues.", "what": "1. Add source parameter to do() tool 2. Route doc_type=sheet through new path: read deposit → parse CSV → assemble workbook → upload+convert → enrich manifest 3. Return FetchResult-consistent response with web_link and cues 4. Handle errors: missing deposit, bad CSV, upload failure 5. Integration test against real Sheets API", "done": "do(operation=create, doc_type=sheet, source=mise-do/...) creates a real Google Sheet from a deposited CSV. Manifest updated with file_id and web_link. Cues in response. Error cases handled gracefully."}, "status": "done", "parent": "mise-beriji", "order": 5, "created_at": "2026-02-16T13:29:12Z", "created_by": "spm1001", "waiting_for": null, "updated_at": "2026-02-16T14:07:25Z", "done_at": "2026-02-16T14:19:24Z"}
{"id": "mise-tigoku", "type": "action", "title": "Capture real Gmail rfc822 forward as test fixture", "brief": {"why": "parse_forwarded_messages() is unit-tested with synthetic payloads but hasn't been verified against a real message/rfc822 MIME part from the Gmail API. The inline forward path (split_forward_sections) was verified against the VOD thread. Need a real rfc822 fixture to catch any MIME structure surprises.", "what": "1. Find a real thread with a message/rfc822 forward (search Gmail) 2. Capture via scripts/capture_fixtures.py 3. Add to fixtures/gmail/ as real_forwarded_thread.json 4. Add round-trip test verifying forwarded content appears in extraction", "done": "Real fixture with message/rfc822 part captured. Round-trip test passes. ForwardedMessage populated with correct headers and body."}, "status": "done", "parent": "mise-wocidi", "order": 8, "created_at": "2026-02-15T21:10:41Z", "created_by": "spm1001", "waiting_for": null, "tactical": {"steps": ["Find a real thread with a message/rfc822 forward (search Gmail)", "Capture via scripts/capture_fixtures.py", "Add to fixtures/gmail/ as real_forwarded_thread.json", "Add round-trip test verifying forwarded content appears in extraction"], "current": 4, "session": "/home/modha/Repos/mise-en-space"}, "updated_at": "2026-02-15T21:46:19Z", "done_at": "2026-02-15T21:46:19Z"}
{"id": "mise-tisatu", "type": "action", "title": "Guard against empty thread_id in Gmail batch callback", "brief": {"why": "Gmail ordering fix (cecuzu) uses results_by_id dict keyed by thread_id. If batch callback fires with empty string id, result is stored under '' and silently dropped from reorder. Edge case but violates 'no silent data loss' principle.", "what": "1. Add guard in handle_thread_response: skip if thread_id is empty 2. Add warning when thread_id missing 3. Test: callback with empty id produces warning, not silent drop", "done": "Empty thread_id produces warning, doesn't silently vanish"}, "status": "done", "parent": "mise-wocidi", "order": 4, "created_at": "2026-02-09T23:20:07Z", "created_by": "spm1001", "waiting_for": null, "updated_at": "2026-02-15T20:25:54Z", "tactical": {"steps": ["Add guard in handle_thread_response: skip if thread_id is empty", "Add warning when thread_id missing", "Test: callback with empty id produces warning, not silent drop"], "current": 3, "session": "/home/modha/Repos/mise-en-space"}, "done_at": "2026-02-15T20:25:54Z"}
{"id": "mise-tivoBu", "type": "action", "title": "Prototype pattern-based structure detection", "brief": {"why": "LLM distillation has cost; pattern matching might handle 80% case cheaply", "what": "Regex/heuristics for common patterns: 'Action Items:', '[ ]', 'Decision:', '@mentions'", "done": "Have extraction that finds action items and decisions without LLM call"}, "status": "open", "parent": "mise-fetifo", "order": 3, "created_at": "2026-01-30T12:49:23Z", "created_by": "spm1001", "waiting_for": null}
{"id": "mise-tld", "title": "Wire resources — self-documenting capabilities", "type": "action", "status": "done", "done_at": "2026-01-25T17:48:57.200251Z", "brief": {"why": "MCP resources that expose dynamic API info: import/export formats, quotas, patterns.", "what": "## From v1\n\nworkspace://capabilities/drive/import-formats  → What can be imported\nworkspace://capabilities/drive/export-formats  → What can be exported\nworkspace://quotas/drive/storage               → User's storage limits\nworkspace://patterns/...                       → Usage patterns\n\n## Why Useful\n\nClaude can read these to discover what's possible without external docs.\nDynamic = fetched from actual API (about.get), not static.\n\n## Simpler for v2?\n\nCould consolidate into fewer resources:\n- workspace://capabilities → all format info\n- workspace://help/{topic} → patterns + usage\n\nOr just rely on help() tool and skip resources.\n\n## Acceptance Criteria\n\n- [ ] Decide: resources vs expanded help() tool\n- [ ] If resources: wire import/export formats\n- [ ] Dynamic content from API (not static)", "done": "When complete"}, "created_at": "2026-01-23T07:26:49.012371Z", "created_by": "spm1001", "order": 1, "parent": "mise-52d", "waiting_for": null}
{"id": "mise-tuguzi", "type": "outcome", "title": "Figured out mise skill strategy", "brief": {"why": "v1 skill exists but references dead tools; need to decide whether mise needs its own skill or none at all", "what": "1. Week-long probe of MCP-only usage 2. Decide: no skill / minimal skill / port v1 3. Archive or replace v1 skill", "done": "v1 skill archived or replaced; decision documented"}, "status": "done", "order": 2, "created_at": "2026-02-01T10:58:09Z", "created_by": "spm1001", "done_at": "2026-02-09T15:23:17Z"}
{"id": "mise-udh", "title": "Stream large files to disk instead of memory", "type": "action", "status": "done", "done_at": "2026-01-25T17:16:11.1059Z", "brief": {"why": "Currently loads entire files into memory (pdf_bytes = download_file()). User reports gigabyte PPTXs at ITV. Need streaming pattern to handle large files without OOM.", "what": "## Problem\nPDF/Office adapters load entire files into RAM before processing.\nThis fails for large files (user reports gigabyte PPTXs).\n\n## Approach Options\n1. **Stream download to temp file** — Download in chunks to disk, then process\n2. **Lazy loading** — Only load chunks as needed during extraction\n3. **File size check first** — Fail fast with error for files over threshold\n\n## Affected Files\n- adapters/pdf.py: `extract_pdf_content(file_bytes: bytes, ...)`\n- adapters/office.py: `extract_office_content(file_bytes: bytes, ...)`\n- adapters/drive.py: `download_file()` returns bytes\n\n## Acceptance Criteria\n- [ ] Files over 100MB don't cause OOM\n- [ ] Streaming pattern documented\n- [ ] Tests with large file simulation", "done": "When complete"}, "created_at": "2026-01-25T16:59:40.853018Z", "created_by": "spm1001", "order": 1, "waiting_for": null}
{"id": "mise-v79", "title": "Refactor retry decorator duplication", "type": "action", "status": "done", "done_at": "2026-01-24T20:34:20.265849Z", "brief": {"why": "async_wrapper and sync_wrapper in retry.py are nearly identical. DRY violation.", "what": "## Problem\n\nIn retry.py lines 143-204:\n- `async_wrapper()` (lines 143-174)\n- `sync_wrapper()` (lines 177-204)\n\nBoth have identical:\n- Retry loop structure\n- Exponential backoff calculation\n- Jitter application\n- Exception handling logic\n\nOnly difference: `await func(...)` vs `func(...)`\n\n## Approach\n\nExtract common logic into helper:\n\n```python\ndef _calculate_delay(attempt: int, delay_ms: int, jitter_ms: int) -> float:\n    \"\"\"Calculate delay with exponential backoff and jitter.\"\"\"\n    base = delay_ms * (2 ** attempt) / 1000\n    jitter = random.uniform(0, jitter_ms / 1000)\n    return base + jitter\n\ndef _should_retry(exception: Exception, retryable_errors: tuple) -> bool:\n    \"\"\"Determine if exception is retryable.\"\"\"\n    # Check HTTP status codes, error types, etc.\n```\n\nThen both wrappers become thin:\n```python\nasync def async_wrapper(*args, **kwargs):\n    for attempt in range(max_attempts):\n        try:\n            return await func(*args, **kwargs)\n        except retryable_errors as e:\n            if not _should_retry(e, retryable_errors):\n                raise\n            delay = _calculate_delay(attempt, delay_ms, jitter_ms)\n            await asyncio.sleep(delay)\n    raise\n```\n\n## Acceptance Criteria\n\n- [ ] Common retry logic extracted\n- [ ] async_wrapper uses helper functions\n- [ ] sync_wrapper uses helper functions\n- [ ] Tests still pass\n- [ ] No behavior change (just refactor)", "done": "When complete"}, "created_at": "2026-01-24T20:10:24.232062Z", "created_by": "spm1001", "order": 1, "parent": "mise-52d", "waiting_for": null}
{"id": "mise-vetaku", "type": "action", "title": "Folder cues pattern deduplicated", "brief": {"why": "Identical folder-name-resolution logic copy-pasted between _create_doc() and _create_sheet() in tools/create.py — will triple when slides creation arrives", "what": "1. Extract shared _resolve_folder_cues() helper 2. Both create functions call it 3. Test helper independently", "done": "No duplicated folder cues code in tools/create.py, helper tested"}, "status": "open", "parent": "mise-niraci", "order": 4, "created_at": "2026-02-18T22:28:39Z", "created_by": "spm1001", "waiting_for": null}
{"id": "mise-voSovu", "type": "action", "title": "Embed extracted PDF content in content.md", "brief": {"why": "Currently PDFs are deposited separately; Claude must Read them explicitly. Should embed text like Drive PDFs.", "what": "After extracting PDF attachment, append extracted markdown to thread content.md (like we do for Drive PDFs). Keep raw PDF file for reference.", "done": "content.md includes inline PDF text for all extracted attachments"}, "status": "done", "parent": "mise-4mj", "order": 7, "created_at": "2026-02-01T10:59:03Z", "created_by": "spm1001", "waiting_for": null, "tactical": {"steps": ["Trace current deposit flow to understand where content.md is written", "Append extracted attachment content to thread content.md after extraction", "Add tests for inline embedding", "Verify with full test suite"], "current": 4}, "done_at": "2026-02-08T19:41:33Z"}
{"id": "mise-vosohi", "type": "outcome", "title": "Layer violations cleaned: extractors pure, adapters named correctly", "brief": {"why": "Audit found: extractors/gmail.py does tempfile I/O, adapters/pdf.py and adapters/office.py have extract_* functions mixing pure extraction with API calls, tools/fetch/drive.py builds video markdown inline with no extractor", "what": "1. Move _convert_html_to_markdown tempfile logic out of extractors/gmail.py 2. Split adapters/pdf.py extract_pdf_content into pure extraction + API orchestration 3. Create extractors/video.py for fetch_video markdown assembly 4. Rename adapter functions to not use extract_ prefix", "done": "No I/O in extractors/, no extract_* names in adapters/, video extractor exists"}, "status": "open", "order": 46, "created_at": "2026-02-18T22:28:50Z", "created_by": "spm1001"}
{"id": "mise-votiru", "type": "action", "title": "Cues work end-to-end in live fetch", "brief": {"why": "Cues (6af6745) shipped based on design feedback but haven't been verified with a real fetch against the running MCP. Response shape may differ from spec.", "what": "1. Start mise MCP locally 2. Run a fetch against a known Drive doc 3. Verify cues block contains files, open_comment_count, warnings, content_length, email_context 4. Run a Gmail fetch and verify participants, has_attachments, date_range appear 5. Run a search and verify preview block with top 3 per source", "done": "All three tool types (fetch doc, fetch gmail, search) return cues/preview blocks matching the shipped spec"}, "status": "done", "parent": "mise-jy3", "order": 10, "created_at": "2026-02-09T20:53:30Z", "created_by": "spm1001", "waiting_for": null, "tactical": {"steps": ["Start mise MCP locally", "Run a fetch against a known Drive doc", "Verify cues block contains files, open_comment_count, warnings, content_length, email_context", "Run a Gmail fetch and verify participants, has_attachments, date_range appear", "Run a search and verify preview block with top 3 per source"], "current": 5}, "done_at": "2026-02-09T22:49:59Z"}
{"id": "mise-vufuzu", "type": "action", "title": "Update do() recipe in CLAUDE.md after cetoha dispatch refactor", "brief": {"why": "CLAUDE.md 'How to add a do() operation' recipe says to add elif branch in server.py — cetoha replaces this with operation registry, making the recipe misleading", "what": "1. After cetoha ships, update recipe to describe OpSpec registration pattern 2. Update response shape guidance if DoResult dataclass lands", "done": "Recipe matches actual dispatch pattern"}, "status": "done", "order": 12, "created_at": "2026-02-18T22:28:50Z", "created_by": "spm1001", "parent": "mise-hijute", "waiting_for": null, "updated_at": "2026-02-18T22:29:05Z", "done_at": "2026-02-18T23:14:27Z"}
{"id": "mise-vusagu", "type": "action", "title": "Stale file cleanup on deposit folder re-fetch", "brief": {"why": "When fetch_gmail is called for a thread that has a previous deposit folder, it overwrites files for successfully-extracted attachments but leaves orphaned files from the previous fetch untouched. In the scribbles case, old DOCX-as-PNG files from a broken previous fetch remained alongside the correctly-deposited files from the new fetch, because those two were skipped (attachment limit hit) and never overwritten. Claude reading the folder sees a mix of old and new content.", "what": "1. Audit what deposit folder re-fetch currently does (does write_manifest overwrite? does get_deposit_folder reuse same path?) 2. Decide policy: (a) wipe and recreate on re-fetch, (b) wipe only image/attachment files and keep manifest+content, (c) keep everything and accept staleness 3. Implement chosen policy 4. Consider whether manifest should record a fetched_at timestamp and list of deposited files so stale detection is possible 5. Tests for re-fetch scenario", "done": "Re-fetching a thread with a previous deposit folder leaves no files from the previous fetch that are not present in the new fetch; or staleness is clearly signalled in manifest"}, "status": "open", "parent": "mise-jy3", "order": 24, "created_at": "2026-02-20T08:32:10Z", "created_by": "spm1001", "waiting_for": null}
{"id": "mise-w6f", "title": "Render Sheets charts as PNGs via Slides API", "type": "action", "status": "done", "done_at": "2026-01-25T20:42:03.58269Z", "brief": {"why": "When fetching spreadsheets with charts, render chart images by embedding in a temp Slides presentation, fetching contentUrl, and saving as PNGs. This makes chart content visible to Claude.", "what": "## Decisions (Jan 2026)\n\n1. **Default on** — Always render charts when present\n2. **Per-fetch presentation** — Create/cleanup per fetch (not persistent)\n3. **All charts** — Both chart sheets (sheetType=OBJECT) AND embedded charts on GRID sheets\n\n## Approach\n\nFrom itv-slides-formatter research (contentUrl-resolution.md, explore_charts.py):\n\n1. Get spreadsheet metadata including `sheets[].charts[]` array\n2. For each chart (floating or sheet chart):\n   - Insert into temp Slides presentation via `createSheetsChart`\n   - Get `contentUrl` from resulting pageElement\n   - Fetch PNG via HTTP GET\n   - Save to deposit folder\n3. Clean up temp presentation\n\n## Timing Expectations\n\n- ~500ms per chart (similar to slides thumbnails)\n- Spreadsheet with 5 charts ≈ 2.5s additional latency\n- Document in manifest: `chart_count`, `chart_fetch_time_ms`\n\n## Test Fixture\n\nhttps://docs.google.com/spreadsheets/d/1UlWoEsfjzqbuS_tKD6Drm4wmbPLeGOKWVBVip5AI-xw/edit\n\n## Deposit Structure\n\n```\nmise-fetch/sheet--budget-2026--abc123/\n├── manifest.json\n├── content.md          # Grid data as markdown tables  \n├── chart_1.png         # Rendered chart images\n├── chart_2.png\n└── charts.json         # Metadata: title, type, source ranges\n```\n\n## Also Needed\n\n- Filter GRID sheets for values (mise-bpm) — but now as part of this work\n- Handle sheetType=OBJECT (chart sheets) by rendering, not skipping\n\n## Acceptance Criteria\n\n- [ ] Charts extracted from both GRID sheets and chart sheets\n- [ ] PNG files saved to deposit folder\n- [ ] charts.json has metadata (title, type, chartId)\n- [ ] Timing tests: fetch with/without charts\n- [ ] Test against fixture spreadsheet", "done": "When complete"}, "created_at": "2026-01-25T20:13:34.882308Z", "created_by": "spm1001", "order": 1, "parent": "mise-6kh", "waiting_for": null}
{"id": "mise-waguwe", "type": "action", "title": "Third verb scaffolding: create becomes do(operation=...)", "brief": {"why": "Current MCP has 'create' as a standalone tool. The do-verb design (Feb 2026) replaces it with do(operation=create|move|rename|share). Need the plumbing before any operation beyond create can land.", "what": "1. Rename create tool to do in server.py 2. Add operation param (default='create' for backwards compat) 3. Route operation=create to existing do_create 4. Update MCP tool description and docstring 5. Update tests", "done": "do(operation='create') works identically to old create tool; new operations can be added by wiring"}, "status": "done", "parent": "mise-hijute", "order": 1, "created_at": "2026-02-09T23:17:15Z", "created_by": "spm1001", "waiting_for": null, "tactical": {"steps": ["Rename create tool to do in server.py", "Add operation param (default='create' for backwards compat)", "Route operation=create to existing do_create", "Update MCP tool description and docstring", "Update tests"], "current": 5, "session": "/home/modha/Repos/mise-en-space"}, "updated_at": "2026-02-15T22:12:54Z", "done_at": "2026-02-15T22:12:54Z"}
{"id": "mise-weduje", "type": "outcome", "title": "Faster mise operations", "brief": {"why": "Benchmarks (Feb 2026) show several paths where latency could be halved. Search-both at 2.3s is the most impactful since it's the default mode. Slides and Office are slower but constrained by Google APIs.", "what": "Parallel search, investigate slides batch alternatives, profile Office conversion pipeline", "done": "Search-both under 1.5s. Slides and Office paths profiled with clear picture of what's API-bound vs what we control."}, "status": "done", "order": 27, "created_at": "2026-02-09T06:36:57Z", "created_by": "spm1001", "done_at": "2026-02-09T14:15:30Z"}
{"id": "mise-wemuve", "type": "outcome", "title": "Gmail image deposits don't break the API", "brief": {"why": "Fetching a Gmail thread with large PNG attachments deposits the images alongside content.md. When Claude reads the deposit folder and passes oversized images to the Claude API, it throws 'Could not process image' (400) and crashes the conversation. Observed in gmail--scribbles--19c1f439271e with two PNGs at 3.4MB and 3.6MB.", "what": "1. Decide policy: strip images by default, filter by size, or flag for explicit opt-in 2. Implement chosen policy in gmail adapter 3. Confirm existing scribbles thread no longer triggers the error", "done": "Fetching a Gmail thread with large image attachments completes without API error; images either absent or safely handled"}, "status": "done", "order": 47, "created_at": "2026-02-19T14:50:48Z", "created_by": "spm1001", "done_at": "2026-02-20T08:31:37Z"}
{"id": "mise-wevila", "type": "action", "title": "Investigate mise-tasofe done-but-no-code discrepancy", "brief": {"why": "mise-tasofe (wire end-to-end sheet creation) marked done 2026-02-16 but tools/create.py still had doc_type=sheet returning not_implemented. Either tasofe did something else that was reverted, or it was incorrectly closed.", "what": "1. Check git log for tasofe-related commits around 2026-02-16 2. Determine what it actually did 3. Either document or re-close with accurate notes", "done": "Discrepancy explained — either tasofe's work identified, or item re-annotated"}, "status": "done", "parent": "mise-beriji", "order": 9, "created_at": "2026-02-16T17:59:12Z", "created_by": "spm1001", "waiting_for": null, "done_at": "2026-02-18T14:04:15Z"}
{"id": "mise-wiBoKe", "type": "action", "title": "Add attachments/drive_links to Gmail FetchResult.metadata", "brief": {"why": "Currently only in markdown text, need structured data for programmatic follow-up", "what": "Extract attachment list and drive_links to FetchResult.metadata dict", "done": "Gmail fetch returns metadata with attachments and drive_links arrays"}, "status": "done", "parent": "mise-cunufu", "order": 4, "created_at": "2026-01-29T21:12:35Z", "created_by": "spm1001", "waiting_for": null, "done_at": "2026-01-29T21:14:32Z"}
{"id": "mise-wijupo", "type": "action", "title": "Blind test search preview", "brief": {"why": "Search preview was designed based on test Claude's jq struggles but never validated live. Fetch cues were validated by blind test; search preview was not. Need to verify preview actually eliminates field guessing and jq triage step.", "what": "1. Give fresh Claude a search-heavy prompt (multi-source, needs filtering) 2. Observe: does it use preview to pick what to fetch? Does it skip jq? 3. Check if top 3 is enough or if it still reads deposited file", "done": "Live test confirms preview reduces or eliminates jq triage calls"}, "status": "done", "parent": "mise-jy3", "order": 9, "created_at": "2026-02-09T20:44:25Z", "created_by": "spm1001", "waiting_for": null, "done_at": "2026-02-09T23:07:48Z"}
{"id": "mise-wocidi", "type": "outcome", "title": "Field Report: extraction quality gaps across file types", "brief": {"why": "During real research session (Trivago briefing, 2026-02-13), two extraction gaps caused information loss:\n\n**Issue 1 — Table extraction flattens data-heavy PDFs:**\nArtifact: Drive file 'ITV UK - 2025 05 Nov.pdf' (attached to Gmail thread 19b26c2d09b1291d, from Mike Liall). Originally a PPTX with dense CPM/VPMC/CPV tables by channel, daypart, programme. markitdown extracted it as disconnected numbers with no row/column structure.\n\n**Issue 2 — Forwarded messages vanish from thread fetch:**\nArtifact: Gmail thread 19be6358cbc99eca ('ITV VOD measurement call follow up', 10 messages). Forwarded message from Dennie Hoogenkamp (Trivago) with detailed business requirements (pricing, incremental reach, ROAS) was invisible in the deposit. strip_signature_and_quotes() was destroying forwarded content alongside reply quotes. (Note: original brief referenced Peter Fergusson — his content was actually a reply-quote by Jason, correctly stripped. The real loss was Dennie's forwarded requirements.)", "what": "Two fixes, both in our parser (not API limitations):\n(a) PDF structural quality gate: add _looks_like_flattened_tables() heuristic to adapters/pdf.py — three-signal detection (short_ratio, sentence_ratio, numeric_ratio) triggers Drive fallback when markitdown flattens table structure. No new dependencies — Drive conversion already handles tables.\n(b) Gmail forward preservation: split_forward_sections() in talon_signature.py detects Gmail/Apple forward markers before stripping, preserves forwarded content. parse_forwarded_messages() in gmail.py handles MIME message/rfc822 parts.\nSee plan: ~/.claude/plans/cozy-gliding-mccarthy.md for full design rationale, false positive analysis, and edge cases.", "done": "Table-heavy PDF deposits preserve row/column structure. Forwarded messages appear in thread content.md."}, "status": "done", "order": 33, "created_at": "2026-02-13T14:00:15Z", "created_by": "spm1001", "updated_at": "2026-02-15T21:10:23Z", "done_at": "2026-02-15T21:50:59Z"}
{"id": "mise-wodufo", "type": "action", "title": "Detect exfil'd files and expose email context", "brief": {"why": "When file is in Email Attachments folder with Message ID, surface email context", "what": "Parse description for Message ID, detect folder membership, add email_context to results", "done": "Drive results show email linkage when available"}, "status": "done", "parent": "mise-cunufu", "order": 3, "created_at": "2026-01-29T21:12:30Z", "created_by": "spm1001", "waiting_for": null, "done_at": "2026-01-29T21:14:13Z"}
{"id": "mise-wonaru", "type": "action", "title": "Evaluate v1 PDF thumbnail extraction", "brief": {"why": "v1 surfaces thumbnails + markdown for PDFs. Need to evaluate if worth porting. PDFs with charts/graphs/scans may benefit from visual extraction.", "what": "1. Review v1 PDF code 2. Test on sample PDFs 3. Compare output quality/speed/deps 4. Assess license implications (PyMuPDF=AGPL, pdf2image=MIT) 5. Decide: port or close", "done": "v1 PDF thumbnail code reviewed. Sample PDFs tested (text-heavy, chart-heavy, scanned). Quality/speed/dependency comparison written up. Decision recorded: port (with implementation plan), decline (with rationale), or defer (with conditions for revisiting)."}, "status": "done", "parent": "mise-jy3", "order": 2, "created_at": "2026-01-29T22:14:29Z", "created_by": "spm1001", "waiting_for": null, "updated_at": "2026-02-15T19:32:03Z", "done_at": "2026-02-18T20:21:29Z"}
{"id": "mise-wucasa", "type": "action", "title": "Skill teaches callers how to create Sheets from deposits", "brief": {"why": "The tool is ~10 lines of code. The skill is where callers learn the conventions that make it work: deposit-first workflow, tick prefix for text, let Google handle type detection, formula syntax, date format preferences, USD limitation. Without the skill, callers will inline data (wasting tokens), strip currency symbols (losing formatting), and forget leading zeros (corrupting IDs). The blind test pattern tells us tool Claudes skip documentation they haven't been taught.", "what": "1. Add Sheet creation workflow to mise skill: pass CSV as content to do(operation=create, doc_type=sheet) 2. Teach tick prefix convention for text-that-looks-like-numbers (caller prepends ' in CSV) 3. Teach what Google auto-detects well (£/€, percentages, dates, booleans, formulas) and what it doesn't (USD, US dates, leading zeros) 4. Add examples: simple data dump, data with currency, data with IDs that have leading zeros 5. Add anti-patterns: don't inline large CSVs unnecessarily, don't strip currency symbols, don't format numbers yourself", "done": "Skill has a Sheet creation section that a test Claude follows correctly on first attempt. Covers deposit workflow, type detection trust, tick prefix, and common pitfalls."}, "status": "done", "parent": "mise-beriji", "order": 6, "created_at": "2026-02-16T16:23:54Z", "created_by": "spm1001", "waiting_for": null, "updated_at": "2026-02-17T21:59:58Z", "tactical": {"steps": ["Add Sheet creation workflow to mise skill: pass CSV as content to do(operation=create, doc_type=sheet)", "Teach tick prefix convention for text-that-looks-like-numbers (caller prepends ' in CSV)", "Teach what Google auto-detects well (£/€, percentages, dates, booleans, formulas) and what it doesn't (USD, US dates, leading zeros)", "Add examples: simple data dump, data with currency, data with IDs that have leading zeros", "Add anti-patterns: don't inline large CSVs unnecessarily, don't strip currency symbols, don't format numbers yourself"], "current": 5, "session": "/home/modha/Repos/mise-en-space"}, "done_at": "2026-02-17T21:59:58Z"}
{"id": "mise-wudutu", "type": "action", "title": "Scoped titans rerun on cues code path", "brief": {"why": "Feb 2026 titans review covered the whole codebase (broad-but-shallow). After mise-votiru validates cues live, a focused re-review on just the cues/preview code path (tools/fetch.py _build_cues, models.py FetchResult/SearchResult, and the 15 fetch call sites) would go deeper on the newest, least-battle-tested code.", "what": "1. Complete mise-votiru first (live validation) 2. Run titans scoped to: _build_cues, _build_email_context_metadata, FetchResult.to_dict, SearchResult._build_preview, and their call sites 3. Act on findings", "done": "Focused review complete, findings addressed or filed"}, "status": "done", "parent": "mise-jy3", "order": 13, "created_at": "2026-02-09T21:21:16Z", "created_by": "spm1001", "waiting_for": null, "done_at": "2026-02-09T23:15:31Z"}
{"id": "mise-wuwige", "type": "action", "title": "DRIVE_LINK_PATTERN in adapters/gmail.py has dedicated tests", "brief": {"why": "Removed dead _extract_drive_links from extractors/gmail.py during tidy-up — the live version (DRIVE_LINK_PATTERN regex in adapters/gmail.py) lost its only indirect test coverage", "what": "1. Add TestDriveLinkPattern class to test_gmail_adapter.py 2. Cover URL variants (docs, sheets, slides, drive file, drive folder)", "done": "DRIVE_LINK_PATTERN regex has dedicated unit tests"}, "status": "open", "parent": "mise-jy3", "order": 21, "created_at": "2026-02-18T22:28:40Z", "created_by": "spm1001", "waiting_for": null}
{"id": "mise-wuzani", "type": "action", "title": "Add poppler-utils to Pi/CLI setup checklist", "brief": {"why": "PDF thumbnails require poppler-utils system package. On Pi, this is ~30MB. Without it, PDF fetches silently degrade to text-only with a warning. Setup docs should mention this dependency.", "what": "1. Check mise-mavoze setup docs/checklist for system dependencies 2. Add poppler-utils alongside any existing apt-get instructions 3. Note it's optional (text extraction works without it)", "done": "Pi setup instructions mention poppler-utils. A fresh Pi following the docs gets thumbnails on first PDF fetch."}, "status": "open", "parent": "mise-mavoze", "order": 1, "created_at": "2026-02-18T21:15:16Z", "created_by": "spm1001", "waiting_for": null}
{"id": "mise-x2r", "title": "Add logging to extractors", "type": "action", "status": "done", "done_at": "2026-01-23T17:51:06.062342Z", "brief": {"why": "Extractors are pure functions with zero visibility. When extraction fails silently or produces garbage, no trace of what happened. Add key log points: DEBUG for processing counts, WARNING for unknown elements or truncation.", "what": "See title", "done": "When complete"}, "created_at": "2026-01-23T14:50:12.392463Z", "created_by": "spm1001", "order": 1, "waiting_for": null}
{"id": "mise-zapeZo", "type": "action", "title": "Prototype LLM distillation for one meeting note", "brief": {"why": "Test whether simple prompt can extract decisions/actions/key-points reliably", "what": "Prompt Sonnet with raw meeting note, evaluate output quality and token cost", "done": "Have sample distillation with quality notes and cost estimate"}, "status": "open", "parent": "mise-fetifo", "order": 2, "created_at": "2026-01-30T12:49:22Z", "created_by": "spm1001", "waiting_for": null}
{"id": "mise-zavizi", "type": "action", "title": "Subagent-test the mise skill", "brief": {"why": "Rewritten mise skill (CSO 91) passed lint and manual review but has no real-world validation. Subagent testing reveals behavioural gaps that reading can't — does a test Claude actually follow the workflow, use base_path, check comments?", "what": "1. Run skill-forge test_skill.py against mise skill 2. Review test Claude behaviour with skill loaded 3. Fix gaps revealed by testing", "done": "Test Claude correctly loads skill, follows post-fetch checklist, uses base_path on fetch/search"}, "status": "done", "parent": null, "order": 3, "created_at": "2026-02-09T15:48:38Z", "created_by": "spm1001", "waiting_for": null, "tactical": {"steps": ["Run skill-forge test_skill.py against mise skill", "Review test Claude behaviour with skill loaded", "Fix gaps revealed by testing"], "current": 3}, "done_at": "2026-02-09T20:30:53Z"}
{"id": "mise-zetoka", "type": "action", "title": "Profile Office conversion pipeline (DOCX 9s, XLSX 6s)", "brief": {"why": "Office files are the slowest fetch path (DOCX ~9s, XLSX ~6s). The pipeline is upload→convert→export→delete — unclear which step dominates. Can't optimise what you haven't measured.", "what": "1. Add per-step timing to conversion.py (upload, convert, export, cleanup) 2. Run against DOCX and XLSX test files 3. Document which step dominates 4. If upload dominates, investigate streaming upload; if convert, nothing to do (server-side)", "done": "Per-step timing data showing where the 9s goes. Filed actions for any steps we control, or documented that it's API-bound."}, "status": "done", "parent": "mise-weduje", "order": 2, "created_at": "2026-02-09T06:37:09Z", "created_by": "spm1001", "waiting_for": null, "tactical": {"steps": ["Add per-step timing to conversion.py (upload, convert, export, cleanup)", "Run against DOCX and XLSX test files", "Document which step dominates", "If upload dominates, investigate streaming upload; if convert, nothing to do (server-side)"], "current": 4}, "done_at": "2026-02-09T14:15:22Z"}
{"id": "mise-zezuva", "type": "outcome", "title": "Activity API search source", "brief": {"why": "Action items discovery needs efficient cross-file search. Activity API can find all comment events in one call vs N+1 queries through comments endpoint.", "what": "1. Add source='activity' option to search tool\n2. Query activity.query(filter='detail.action_detail_case:COMMENT')\n3. Filter to mentionedUsers containing current user\n4. Return files with open action items", "done": "search(source='activity') returns files with action items for current user"}, "status": "done", "order": 13, "created_at": "2026-01-31T17:49:05Z", "created_by": "spm1001", "done_at": "2026-02-01T10:58:33Z"}
{"id": "mise-zisiwe", "type": "action", "title": "Stream large web PDFs to temp file", "brief": {"why": "Web PDF fetch loads entire response.content into memory. A 200MB PDF kills the process. Drive path has streaming (via STREAMING_THRESHOLD_BYTES); web path doesn't.", "what": "1. Use httpx streaming (client.stream) in adapters/web.py for binary content types 2. Check Content-Length header — if over threshold, stream to temp file and set raw_bytes=None, add a temp_path field to WebData 3. Update _fetch_web_pdf in tools/fetch.py to handle both raw_bytes and temp_path 4. Add test with mocked large response", "done": "Web PDF fetch for files >50MB streams to temp without OOM. Unit test confirms temp_path path. Existing small-PDF path unchanged."}, "status": "done", "parent": "mise-jy3", "order": 5, "created_at": "2026-02-07T16:17:50Z", "created_by": "spm1001", "waiting_for": null, "tactical": {"steps": ["Use httpx streaming (client.stream) in adapters/web.py for binary content types", "Check Content-Length header — if over threshold, stream to temp file and set raw_bytes=None, add a temp_path field to WebData", "Update _fetch_web_pdf in tools/fetch.py to handle both raw_bytes and temp_path", "Add test with mocked large response"], "current": 4}, "done_at": "2026-02-07T16:32:53Z"}
{"id": "mise-zujapu", "type": "action", "title": "tools/fetch.py test coverage", "brief": {"why": "tools/fetch.py is 64% covered (215 uncovered lines) — the last significant gap. All adapters, extractors, and workspace are at 100%. fetch.py is 1400 lines of orchestration that exercises everything underneath.", "what": "1. Read fetch.py top-to-bottom mapping all uncovered paths 2. Write validation/routing tests (detect_id_type, URL parsing) 3. Write fetch_drive orchestration tests 4. Write fetch_gmail orchestration tests 5. Write fetch_web orchestration tests 6. Target 90%+ coverage", "done": "uv run pytest shows tools/fetch.py at 90%+ coverage"}, "status": "done", "parent": "mise-3uu", "order": 15, "created_at": "2026-02-09T08:09:31Z", "created_by": "spm1001", "waiting_for": null, "tactical": {"steps": ["Read fetch.py top-to-bottom mapping all uncovered paths", "Write validation/routing tests (detect_id_type, URL parsing)", "Write fetch_drive orchestration tests", "Write fetch_gmail orchestration tests", "Write fetch_web orchestration tests", "Target 90%+ coverage"], "current": 6}, "done_at": "2026-02-09T08:33:04Z"}
